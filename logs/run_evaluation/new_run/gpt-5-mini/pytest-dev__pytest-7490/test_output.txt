+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	deleted:    testing/python/metafunc.py
	deleted:    testing/test_junitxml.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	testing/python/metafunc.py.disabled
	testing/test_junitxml.py.disabled

no changes added to commit (use "git add" and/or "git commit -a")
+ git show
commit cd4beeff4caa6021666aaf6d8e5027333c5a5b10
Author: SWE-bench <setup@swebench.config>
Date:   Wed Sep 10 15:12:12 2025 +0000

    SWE-bench
+ git -c core.fileMode=false diff 7f7a36478abe7dd1fa993b115d22606aa0e35e88
diff --git a/testing/python/metafunc.py b/testing/python/metafunc.py
deleted file mode 100644
index 4e6cfaf91..000000000
--- a/testing/python/metafunc.py
+++ /dev/null
@@ -1,1887 +0,0 @@
-import itertools
-import re
-import sys
-import textwrap
-from typing import Any
-from typing import cast
-from typing import Dict
-from typing import Iterator
-from typing import List
-from typing import Optional
-from typing import Sequence
-from typing import Tuple
-from typing import Union
-
-import attr
-import hypothesis
-from hypothesis import strategies
-
-import pytest
-from _pytest import fixtures
-from _pytest import python
-from _pytest.outcomes import fail
-from _pytest.pytester import Testdir
-from _pytest.python import _idval
-from _pytest.python import idmaker
-
-
-class TestMetafunc:
-    def Metafunc(self, func, config=None) -> python.Metafunc:
-        # the unit tests of this class check if things work correctly
-        # on the funcarg level, so we don't need a full blown
-        # initialization
-        class FuncFixtureInfoMock:
-            name2fixturedefs = None
-
-            def __init__(self, names):
-                self.names_closure = names
-
-        @attr.s
-        class DefinitionMock(python.FunctionDefinition):
-            obj = attr.ib()
-            _nodeid = attr.ib()
-
-        names = fixtures.getfuncargnames(func)
-        fixtureinfo = FuncFixtureInfoMock(names)  # type: Any
-        definition = DefinitionMock._create(func, "mock::nodeid")  # type: Any
-        return python.Metafunc(definition, fixtureinfo, config)
-
-    def test_no_funcargs(self) -> None:
-        def function():
-            pass
-
-        metafunc = self.Metafunc(function)
-        assert not metafunc.fixturenames
-        repr(metafunc._calls)
-
-    def test_function_basic(self) -> None:
-        def func(arg1, arg2="qwe"):
-            pass
-
-        metafunc = self.Metafunc(func)
-        assert len(metafunc.fixturenames) == 1
-        assert "arg1" in metafunc.fixturenames
-        assert metafunc.function is func
-        assert metafunc.cls is None
-
-    def test_parametrize_error(self) -> None:
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-        metafunc.parametrize("x", [1, 2])
-        pytest.raises(ValueError, lambda: metafunc.parametrize("x", [5, 6]))
-        pytest.raises(ValueError, lambda: metafunc.parametrize("x", [5, 6]))
-        metafunc.parametrize("y", [1, 2])
-        pytest.raises(ValueError, lambda: metafunc.parametrize("y", [5, 6]))
-        pytest.raises(ValueError, lambda: metafunc.parametrize("y", [5, 6]))
-
-        with pytest.raises(TypeError, match="^ids must be a callable or an iterable$"):
-            metafunc.parametrize("y", [5, 6], ids=42)  # type: ignore[arg-type]
-
-    def test_parametrize_error_iterator(self) -> None:
-        def func(x):
-            raise NotImplementedError()
-
-        class Exc(Exception):
-            def __repr__(self):
-                return "Exc(from_gen)"
-
-        def gen() -> Iterator[Union[int, None, Exc]]:
-            yield 0
-            yield None
-            yield Exc()
-
-        metafunc = self.Metafunc(func)
-        # When the input is an iterator, only len(args) are taken,
-        # so the bad Exc isn't reached.
-        metafunc.parametrize("x", [1, 2], ids=gen())  # type: ignore[arg-type]
-        assert [(x.funcargs, x.id) for x in metafunc._calls] == [
-            ({"x": 1}, "0"),
-            ({"x": 2}, "2"),
-        ]
-        with pytest.raises(
-            fail.Exception,
-            match=(
-                r"In func: ids must be list of string/float/int/bool, found:"
-                r" Exc\(from_gen\) \(type: <class .*Exc'>\) at index 2"
-            ),
-        ):
-            metafunc.parametrize("x", [1, 2, 3], ids=gen())  # type: ignore[arg-type]
-
-    def test_parametrize_bad_scope(self) -> None:
-        def func(x):
-            pass
-
-        metafunc = self.Metafunc(func)
-        with pytest.raises(
-            fail.Exception,
-            match=r"parametrize\(\) call in func got an unexpected scope value 'doggy'",
-        ):
-            metafunc.parametrize("x", [1], scope="doggy")  # type: ignore[arg-type]
-
-    def test_parametrize_request_name(self, testdir: Testdir) -> None:
-        """Show proper error  when 'request' is used as a parameter name in parametrize (#6183)"""
-
-        def func(request):
-            raise NotImplementedError()
-
-        metafunc = self.Metafunc(func)
-        with pytest.raises(
-            fail.Exception,
-            match=r"'request' is a reserved name and cannot be used in @pytest.mark.parametrize",
-        ):
-            metafunc.parametrize("request", [1])
-
-    def test_find_parametrized_scope(self) -> None:
-        """unittest for _find_parametrized_scope (#3941)"""
-        from _pytest.python import _find_parametrized_scope
-
-        @attr.s
-        class DummyFixtureDef:
-            scope = attr.ib()
-
-        fixtures_defs = cast(
-            Dict[str, Sequence[fixtures.FixtureDef]],
-            dict(
-                session_fix=[DummyFixtureDef("session")],
-                package_fix=[DummyFixtureDef("package")],
-                module_fix=[DummyFixtureDef("module")],
-                class_fix=[DummyFixtureDef("class")],
-                func_fix=[DummyFixtureDef("function")],
-            ),
-        )
-
-        # use arguments to determine narrow scope; the cause of the bug is that it would look on all
-        # fixture defs given to the method
-        def find_scope(argnames, indirect):
-            return _find_parametrized_scope(argnames, fixtures_defs, indirect=indirect)
-
-        assert find_scope(["func_fix"], indirect=True) == "function"
-        assert find_scope(["class_fix"], indirect=True) == "class"
-        assert find_scope(["module_fix"], indirect=True) == "module"
-        assert find_scope(["package_fix"], indirect=True) == "package"
-        assert find_scope(["session_fix"], indirect=True) == "session"
-
-        assert find_scope(["class_fix", "func_fix"], indirect=True) == "function"
-        assert find_scope(["func_fix", "session_fix"], indirect=True) == "function"
-        assert find_scope(["session_fix", "class_fix"], indirect=True) == "class"
-        assert find_scope(["package_fix", "session_fix"], indirect=True) == "package"
-        assert find_scope(["module_fix", "session_fix"], indirect=True) == "module"
-
-        # when indirect is False or is not for all scopes, always use function
-        assert find_scope(["session_fix", "module_fix"], indirect=False) == "function"
-        assert (
-            find_scope(["session_fix", "module_fix"], indirect=["module_fix"])
-            == "function"
-        )
-        assert (
-            find_scope(
-                ["session_fix", "module_fix"], indirect=["session_fix", "module_fix"]
-            )
-            == "module"
-        )
-
-    def test_parametrize_and_id(self) -> None:
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-
-        metafunc.parametrize("x", [1, 2], ids=["basic", "advanced"])
-        metafunc.parametrize("y", ["abc", "def"])
-        ids = [x.id for x in metafunc._calls]
-        assert ids == ["basic-abc", "basic-def", "advanced-abc", "advanced-def"]
-
-    def test_parametrize_and_id_unicode(self) -> None:
-        """Allow unicode strings for "ids" parameter in Python 2 (##1905)"""
-
-        def func(x):
-            pass
-
-        metafunc = self.Metafunc(func)
-        metafunc.parametrize("x", [1, 2], ids=["basic", "advanced"])
-        ids = [x.id for x in metafunc._calls]
-        assert ids == ["basic", "advanced"]
-
-    def test_parametrize_with_wrong_number_of_ids(self) -> None:
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-
-        with pytest.raises(fail.Exception):
-            metafunc.parametrize("x", [1, 2], ids=["basic"])
-
-        with pytest.raises(fail.Exception):
-            metafunc.parametrize(
-                ("x", "y"), [("abc", "def"), ("ghi", "jkl")], ids=["one"]
-            )
-
-    def test_parametrize_ids_iterator_without_mark(self) -> None:
-        def func(x, y):
-            pass
-
-        it = itertools.count()
-
-        metafunc = self.Metafunc(func)
-        metafunc.parametrize("x", [1, 2], ids=it)
-        metafunc.parametrize("y", [3, 4], ids=it)
-        ids = [x.id for x in metafunc._calls]
-        assert ids == ["0-2", "0-3", "1-2", "1-3"]
-
-        metafunc = self.Metafunc(func)
-        metafunc.parametrize("x", [1, 2], ids=it)
-        metafunc.parametrize("y", [3, 4], ids=it)
-        ids = [x.id for x in metafunc._calls]
-        assert ids == ["4-6", "4-7", "5-6", "5-7"]
-
-    def test_parametrize_empty_list(self) -> None:
-        """#510"""
-
-        def func(y):
-            pass
-
-        class MockConfig:
-            def getini(self, name):
-                return ""
-
-            @property
-            def hook(self):
-                return self
-
-            def pytest_make_parametrize_id(self, **kw):
-                pass
-
-        metafunc = self.Metafunc(func, MockConfig())
-        metafunc.parametrize("y", [])
-        assert "skip" == metafunc._calls[0].marks[0].name
-
-    def test_parametrize_with_userobjects(self) -> None:
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-
-        class A:
-            pass
-
-        metafunc.parametrize("x", [A(), A()])
-        metafunc.parametrize("y", list("ab"))
-        assert metafunc._calls[0].id == "x0-a"
-        assert metafunc._calls[1].id == "x0-b"
-        assert metafunc._calls[2].id == "x1-a"
-        assert metafunc._calls[3].id == "x1-b"
-
-    @hypothesis.given(strategies.text() | strategies.binary())
-    @hypothesis.settings(
-        deadline=400.0
-    )  # very close to std deadline and CI boxes are not reliable in CPU power
-    def test_idval_hypothesis(self, value) -> None:
-        escaped = _idval(value, "a", 6, None, nodeid=None, config=None)
-        assert isinstance(escaped, str)
-        escaped.encode("ascii")
-
-    def test_unicode_idval(self) -> None:
-        """This tests that Unicode strings outside the ASCII character set get
-        escaped, using byte escapes if they're in that range or unicode
-        escapes if they're not.
-
-        """
-        values = [
-            ("", ""),
-            ("ascii", "ascii"),
-            ("ação", "a\\xe7\\xe3o"),
-            ("josé@blah.com", "jos\\xe9@blah.com"),
-            (
-                "δοκ.ιμή@παράδειγμα.δοκιμή",
-                "\\u03b4\\u03bf\\u03ba.\\u03b9\\u03bc\\u03ae@\\u03c0\\u03b1\\u03c1\\u03ac\\u03b4\\u03b5\\u03b9\\u03b3"
-                "\\u03bc\\u03b1.\\u03b4\\u03bf\\u03ba\\u03b9\\u03bc\\u03ae",
-            ),
-        ]
-        for val, expected in values:
-            assert _idval(val, "a", 6, None, nodeid=None, config=None) == expected
-
-    def test_unicode_idval_with_config(self) -> None:
-        """unittest for expected behavior to obtain ids with
-        disable_test_id_escaping_and_forfeit_all_rights_to_community_support
-        option. (#5294)
-        """
-
-        class MockConfig:
-            def __init__(self, config):
-                self.config = config
-
-            @property
-            def hook(self):
-                return self
-
-            def pytest_make_parametrize_id(self, **kw):
-                pass
-
-            def getini(self, name):
-                return self.config[name]
-
-        option = "disable_test_id_escaping_and_forfeit_all_rights_to_community_support"
-
-        values = [
-            ("ação", MockConfig({option: True}), "ação"),
-            ("ação", MockConfig({option: False}), "a\\xe7\\xe3o"),
-        ]  # type: List[Tuple[str, Any, str]]
-        for val, config, expected in values:
-            actual = _idval(val, "a", 6, None, nodeid=None, config=config)
-            assert actual == expected
-
-    def test_bytes_idval(self) -> None:
-        """unittest for the expected behavior to obtain ids for parametrized
-        bytes values:
-        - python2: non-ascii strings are considered bytes and formatted using
-        "binary escape", where any byte < 127 is escaped into its hex form.
-        - python3: bytes objects are always escaped using "binary escape".
-        """
-        values = [
-            (b"", ""),
-            (b"\xc3\xb4\xff\xe4", "\\xc3\\xb4\\xff\\xe4"),
-            (b"ascii", "ascii"),
-            ("αρά".encode(), "\\xce\\xb1\\xcf\\x81\\xce\\xac"),
-        ]
-        for val, expected in values:
-            assert _idval(val, "a", 6, idfn=None, nodeid=None, config=None) == expected
-
-    def test_class_or_function_idval(self) -> None:
-        """unittest for the expected behavior to obtain ids for parametrized
-        values that are classes or functions: their __name__.
-        """
-
-        class TestClass:
-            pass
-
-        def test_function():
-            pass
-
-        values = [(TestClass, "TestClass"), (test_function, "test_function")]
-        for val, expected in values:
-            assert _idval(val, "a", 6, None, nodeid=None, config=None) == expected
-
-    def test_idmaker_autoname(self) -> None:
-        """#250"""
-        result = idmaker(
-            ("a", "b"), [pytest.param("string", 1.0), pytest.param("st-ring", 2.0)]
-        )
-        assert result == ["string-1.0", "st-ring-2.0"]
-
-        result = idmaker(
-            ("a", "b"), [pytest.param(object(), 1.0), pytest.param(object(), object())]
-        )
-        assert result == ["a0-1.0", "a1-b1"]
-        # unicode mixing, issue250
-        result = idmaker(("a", "b"), [pytest.param({}, b"\xc3\xb4")])
-        assert result == ["a0-\\xc3\\xb4"]
-
-    def test_idmaker_with_bytes_regex(self) -> None:
-        result = idmaker(("a"), [pytest.param(re.compile(b"foo"), 1.0)])
-        assert result == ["foo"]
-
-    def test_idmaker_native_strings(self) -> None:
-        result = idmaker(
-            ("a", "b"),
-            [
-                pytest.param(1.0, -1.1),
-                pytest.param(2, -202),
-                pytest.param("three", "three hundred"),
-                pytest.param(True, False),
-                pytest.param(None, None),
-                pytest.param(re.compile("foo"), re.compile("bar")),
-                pytest.param(str, int),
-                pytest.param(list("six"), [66, 66]),
-                pytest.param({7}, set("seven")),
-                pytest.param(tuple("eight"), (8, -8, 8)),
-                pytest.param(b"\xc3\xb4", b"name"),
-                pytest.param(b"\xc3\xb4", "other"),
-            ],
-        )
-        assert result == [
-            "1.0--1.1",
-            "2--202",
-            "three-three hundred",
-            "True-False",
-            "None-None",
-            "foo-bar",
-            "str-int",
-            "a7-b7",
-            "a8-b8",
-            "a9-b9",
-            "\\xc3\\xb4-name",
-            "\\xc3\\xb4-other",
-        ]
-
-    def test_idmaker_non_printable_characters(self) -> None:
-        result = idmaker(
-            ("s", "n"),
-            [
-                pytest.param("\x00", 1),
-                pytest.param("\x05", 2),
-                pytest.param(b"\x00", 3),
-                pytest.param(b"\x05", 4),
-                pytest.param("\t", 5),
-                pytest.param(b"\t", 6),
-            ],
-        )
-        assert result == ["\\x00-1", "\\x05-2", "\\x00-3", "\\x05-4", "\\t-5", "\\t-6"]
-
-    def test_idmaker_manual_ids_must_be_printable(self) -> None:
-        result = idmaker(
-            ("s",),
-            [
-                pytest.param("x00", id="hello \x00"),
-                pytest.param("x05", id="hello \x05"),
-            ],
-        )
-        assert result == ["hello \\x00", "hello \\x05"]
-
-    def test_idmaker_enum(self) -> None:
-        enum = pytest.importorskip("enum")
-        e = enum.Enum("Foo", "one, two")
-        result = idmaker(("a", "b"), [pytest.param(e.one, e.two)])
-        assert result == ["Foo.one-Foo.two"]
-
-    def test_idmaker_idfn(self) -> None:
-        """#351"""
-
-        def ids(val: object) -> Optional[str]:
-            if isinstance(val, Exception):
-                return repr(val)
-            return None
-
-        result = idmaker(
-            ("a", "b"),
-            [
-                pytest.param(10.0, IndexError()),
-                pytest.param(20, KeyError()),
-                pytest.param("three", [1, 2, 3]),
-            ],
-            idfn=ids,
-        )
-        assert result == ["10.0-IndexError()", "20-KeyError()", "three-b2"]
-
-    def test_idmaker_idfn_unique_names(self) -> None:
-        """#351"""
-
-        def ids(val: object) -> str:
-            return "a"
-
-        result = idmaker(
-            ("a", "b"),
-            [
-                pytest.param(10.0, IndexError()),
-                pytest.param(20, KeyError()),
-                pytest.param("three", [1, 2, 3]),
-            ],
-            idfn=ids,
-        )
-        assert result == ["a-a0", "a-a1", "a-a2"]
-
-    def test_idmaker_with_idfn_and_config(self) -> None:
-        """unittest for expected behavior to create ids with idfn and
-        disable_test_id_escaping_and_forfeit_all_rights_to_community_support
-        option. (#5294)
-        """
-
-        class MockConfig:
-            def __init__(self, config):
-                self.config = config
-
-            @property
-            def hook(self):
-                return self
-
-            def pytest_make_parametrize_id(self, **kw):
-                pass
-
-            def getini(self, name):
-                return self.config[name]
-
-        option = "disable_test_id_escaping_and_forfeit_all_rights_to_community_support"
-
-        values = [
-            (MockConfig({option: True}), "ação"),
-            (MockConfig({option: False}), "a\\xe7\\xe3o"),
-        ]  # type: List[Tuple[Any, str]]
-        for config, expected in values:
-            result = idmaker(
-                ("a",), [pytest.param("string")], idfn=lambda _: "ação", config=config,
-            )
-            assert result == [expected]
-
-    def test_idmaker_with_ids_and_config(self) -> None:
-        """unittest for expected behavior to create ids with ids and
-        disable_test_id_escaping_and_forfeit_all_rights_to_community_support
-        option. (#5294)
-        """
-
-        class MockConfig:
-            def __init__(self, config):
-                self.config = config
-
-            @property
-            def hook(self):
-                return self
-
-            def pytest_make_parametrize_id(self, **kw):
-                pass
-
-            def getini(self, name):
-                return self.config[name]
-
-        option = "disable_test_id_escaping_and_forfeit_all_rights_to_community_support"
-
-        values = [
-            (MockConfig({option: True}), "ação"),
-            (MockConfig({option: False}), "a\\xe7\\xe3o"),
-        ]  # type: List[Tuple[Any, str]]
-        for config, expected in values:
-            result = idmaker(
-                ("a",), [pytest.param("string")], ids=["ação"], config=config,
-            )
-            assert result == [expected]
-
-    def test_parametrize_ids_exception(self, testdir: Testdir) -> None:
-        """
-        :param testdir: the instance of Testdir class, a temporary
-        test directory.
-        """
-        testdir.makepyfile(
-            """
-                import pytest
-
-                def ids(arg):
-                    raise Exception("bad ids")
-
-                @pytest.mark.parametrize("arg", ["a", "b"], ids=ids)
-                def test_foo(arg):
-                    pass
-            """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(
-            [
-                "*Exception: bad ids",
-                "*test_foo: error raised while trying to determine id of parameter 'arg' at position 0",
-            ]
-        )
-
-    def test_parametrize_ids_returns_non_string(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """\
-            import pytest
-
-            def ids(d):
-                return d
-
-            @pytest.mark.parametrize("arg", ({1: 2}, {3, 4}), ids=ids)
-            def test(arg):
-                assert arg
-
-            @pytest.mark.parametrize("arg", (1, 2.0, True), ids=ids)
-            def test_int(arg):
-                assert arg
-            """
-        )
-        result = testdir.runpytest("-vv", "-s")
-        result.stdout.fnmatch_lines(
-            [
-                "test_parametrize_ids_returns_non_string.py::test[arg0] PASSED",
-                "test_parametrize_ids_returns_non_string.py::test[arg1] PASSED",
-                "test_parametrize_ids_returns_non_string.py::test_int[1] PASSED",
-                "test_parametrize_ids_returns_non_string.py::test_int[2.0] PASSED",
-                "test_parametrize_ids_returns_non_string.py::test_int[True] PASSED",
-            ]
-        )
-
-    def test_idmaker_with_ids(self) -> None:
-        result = idmaker(
-            ("a", "b"), [pytest.param(1, 2), pytest.param(3, 4)], ids=["a", None]
-        )
-        assert result == ["a", "3-4"]
-
-    def test_idmaker_with_paramset_id(self) -> None:
-        result = idmaker(
-            ("a", "b"),
-            [pytest.param(1, 2, id="me"), pytest.param(3, 4, id="you")],
-            ids=["a", None],
-        )
-        assert result == ["me", "you"]
-
-    def test_idmaker_with_ids_unique_names(self) -> None:
-        result = idmaker(
-            ("a"), map(pytest.param, [1, 2, 3, 4, 5]), ids=["a", "a", "b", "c", "b"]
-        )
-        assert result == ["a0", "a1", "b0", "c", "b1"]
-
-    def test_parametrize_indirect(self) -> None:
-        """#714"""
-
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-        metafunc.parametrize("x", [1], indirect=True)
-        metafunc.parametrize("y", [2, 3], indirect=True)
-        assert len(metafunc._calls) == 2
-        assert metafunc._calls[0].funcargs == {}
-        assert metafunc._calls[1].funcargs == {}
-        assert metafunc._calls[0].params == dict(x=1, y=2)
-        assert metafunc._calls[1].params == dict(x=1, y=3)
-
-    def test_parametrize_indirect_list(self) -> None:
-        """#714"""
-
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-        metafunc.parametrize("x, y", [("a", "b")], indirect=["x"])
-        assert metafunc._calls[0].funcargs == dict(y="b")
-        assert metafunc._calls[0].params == dict(x="a")
-
-    def test_parametrize_indirect_list_all(self) -> None:
-        """#714"""
-
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-        metafunc.parametrize("x, y", [("a", "b")], indirect=["x", "y"])
-        assert metafunc._calls[0].funcargs == {}
-        assert metafunc._calls[0].params == dict(x="a", y="b")
-
-    def test_parametrize_indirect_list_empty(self) -> None:
-        """#714"""
-
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-        metafunc.parametrize("x, y", [("a", "b")], indirect=[])
-        assert metafunc._calls[0].funcargs == dict(x="a", y="b")
-        assert metafunc._calls[0].params == {}
-
-    def test_parametrize_indirect_wrong_type(self) -> None:
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-        with pytest.raises(
-            fail.Exception,
-            match="In func: expected Sequence or boolean for indirect, got dict",
-        ):
-            metafunc.parametrize("x, y", [("a", "b")], indirect={})  # type: ignore[arg-type]
-
-    def test_parametrize_indirect_list_functional(self, testdir: Testdir) -> None:
-        """
-        #714
-        Test parametrization with 'indirect' parameter applied on
-        particular arguments. As y is is direct, its value should
-        be used directly rather than being passed to the fixture
-        y.
-
-        :param testdir: the instance of Testdir class, a temporary
-        test directory.
-        """
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.fixture(scope='function')
-            def x(request):
-                return request.param * 3
-            @pytest.fixture(scope='function')
-            def y(request):
-                return request.param * 2
-            @pytest.mark.parametrize('x, y', [('a', 'b')], indirect=['x'])
-            def test_simple(x,y):
-                assert len(x) == 3
-                assert len(y) == 1
-        """
-        )
-        result = testdir.runpytest("-v")
-        result.stdout.fnmatch_lines(["*test_simple*a-b*", "*1 passed*"])
-
-    def test_parametrize_indirect_list_error(self) -> None:
-        """#714"""
-
-        def func(x, y):
-            pass
-
-        metafunc = self.Metafunc(func)
-        with pytest.raises(fail.Exception):
-            metafunc.parametrize("x, y", [("a", "b")], indirect=["x", "z"])
-
-    def test_parametrize_uses_no_fixture_error_indirect_false(
-        self, testdir: Testdir
-    ) -> None:
-        """The 'uses no fixture' error tells the user at collection time
-        that the parametrize data they've set up doesn't correspond to the
-        fixtures in their test function, rather than silently ignoring this
-        and letting the test potentially pass.
-
-        #714
-        """
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.mark.parametrize('x, y', [('a', 'b')], indirect=False)
-            def test_simple(x):
-                assert len(x) == 3
-        """
-        )
-        result = testdir.runpytest("--collect-only")
-        result.stdout.fnmatch_lines(["*uses no argument 'y'*"])
-
-    def test_parametrize_uses_no_fixture_error_indirect_true(
-        self, testdir: Testdir
-    ) -> None:
-        """#714"""
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.fixture(scope='function')
-            def x(request):
-                return request.param * 3
-            @pytest.fixture(scope='function')
-            def y(request):
-                return request.param * 2
-
-            @pytest.mark.parametrize('x, y', [('a', 'b')], indirect=True)
-            def test_simple(x):
-                assert len(x) == 3
-        """
-        )
-        result = testdir.runpytest("--collect-only")
-        result.stdout.fnmatch_lines(["*uses no fixture 'y'*"])
-
-    def test_parametrize_indirect_uses_no_fixture_error_indirect_string(
-        self, testdir: Testdir
-    ) -> None:
-        """#714"""
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.fixture(scope='function')
-            def x(request):
-                return request.param * 3
-
-            @pytest.mark.parametrize('x, y', [('a', 'b')], indirect='y')
-            def test_simple(x):
-                assert len(x) == 3
-        """
-        )
-        result = testdir.runpytest("--collect-only")
-        result.stdout.fnmatch_lines(["*uses no fixture 'y'*"])
-
-    def test_parametrize_indirect_uses_no_fixture_error_indirect_list(
-        self, testdir: Testdir
-    ) -> None:
-        """#714"""
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.fixture(scope='function')
-            def x(request):
-                return request.param * 3
-
-            @pytest.mark.parametrize('x, y', [('a', 'b')], indirect=['y'])
-            def test_simple(x):
-                assert len(x) == 3
-        """
-        )
-        result = testdir.runpytest("--collect-only")
-        result.stdout.fnmatch_lines(["*uses no fixture 'y'*"])
-
-    def test_parametrize_argument_not_in_indirect_list(self, testdir: Testdir) -> None:
-        """#714"""
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.fixture(scope='function')
-            def x(request):
-                return request.param * 3
-
-            @pytest.mark.parametrize('x, y', [('a', 'b')], indirect=['x'])
-            def test_simple(x):
-                assert len(x) == 3
-        """
-        )
-        result = testdir.runpytest("--collect-only")
-        result.stdout.fnmatch_lines(["*uses no argument 'y'*"])
-
-    def test_parametrize_gives_indicative_error_on_function_with_default_argument(
-        self, testdir
-    ) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.mark.parametrize('x, y', [('a', 'b')])
-            def test_simple(x, y=1):
-                assert len(x) == 1
-        """
-        )
-        result = testdir.runpytest("--collect-only")
-        result.stdout.fnmatch_lines(
-            ["*already takes an argument 'y' with a default value"]
-        )
-
-    def test_parametrize_functional(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize('x', [1,2], indirect=True)
-                metafunc.parametrize('y', [2])
-            @pytest.fixture
-            def x(request):
-                return request.param * 10
-
-            def test_simple(x,y):
-                assert x in (10,20)
-                assert y == 2
-        """
-        )
-        result = testdir.runpytest("-v")
-        result.stdout.fnmatch_lines(
-            ["*test_simple*1-2*", "*test_simple*2-2*", "*2 passed*"]
-        )
-
-    def test_parametrize_onearg(self) -> None:
-        metafunc = self.Metafunc(lambda x: None)
-        metafunc.parametrize("x", [1, 2])
-        assert len(metafunc._calls) == 2
-        assert metafunc._calls[0].funcargs == dict(x=1)
-        assert metafunc._calls[0].id == "1"
-        assert metafunc._calls[1].funcargs == dict(x=2)
-        assert metafunc._calls[1].id == "2"
-
-    def test_parametrize_onearg_indirect(self) -> None:
-        metafunc = self.Metafunc(lambda x: None)
-        metafunc.parametrize("x", [1, 2], indirect=True)
-        assert metafunc._calls[0].params == dict(x=1)
-        assert metafunc._calls[0].id == "1"
-        assert metafunc._calls[1].params == dict(x=2)
-        assert metafunc._calls[1].id == "2"
-
-    def test_parametrize_twoargs(self) -> None:
-        metafunc = self.Metafunc(lambda x, y: None)
-        metafunc.parametrize(("x", "y"), [(1, 2), (3, 4)])
-        assert len(metafunc._calls) == 2
-        assert metafunc._calls[0].funcargs == dict(x=1, y=2)
-        assert metafunc._calls[0].id == "1-2"
-        assert metafunc._calls[1].funcargs == dict(x=3, y=4)
-        assert metafunc._calls[1].id == "3-4"
-
-    def test_parametrize_multiple_times(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-            pytestmark = pytest.mark.parametrize("x", [1,2])
-            def test_func(x):
-                assert 0, x
-            class TestClass(object):
-                pytestmark = pytest.mark.parametrize("y", [3,4])
-                def test_meth(self, x, y):
-                    assert 0, x
-        """
-        )
-        result = testdir.runpytest()
-        assert result.ret == 1
-        result.assert_outcomes(failed=6)
-
-    def test_parametrize_CSV(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.mark.parametrize("x, y,", [(1,2), (2,3)])
-            def test_func(x, y):
-                assert x+1 == y
-        """
-        )
-        reprec = testdir.inline_run()
-        reprec.assertoutcome(passed=2)
-
-    def test_parametrize_class_scenarios(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-        # same as doc/en/example/parametrize scenario example
-        def pytest_generate_tests(metafunc):
-            idlist = []
-            argvalues = []
-            for scenario in metafunc.cls.scenarios:
-                idlist.append(scenario[0])
-                items = scenario[1].items()
-                argnames = [x[0] for x in items]
-                argvalues.append(([x[1] for x in items]))
-            metafunc.parametrize(argnames, argvalues, ids=idlist, scope="class")
-
-        class Test(object):
-               scenarios = [['1', {'arg': {1: 2}, "arg2": "value2"}],
-                            ['2', {'arg':'value2', "arg2": "value2"}]]
-
-               def test_1(self, arg, arg2):
-                  pass
-
-               def test_2(self, arg2, arg):
-                  pass
-
-               def test_3(self, arg, arg2):
-                  pass
-        """
-        )
-        result = testdir.runpytest("-v")
-        assert result.ret == 0
-        result.stdout.fnmatch_lines(
-            """
-            *test_1*1*
-            *test_2*1*
-            *test_3*1*
-            *test_1*2*
-            *test_2*2*
-            *test_3*2*
-            *6 passed*
-        """
-        )
-
-    def test_format_args(self) -> None:
-        def function1():
-            pass
-
-        assert fixtures._format_args(function1) == "()"
-
-        def function2(arg1):
-            pass
-
-        assert fixtures._format_args(function2) == "(arg1)"
-
-        def function3(arg1, arg2="qwe"):
-            pass
-
-        assert fixtures._format_args(function3) == "(arg1, arg2='qwe')"
-
-        def function4(arg1, *args, **kwargs):
-            pass
-
-        assert fixtures._format_args(function4) == "(arg1, *args, **kwargs)"
-
-
-class TestMetafuncFunctional:
-    def test_attributes(self, testdir: Testdir) -> None:
-        p = testdir.makepyfile(
-            """
-            # assumes that generate/provide runs in the same process
-            import sys, pytest
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize('metafunc', [metafunc])
-
-            @pytest.fixture
-            def metafunc(request):
-                return request.param
-
-            def test_function(metafunc, pytestconfig):
-                assert metafunc.config == pytestconfig
-                assert metafunc.module.__name__ == __name__
-                assert metafunc.function == test_function
-                assert metafunc.cls is None
-
-            class TestClass(object):
-                def test_method(self, metafunc, pytestconfig):
-                    assert metafunc.config == pytestconfig
-                    assert metafunc.module.__name__ == __name__
-                    unbound = TestClass.test_method
-                    assert metafunc.function == unbound
-                    assert metafunc.cls == TestClass
-        """
-        )
-        result = testdir.runpytest(p, "-v")
-        result.assert_outcomes(passed=2)
-
-    def test_two_functions(self, testdir: Testdir) -> None:
-        p = testdir.makepyfile(
-            """
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize('arg1', [10, 20], ids=['0', '1'])
-
-            def test_func1(arg1):
-                assert arg1 == 10
-
-            def test_func2(arg1):
-                assert arg1 in (10, 20)
-        """
-        )
-        result = testdir.runpytest("-v", p)
-        result.stdout.fnmatch_lines(
-            [
-                "*test_func1*0*PASS*",
-                "*test_func1*1*FAIL*",
-                "*test_func2*PASS*",
-                "*test_func2*PASS*",
-                "*1 failed, 3 passed*",
-            ]
-        )
-
-    def test_noself_in_method(self, testdir: Testdir) -> None:
-        p = testdir.makepyfile(
-            """
-            def pytest_generate_tests(metafunc):
-                assert 'xyz' not in metafunc.fixturenames
-
-            class TestHello(object):
-                def test_hello(xyz):
-                    pass
-        """
-        )
-        result = testdir.runpytest(p)
-        result.assert_outcomes(passed=1)
-
-    def test_generate_tests_in_class(self, testdir: Testdir) -> None:
-        p = testdir.makepyfile(
-            """
-            class TestClass(object):
-                def pytest_generate_tests(self, metafunc):
-                    metafunc.parametrize('hello', ['world'], ids=['hellow'])
-
-                def test_myfunc(self, hello):
-                    assert hello == "world"
-        """
-        )
-        result = testdir.runpytest("-v", p)
-        result.stdout.fnmatch_lines(["*test_myfunc*hello*PASS*", "*1 passed*"])
-
-    def test_two_functions_not_same_instance(self, testdir: Testdir) -> None:
-        p = testdir.makepyfile(
-            """
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize('arg1', [10, 20], ids=["0", "1"])
-
-            class TestClass(object):
-                def test_func(self, arg1):
-                    assert not hasattr(self, 'x')
-                    self.x = 1
-        """
-        )
-        result = testdir.runpytest("-v", p)
-        result.stdout.fnmatch_lines(
-            ["*test_func*0*PASS*", "*test_func*1*PASS*", "*2 pass*"]
-        )
-
-    def test_issue28_setup_method_in_generate_tests(self, testdir: Testdir) -> None:
-        p = testdir.makepyfile(
-            """
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize('arg1', [1])
-
-            class TestClass(object):
-                def test_method(self, arg1):
-                    assert arg1 == self.val
-                def setup_method(self, func):
-                    self.val = 1
-            """
-        )
-        result = testdir.runpytest(p)
-        result.assert_outcomes(passed=1)
-
-    def test_parametrize_functional2(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize("arg1", [1,2])
-                metafunc.parametrize("arg2", [4,5])
-            def test_hello(arg1, arg2):
-                assert 0, (arg1, arg2)
-        """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(
-            ["*(1, 4)*", "*(1, 5)*", "*(2, 4)*", "*(2, 5)*", "*4 failed*"]
-        )
-
-    def test_parametrize_and_inner_getfixturevalue(self, testdir: Testdir) -> None:
-        p = testdir.makepyfile(
-            """
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize("arg1", [1], indirect=True)
-                metafunc.parametrize("arg2", [10], indirect=True)
-
-            import pytest
-            @pytest.fixture
-            def arg1(request):
-                x = request.getfixturevalue("arg2")
-                return x + request.param
-
-            @pytest.fixture
-            def arg2(request):
-                return request.param
-
-            def test_func1(arg1, arg2):
-                assert arg1 == 11
-        """
-        )
-        result = testdir.runpytest("-v", p)
-        result.stdout.fnmatch_lines(["*test_func1*1*PASS*", "*1 passed*"])
-
-    def test_parametrize_on_setup_arg(self, testdir: Testdir) -> None:
-        p = testdir.makepyfile(
-            """
-            def pytest_generate_tests(metafunc):
-                assert "arg1" in metafunc.fixturenames
-                metafunc.parametrize("arg1", [1], indirect=True)
-
-            import pytest
-            @pytest.fixture
-            def arg1(request):
-                return request.param
-
-            @pytest.fixture
-            def arg2(request, arg1):
-                return 10 * arg1
-
-            def test_func(arg2):
-                assert arg2 == 10
-        """
-        )
-        result = testdir.runpytest("-v", p)
-        result.stdout.fnmatch_lines(["*test_func*1*PASS*", "*1 passed*"])
-
-    def test_parametrize_with_ids(self, testdir: Testdir) -> None:
-        testdir.makeini(
-            """
-            [pytest]
-            console_output_style=classic
-        """
-        )
-        testdir.makepyfile(
-            """
-            import pytest
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize(("a", "b"), [(1,1), (1,2)],
-                                     ids=["basic", "advanced"])
-
-            def test_function(a, b):
-                assert a == b
-        """
-        )
-        result = testdir.runpytest("-v")
-        assert result.ret == 1
-        result.stdout.fnmatch_lines_random(
-            ["*test_function*basic*PASSED", "*test_function*advanced*FAILED"]
-        )
-
-    def test_parametrize_without_ids(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize(("a", "b"),
-                                     [(1,object()), (1.3,object())])
-
-            def test_function(a, b):
-                assert 1
-        """
-        )
-        result = testdir.runpytest("-v")
-        result.stdout.fnmatch_lines(
-            """
-            *test_function*1-b0*
-            *test_function*1.3-b1*
-        """
-        )
-
-    def test_parametrize_with_None_in_ids(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize(("a", "b"), [(1,1), (1,1), (1,2)],
-                                     ids=["basic", None, "advanced"])
-
-            def test_function(a, b):
-                assert a == b
-        """
-        )
-        result = testdir.runpytest("-v")
-        assert result.ret == 1
-        result.stdout.fnmatch_lines_random(
-            [
-                "*test_function*basic*PASSED*",
-                "*test_function*1-1*PASSED*",
-                "*test_function*advanced*FAILED*",
-            ]
-        )
-
-    def test_fixture_parametrized_empty_ids(self, testdir: Testdir) -> None:
-        """Fixtures parametrized with empty ids cause an internal error (#1849)."""
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture(scope="module", ids=[], params=[])
-            def temp(request):
-               return request.param
-
-            def test_temp(temp):
-                 pass
-        """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(["* 1 skipped *"])
-
-    def test_parametrized_empty_ids(self, testdir: Testdir) -> None:
-        """Tests parametrized with empty ids cause an internal error (#1849)."""
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.mark.parametrize('temp', [], ids=list())
-            def test_temp(temp):
-                 pass
-        """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(["* 1 skipped *"])
-
-    def test_parametrized_ids_invalid_type(self, testdir: Testdir) -> None:
-        """Test error with non-strings/non-ints, without generator (#1857)."""
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.mark.parametrize("x, expected", [(1, 2), (3, 4), (5, 6)], ids=(None, 2, type))
-            def test_ids_numbers(x,expected):
-                assert x * 2 == expected
-        """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(
-            [
-                "In test_ids_numbers: ids must be list of string/float/int/bool,"
-                " found: <class 'type'> (type: <class 'type'>) at index 2"
-            ]
-        )
-
-    def test_parametrize_with_identical_ids_get_unique_names(
-        self, testdir: Testdir
-    ) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-            def pytest_generate_tests(metafunc):
-                metafunc.parametrize(("a", "b"), [(1,1), (1,2)],
-                                     ids=["a", "a"])
-
-            def test_function(a, b):
-                assert a == b
-        """
-        )
-        result = testdir.runpytest("-v")
-        assert result.ret == 1
-        result.stdout.fnmatch_lines_random(
-            ["*test_function*a0*PASSED*", "*test_function*a1*FAILED*"]
-        )
-
-    @pytest.mark.parametrize(("scope", "length"), [("module", 2), ("function", 4)])
-    def test_parametrize_scope_overrides(
-        self, testdir: Testdir, scope: str, length: int
-    ) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-            values = []
-            def pytest_generate_tests(metafunc):
-                if "arg" in metafunc.fixturenames:
-                    metafunc.parametrize("arg", [1,2], indirect=True,
-                                         scope=%r)
-            @pytest.fixture
-            def arg(request):
-                values.append(request.param)
-                return request.param
-            def test_hello(arg):
-                assert arg in (1,2)
-            def test_world(arg):
-                assert arg in (1,2)
-            def test_checklength():
-                assert len(values) == %d
-        """
-            % (scope, length)
-        )
-        reprec = testdir.inline_run()
-        reprec.assertoutcome(passed=5)
-
-    def test_parametrize_issue323(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture(scope='module', params=range(966))
-            def foo(request):
-                return request.param
-
-            def test_it(foo):
-                pass
-            def test_it2(foo):
-                pass
-        """
-        )
-        reprec = testdir.inline_run("--collect-only")
-        assert not reprec.getcalls("pytest_internalerror")
-
-    def test_usefixtures_seen_in_generate_tests(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-            def pytest_generate_tests(metafunc):
-                assert "abc" in metafunc.fixturenames
-                metafunc.parametrize("abc", [1])
-
-            @pytest.mark.usefixtures("abc")
-            def test_function():
-                pass
-        """
-        )
-        reprec = testdir.runpytest()
-        reprec.assert_outcomes(passed=1)
-
-    def test_generate_tests_only_done_in_subdir(self, testdir: Testdir) -> None:
-        sub1 = testdir.mkpydir("sub1")
-        sub2 = testdir.mkpydir("sub2")
-        sub1.join("conftest.py").write(
-            textwrap.dedent(
-                """\
-                def pytest_generate_tests(metafunc):
-                    assert metafunc.function.__name__ == "test_1"
-                """
-            )
-        )
-        sub2.join("conftest.py").write(
-            textwrap.dedent(
-                """\
-                def pytest_generate_tests(metafunc):
-                    assert metafunc.function.__name__ == "test_2"
-                """
-            )
-        )
-        sub1.join("test_in_sub1.py").write("def test_1(): pass")
-        sub2.join("test_in_sub2.py").write("def test_2(): pass")
-        result = testdir.runpytest("--keep-duplicates", "-v", "-s", sub1, sub2, sub1)
-        result.assert_outcomes(passed=3)
-
-    def test_generate_same_function_names_issue403(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            def make_tests():
-                @pytest.mark.parametrize("x", range(2))
-                def test_foo(x):
-                    pass
-                return test_foo
-
-            test_x = make_tests()
-            test_y = make_tests()
-        """
-        )
-        reprec = testdir.runpytest()
-        reprec.assert_outcomes(passed=4)
-
-    def test_parametrize_misspelling(self, testdir: Testdir) -> None:
-        """#463"""
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.mark.parametrise("x", range(2))
-            def test_foo(x):
-                pass
-        """
-        )
-        result = testdir.runpytest("--collectonly")
-        result.stdout.fnmatch_lines(
-            [
-                "collected 0 items / 1 error",
-                "",
-                "*= ERRORS =*",
-                "*_ ERROR collecting test_parametrize_misspelling.py _*",
-                "test_parametrize_misspelling.py:3: in <module>",
-                '    @pytest.mark.parametrise("x", range(2))',
-                "E   Failed: Unknown 'parametrise' mark, did you mean 'parametrize'?",
-                "*! Interrupted: 1 error during collection !*",
-                "*= 1 error in *",
-            ]
-        )
-
-
-class TestMetafuncFunctionalAuto:
-    """
-    Tests related to automatically find out the correct scope for parametrized tests (#1832).
-    """
-
-    def test_parametrize_auto_scope(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture(scope='session', autouse=True)
-            def fixture():
-                return 1
-
-            @pytest.mark.parametrize('animal', ["dog", "cat"])
-            def test_1(animal):
-                assert animal in ('dog', 'cat')
-
-            @pytest.mark.parametrize('animal', ['fish'])
-            def test_2(animal):
-                assert animal == 'fish'
-
-        """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(["* 3 passed *"])
-
-    def test_parametrize_auto_scope_indirect(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture(scope='session')
-            def echo(request):
-                return request.param
-
-            @pytest.mark.parametrize('animal, echo', [("dog", 1), ("cat", 2)], indirect=['echo'])
-            def test_1(animal, echo):
-                assert animal in ('dog', 'cat')
-                assert echo in (1, 2, 3)
-
-            @pytest.mark.parametrize('animal, echo', [('fish', 3)], indirect=['echo'])
-            def test_2(animal, echo):
-                assert animal == 'fish'
-                assert echo in (1, 2, 3)
-        """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(["* 3 passed *"])
-
-    def test_parametrize_auto_scope_override_fixture(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture(scope='session', autouse=True)
-            def animal():
-                return 'fox'
-
-            @pytest.mark.parametrize('animal', ["dog", "cat"])
-            def test_1(animal):
-                assert animal in ('dog', 'cat')
-        """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(["* 2 passed *"])
-
-    def test_parametrize_all_indirects(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture()
-            def animal(request):
-                return request.param
-
-            @pytest.fixture(scope='session')
-            def echo(request):
-                return request.param
-
-            @pytest.mark.parametrize('animal, echo', [("dog", 1), ("cat", 2)], indirect=True)
-            def test_1(animal, echo):
-                assert animal in ('dog', 'cat')
-                assert echo in (1, 2, 3)
-
-            @pytest.mark.parametrize('animal, echo', [("fish", 3)], indirect=True)
-            def test_2(animal, echo):
-                assert animal == 'fish'
-                assert echo in (1, 2, 3)
-        """
-        )
-        result = testdir.runpytest()
-        result.stdout.fnmatch_lines(["* 3 passed *"])
-
-    def test_parametrize_some_arguments_auto_scope(
-        self, testdir: Testdir, monkeypatch
-    ) -> None:
-        """Integration test for (#3941)"""
-        class_fix_setup = []  # type: List[object]
-        monkeypatch.setattr(sys, "class_fix_setup", class_fix_setup, raising=False)
-        func_fix_setup = []  # type: List[object]
-        monkeypatch.setattr(sys, "func_fix_setup", func_fix_setup, raising=False)
-
-        testdir.makepyfile(
-            """
-            import pytest
-            import sys
-
-            @pytest.fixture(scope='class', autouse=True)
-            def class_fix(request):
-                sys.class_fix_setup.append(request.param)
-
-            @pytest.fixture(autouse=True)
-            def func_fix():
-                sys.func_fix_setup.append(True)
-
-            @pytest.mark.parametrize('class_fix', [10, 20], indirect=True)
-            class Test:
-                def test_foo(self):
-                    pass
-                def test_bar(self):
-                    pass
-            """
-        )
-        result = testdir.runpytest_inprocess()
-        result.stdout.fnmatch_lines(["* 4 passed in *"])
-        assert func_fix_setup == [True] * 4
-        assert class_fix_setup == [10, 20]
-
-    def test_parametrize_issue634(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture(scope='module')
-            def foo(request):
-                print('preparing foo-%d' % request.param)
-                return 'foo-%d' % request.param
-
-            def test_one(foo):
-                pass
-
-            def test_two(foo):
-                pass
-
-            test_two.test_with = (2, 3)
-
-            def pytest_generate_tests(metafunc):
-                params = (1, 2, 3, 4)
-                if not 'foo' in metafunc.fixturenames:
-                    return
-
-                test_with = getattr(metafunc.function, 'test_with', None)
-                if test_with:
-                    params = test_with
-                metafunc.parametrize('foo', params, indirect=True)
-        """
-        )
-        result = testdir.runpytest("-s")
-        output = result.stdout.str()
-        assert output.count("preparing foo-2") == 1
-        assert output.count("preparing foo-3") == 1
-
-
-class TestMarkersWithParametrization:
-    """#308"""
-
-    def test_simple_mark(self, testdir: Testdir) -> None:
-        s = """
-            import pytest
-
-            @pytest.mark.foo
-            @pytest.mark.parametrize(("n", "expected"), [
-                (1, 2),
-                pytest.param(1, 3, marks=pytest.mark.bar),
-                (2, 3),
-            ])
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """
-        items = testdir.getitems(s)
-        assert len(items) == 3
-        for item in items:
-            assert "foo" in item.keywords
-        assert "bar" not in items[0].keywords
-        assert "bar" in items[1].keywords
-        assert "bar" not in items[2].keywords
-
-    def test_select_based_on_mark(self, testdir: Testdir) -> None:
-        s = """
-            import pytest
-
-            @pytest.mark.parametrize(("n", "expected"), [
-                (1, 2),
-                pytest.param(2, 3, marks=pytest.mark.foo),
-                (3, 4),
-            ])
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """
-        testdir.makepyfile(s)
-        rec = testdir.inline_run("-m", "foo")
-        passed, skipped, fail = rec.listoutcomes()
-        assert len(passed) == 1
-        assert len(skipped) == 0
-        assert len(fail) == 0
-
-    def test_simple_xfail(self, testdir: Testdir) -> None:
-        s = """
-            import pytest
-
-            @pytest.mark.parametrize(("n", "expected"), [
-                (1, 2),
-                pytest.param(1, 3, marks=pytest.mark.xfail),
-                (2, 3),
-            ])
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """
-        testdir.makepyfile(s)
-        reprec = testdir.inline_run()
-        # xfail is skip??
-        reprec.assertoutcome(passed=2, skipped=1)
-
-    def test_simple_xfail_single_argname(self, testdir: Testdir) -> None:
-        s = """
-            import pytest
-
-            @pytest.mark.parametrize("n", [
-                2,
-                pytest.param(3, marks=pytest.mark.xfail),
-                4,
-            ])
-            def test_isEven(n):
-                assert n % 2 == 0
-        """
-        testdir.makepyfile(s)
-        reprec = testdir.inline_run()
-        reprec.assertoutcome(passed=2, skipped=1)
-
-    def test_xfail_with_arg(self, testdir: Testdir) -> None:
-        s = """
-            import pytest
-
-            @pytest.mark.parametrize(("n", "expected"), [
-                (1, 2),
-                pytest.param(1, 3, marks=pytest.mark.xfail("True")),
-                (2, 3),
-            ])
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """
-        testdir.makepyfile(s)
-        reprec = testdir.inline_run()
-        reprec.assertoutcome(passed=2, skipped=1)
-
-    def test_xfail_with_kwarg(self, testdir: Testdir) -> None:
-        s = """
-            import pytest
-
-            @pytest.mark.parametrize(("n", "expected"), [
-                (1, 2),
-                pytest.param(1, 3, marks=pytest.mark.xfail(reason="some bug")),
-                (2, 3),
-            ])
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """
-        testdir.makepyfile(s)
-        reprec = testdir.inline_run()
-        reprec.assertoutcome(passed=2, skipped=1)
-
-    def test_xfail_with_arg_and_kwarg(self, testdir: Testdir) -> None:
-        s = """
-            import pytest
-
-            @pytest.mark.parametrize(("n", "expected"), [
-                (1, 2),
-                pytest.param(1, 3, marks=pytest.mark.xfail("True", reason="some bug")),
-                (2, 3),
-            ])
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """
-        testdir.makepyfile(s)
-        reprec = testdir.inline_run()
-        reprec.assertoutcome(passed=2, skipped=1)
-
-    @pytest.mark.parametrize("strict", [True, False])
-    def test_xfail_passing_is_xpass(self, testdir: Testdir, strict: bool) -> None:
-        s = """
-            import pytest
-
-            m = pytest.mark.xfail("sys.version_info > (0, 0, 0)", reason="some bug", strict={strict})
-
-            @pytest.mark.parametrize(("n", "expected"), [
-                (1, 2),
-                pytest.param(2, 3, marks=m),
-                (3, 4),
-            ])
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """.format(
-            strict=strict
-        )
-        testdir.makepyfile(s)
-        reprec = testdir.inline_run()
-        passed, failed = (2, 1) if strict else (3, 0)
-        reprec.assertoutcome(passed=passed, failed=failed)
-
-    def test_parametrize_called_in_generate_tests(self, testdir: Testdir) -> None:
-        s = """
-            import pytest
-
-
-            def pytest_generate_tests(metafunc):
-                passingTestData = [(1, 2),
-                                   (2, 3)]
-                failingTestData = [(1, 3),
-                                   (2, 2)]
-
-                testData = passingTestData + [pytest.param(*d, marks=pytest.mark.xfail)
-                                  for d in failingTestData]
-                metafunc.parametrize(("n", "expected"), testData)
-
-
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """
-        testdir.makepyfile(s)
-        reprec = testdir.inline_run()
-        reprec.assertoutcome(passed=2, skipped=2)
-
-    def test_parametrize_ID_generation_string_int_works(self, testdir: Testdir) -> None:
-        """#290"""
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture
-            def myfixture():
-                return 'example'
-            @pytest.mark.parametrize(
-                'limit', (0, '0'))
-            def test_limit(limit, myfixture):
-                return
-        """
-        )
-        reprec = testdir.inline_run()
-        reprec.assertoutcome(passed=2)
-
-    @pytest.mark.parametrize("strict", [True, False])
-    def test_parametrize_marked_value(self, testdir: Testdir, strict: bool) -> None:
-        s = """
-            import pytest
-
-            @pytest.mark.parametrize(("n", "expected"), [
-                pytest.param(
-                    2,3,
-                    marks=pytest.mark.xfail("sys.version_info > (0, 0, 0)", reason="some bug", strict={strict}),
-                ),
-                pytest.param(
-                    2,3,
-                    marks=[pytest.mark.xfail("sys.version_info > (0, 0, 0)", reason="some bug", strict={strict})],
-                ),
-            ])
-            def test_increment(n, expected):
-                assert n + 1 == expected
-        """.format(
-            strict=strict
-        )
-        testdir.makepyfile(s)
-        reprec = testdir.inline_run()
-        passed, failed = (0, 2) if strict else (2, 0)
-        reprec.assertoutcome(passed=passed, failed=failed)
-
-    def test_pytest_make_parametrize_id(self, testdir: Testdir) -> None:
-        testdir.makeconftest(
-            """
-            def pytest_make_parametrize_id(config, val):
-                return str(val * 2)
-        """
-        )
-        testdir.makepyfile(
-            """
-                import pytest
-
-                @pytest.mark.parametrize("x", range(2))
-                def test_func(x):
-                    pass
-                """
-        )
-        result = testdir.runpytest("-v")
-        result.stdout.fnmatch_lines(["*test_func*0*PASS*", "*test_func*2*PASS*"])
-
-    def test_pytest_make_parametrize_id_with_argname(self, testdir: Testdir) -> None:
-        testdir.makeconftest(
-            """
-            def pytest_make_parametrize_id(config, val, argname):
-                return str(val * 2 if argname == 'x' else val * 10)
-        """
-        )
-        testdir.makepyfile(
-            """
-                import pytest
-
-                @pytest.mark.parametrize("x", range(2))
-                def test_func_a(x):
-                    pass
-
-                @pytest.mark.parametrize("y", [1])
-                def test_func_b(y):
-                    pass
-                """
-        )
-        result = testdir.runpytest("-v")
-        result.stdout.fnmatch_lines(
-            ["*test_func_a*0*PASS*", "*test_func_a*2*PASS*", "*test_func_b*10*PASS*"]
-        )
-
-    def test_parametrize_positional_args(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.mark.parametrize("a", [1], False)
-            def test_foo(a):
-                pass
-        """
-        )
-        result = testdir.runpytest()
-        result.assert_outcomes(passed=1)
-
-    def test_parametrize_iterator(self, testdir: Testdir) -> None:
-        testdir.makepyfile(
-            """
-            import itertools
-            import pytest
-
-            id_parametrize = pytest.mark.parametrize(
-                ids=("param%d" % i for i in itertools.count())
-            )
-
-            @id_parametrize('y', ['a', 'b'])
-            def test1(y):
-                pass
-
-            @id_parametrize('y', ['a', 'b'])
-            def test2(y):
-                pass
-
-            @pytest.mark.parametrize("a, b", [(1, 2), (3, 4)], ids=itertools.count())
-            def test_converted_to_str(a, b):
-                pass
-        """
-        )
-        result = testdir.runpytest("-vv", "-s")
-        result.stdout.fnmatch_lines(
-            [
-                "test_parametrize_iterator.py::test1[param0] PASSED",
-                "test_parametrize_iterator.py::test1[param1] PASSED",
-                "test_parametrize_iterator.py::test2[param0] PASSED",
-                "test_parametrize_iterator.py::test2[param1] PASSED",
-                "test_parametrize_iterator.py::test_converted_to_str[0] PASSED",
-                "test_parametrize_iterator.py::test_converted_to_str[1] PASSED",
-                "*= 6 passed in *",
-            ]
-        )
diff --git a/testing/test_junitxml.py b/testing/test_junitxml.py
deleted file mode 100644
index eb8475ca5..000000000
--- a/testing/test_junitxml.py
+++ /dev/null
@@ -1,1603 +0,0 @@
-import os
-import platform
-from datetime import datetime
-from typing import cast
-from typing import List
-from typing import Tuple
-from xml.dom import minidom
-
-import py
-import xmlschema
-
-import pytest
-from _pytest.compat import TYPE_CHECKING
-from _pytest.config import Config
-from _pytest.junitxml import bin_xml_escape
-from _pytest.junitxml import LogXML
-from _pytest.pathlib import Path
-from _pytest.reports import BaseReport
-from _pytest.reports import TestReport
-from _pytest.store import Store
-
-
-@pytest.fixture(scope="session")
-def schema():
-    """Returns a xmlschema.XMLSchema object for the junit-10.xsd file"""
-    fn = Path(__file__).parent / "example_scripts/junit-10.xsd"
-    with fn.open() as f:
-        return xmlschema.XMLSchema(f)
-
-
-@pytest.fixture
-def run_and_parse(testdir, schema):
-    """
-    Fixture that returns a function that can be used to execute pytest and return
-    the parsed ``DomNode`` of the root xml node.
-
-    The ``family`` parameter is used to configure the ``junit_family`` of the written report.
-    "xunit2" is also automatically validated against the schema.
-    """
-
-    def run(*args, family="xunit1"):
-        if family:
-            args = ("-o", "junit_family=" + family) + args
-        xml_path = testdir.tmpdir.join("junit.xml")
-        result = testdir.runpytest("--junitxml=%s" % xml_path, *args)
-        if family == "xunit2":
-            with xml_path.open() as f:
-                schema.validate(f)
-        xmldoc = minidom.parse(str(xml_path))
-        return result, DomNode(xmldoc)
-
-    return run
-
-
-def assert_attr(node, **kwargs):
-    __tracebackhide__ = True
-
-    def nodeval(node, name):
-        anode = node.getAttributeNode(name)
-        if anode is not None:
-            return anode.value
-
-    expected = {name: str(value) for name, value in kwargs.items()}
-    on_node = {name: nodeval(node, name) for name in expected}
-    assert on_node == expected
-
-
-class DomNode:
-    def __init__(self, dom):
-        self.__node = dom
-
-    def __repr__(self):
-        return self.__node.toxml()
-
-    def find_first_by_tag(self, tag):
-        return self.find_nth_by_tag(tag, 0)
-
-    def _by_tag(self, tag):
-        return self.__node.getElementsByTagName(tag)
-
-    @property
-    def children(self):
-        return [type(self)(x) for x in self.__node.childNodes]
-
-    @property
-    def get_unique_child(self):
-        children = self.children
-        assert len(children) == 1
-        return children[0]
-
-    def find_nth_by_tag(self, tag, n):
-        items = self._by_tag(tag)
-        try:
-            nth = items[n]
-        except IndexError:
-            pass
-        else:
-            return type(self)(nth)
-
-    def find_by_tag(self, tag):
-        t = type(self)
-        return [t(x) for x in self.__node.getElementsByTagName(tag)]
-
-    def __getitem__(self, key):
-        node = self.__node.getAttributeNode(key)
-        if node is not None:
-            return node.value
-
-    def assert_attr(self, **kwargs):
-        __tracebackhide__ = True
-        return assert_attr(self.__node, **kwargs)
-
-    def toxml(self):
-        return self.__node.toxml()
-
-    @property
-    def text(self):
-        return self.__node.childNodes[0].wholeText
-
-    @property
-    def tag(self):
-        return self.__node.tagName
-
-    @property
-    def next_sibling(self):
-        return type(self)(self.__node.nextSibling)
-
-
-parametrize_families = pytest.mark.parametrize("xunit_family", ["xunit1", "xunit2"])
-
-
-class TestPython:
-    @parametrize_families
-    def test_summing_simple(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            def test_pass():
-                pass
-            def test_fail():
-                assert 0
-            def test_skip():
-                pytest.skip("")
-            @pytest.mark.xfail
-            def test_xfail():
-                assert 0
-            @pytest.mark.xfail
-            def test_xpass():
-                assert 1
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(name="pytest", errors=0, failures=1, skipped=2, tests=5)
-
-    @parametrize_families
-    def test_summing_simple_with_errors(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.fixture
-            def fixture():
-                raise Exception()
-            def test_pass():
-                pass
-            def test_fail():
-                assert 0
-            def test_error(fixture):
-                pass
-            @pytest.mark.xfail
-            def test_xfail():
-                assert False
-            @pytest.mark.xfail(strict=True)
-            def test_xpass():
-                assert True
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(name="pytest", errors=1, failures=2, skipped=1, tests=5)
-
-    @parametrize_families
-    def test_hostname_in_xml(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            def test_pass():
-                pass
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(hostname=platform.node())
-
-    @parametrize_families
-    def test_timestamp_in_xml(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            def test_pass():
-                pass
-        """
-        )
-        start_time = datetime.now()
-        result, dom = run_and_parse(family=xunit_family)
-        node = dom.find_first_by_tag("testsuite")
-        timestamp = datetime.strptime(node["timestamp"], "%Y-%m-%dT%H:%M:%S.%f")
-        assert start_time <= timestamp < datetime.now()
-
-    def test_timing_function(self, testdir, run_and_parse, mock_timing):
-        testdir.makepyfile(
-            """
-            from _pytest import timing
-            def setup_module():
-                timing.sleep(1)
-            def teardown_module():
-                timing.sleep(2)
-            def test_sleep():
-                timing.sleep(4)
-        """
-        )
-        result, dom = run_and_parse()
-        node = dom.find_first_by_tag("testsuite")
-        tnode = node.find_first_by_tag("testcase")
-        val = tnode["time"]
-        assert float(val) == 7.0
-
-    @pytest.mark.parametrize("duration_report", ["call", "total"])
-    def test_junit_duration_report(
-        self, testdir, monkeypatch, duration_report, run_and_parse
-    ):
-
-        # mock LogXML.node_reporter so it always sets a known duration to each test report object
-        original_node_reporter = LogXML.node_reporter
-
-        def node_reporter_wrapper(s, report):
-            report.duration = 1.0
-            reporter = original_node_reporter(s, report)
-            return reporter
-
-        monkeypatch.setattr(LogXML, "node_reporter", node_reporter_wrapper)
-
-        testdir.makepyfile(
-            """
-            def test_foo():
-                pass
-        """
-        )
-        result, dom = run_and_parse(
-            "-o", "junit_duration_report={}".format(duration_report)
-        )
-        node = dom.find_first_by_tag("testsuite")
-        tnode = node.find_first_by_tag("testcase")
-        val = float(tnode["time"])
-        if duration_report == "total":
-            assert val == 3.0
-        else:
-            assert duration_report == "call"
-            assert val == 1.0
-
-    @parametrize_families
-    def test_setup_error(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture
-            def arg(request):
-                raise ValueError("Error reason")
-            def test_function(arg):
-                pass
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(errors=1, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="test_setup_error", name="test_function")
-        fnode = tnode.find_first_by_tag("error")
-        fnode.assert_attr(message='failed on setup with "ValueError: Error reason"')
-        assert "ValueError" in fnode.toxml()
-
-    @parametrize_families
-    def test_teardown_error(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture
-            def arg():
-                yield
-                raise ValueError('Error reason')
-            def test_function(arg):
-                pass
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="test_teardown_error", name="test_function")
-        fnode = tnode.find_first_by_tag("error")
-        fnode.assert_attr(message='failed on teardown with "ValueError: Error reason"')
-        assert "ValueError" in fnode.toxml()
-
-    @parametrize_families
-    def test_call_failure_teardown_error(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture
-            def arg():
-                yield
-                raise Exception("Teardown Exception")
-            def test_function(arg):
-                raise Exception("Call Exception")
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(errors=1, failures=1, tests=1)
-        first, second = dom.find_by_tag("testcase")
-        if not first or not second or first == second:
-            assert 0
-        fnode = first.find_first_by_tag("failure")
-        fnode.assert_attr(message="Exception: Call Exception")
-        snode = second.find_first_by_tag("error")
-        snode.assert_attr(
-            message='failed on teardown with "Exception: Teardown Exception"'
-        )
-
-    @parametrize_families
-    def test_skip_contains_name_reason(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            def test_skip():
-                pytest.skip("hello23")
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret == 0
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(skipped=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="test_skip_contains_name_reason", name="test_skip")
-        snode = tnode.find_first_by_tag("skipped")
-        snode.assert_attr(type="pytest.skip", message="hello23")
-
-    @parametrize_families
-    def test_mark_skip_contains_name_reason(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.mark.skip(reason="hello24")
-            def test_skip():
-                assert True
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret == 0
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(skipped=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(
-            classname="test_mark_skip_contains_name_reason", name="test_skip"
-        )
-        snode = tnode.find_first_by_tag("skipped")
-        snode.assert_attr(type="pytest.skip", message="hello24")
-
-    @parametrize_families
-    def test_mark_skipif_contains_name_reason(
-        self, testdir, run_and_parse, xunit_family
-    ):
-        testdir.makepyfile(
-            """
-            import pytest
-            GLOBAL_CONDITION = True
-            @pytest.mark.skipif(GLOBAL_CONDITION, reason="hello25")
-            def test_skip():
-                assert True
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret == 0
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(skipped=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(
-            classname="test_mark_skipif_contains_name_reason", name="test_skip"
-        )
-        snode = tnode.find_first_by_tag("skipped")
-        snode.assert_attr(type="pytest.skip", message="hello25")
-
-    @parametrize_families
-    def test_mark_skip_doesnt_capture_output(
-        self, testdir, run_and_parse, xunit_family
-    ):
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.mark.skip(reason="foo")
-            def test_skip():
-                print("bar!")
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret == 0
-        node_xml = dom.find_first_by_tag("testsuite").toxml()
-        assert "bar!" not in node_xml
-
-    @parametrize_families
-    def test_classname_instance(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            class TestClass(object):
-                def test_method(self):
-                    assert 0
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(failures=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(
-            classname="test_classname_instance.TestClass", name="test_method"
-        )
-
-    @parametrize_families
-    def test_classname_nested_dir(self, testdir, run_and_parse, xunit_family):
-        p = testdir.tmpdir.ensure("sub", "test_hello.py")
-        p.write("def test_func(): 0/0")
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(failures=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="sub.test_hello", name="test_func")
-
-    @parametrize_families
-    def test_internal_error(self, testdir, run_and_parse, xunit_family):
-        testdir.makeconftest("def pytest_runtest_protocol(): 0 / 0")
-        testdir.makepyfile("def test_function(): pass")
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(errors=1, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="pytest", name="internal")
-        fnode = tnode.find_first_by_tag("error")
-        fnode.assert_attr(message="internal error")
-        assert "Division" in fnode.toxml()
-
-    @pytest.mark.parametrize(
-        "junit_logging", ["no", "log", "system-out", "system-err", "out-err", "all"]
-    )
-    @parametrize_families
-    def test_failure_function(
-        self, testdir, junit_logging, run_and_parse, xunit_family
-    ):
-        testdir.makepyfile(
-            """
-            import logging
-            import sys
-
-            def test_fail():
-                print("hello-stdout")
-                sys.stderr.write("hello-stderr\\n")
-                logging.info('info msg')
-                logging.warning('warning msg')
-                raise ValueError(42)
-        """
-        )
-
-        result, dom = run_and_parse(
-            "-o", "junit_logging=%s" % junit_logging, family=xunit_family
-        )
-        assert result.ret, "Expected ret > 0"
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(failures=1, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="test_failure_function", name="test_fail")
-        fnode = tnode.find_first_by_tag("failure")
-        fnode.assert_attr(message="ValueError: 42")
-        assert "ValueError" in fnode.toxml(), "ValueError not included"
-
-        if junit_logging in ["log", "all"]:
-            logdata = tnode.find_first_by_tag("system-out")
-            log_xml = logdata.toxml()
-            assert logdata.tag == "system-out", "Expected tag: system-out"
-            assert "info msg" not in log_xml, "Unexpected INFO message"
-            assert "warning msg" in log_xml, "Missing WARN message"
-        if junit_logging in ["system-out", "out-err", "all"]:
-            systemout = tnode.find_first_by_tag("system-out")
-            systemout_xml = systemout.toxml()
-            assert systemout.tag == "system-out", "Expected tag: system-out"
-            assert "info msg" not in systemout_xml, "INFO message found in system-out"
-            assert (
-                "hello-stdout" in systemout_xml
-            ), "Missing 'hello-stdout' in system-out"
-        if junit_logging in ["system-err", "out-err", "all"]:
-            systemerr = tnode.find_first_by_tag("system-err")
-            systemerr_xml = systemerr.toxml()
-            assert systemerr.tag == "system-err", "Expected tag: system-err"
-            assert "info msg" not in systemerr_xml, "INFO message found in system-err"
-            assert (
-                "hello-stderr" in systemerr_xml
-            ), "Missing 'hello-stderr' in system-err"
-            assert (
-                "warning msg" not in systemerr_xml
-            ), "WARN message found in system-err"
-        if junit_logging == "no":
-            assert not tnode.find_by_tag("log"), "Found unexpected content: log"
-            assert not tnode.find_by_tag(
-                "system-out"
-            ), "Found unexpected content: system-out"
-            assert not tnode.find_by_tag(
-                "system-err"
-            ), "Found unexpected content: system-err"
-
-    @parametrize_families
-    def test_failure_verbose_message(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import sys
-            def test_fail():
-                assert 0, "An error"
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        node = dom.find_first_by_tag("testsuite")
-        tnode = node.find_first_by_tag("testcase")
-        fnode = tnode.find_first_by_tag("failure")
-        fnode.assert_attr(message="AssertionError: An error assert 0")
-
-    @parametrize_families
-    def test_failure_escape(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.mark.parametrize('arg1', "<&'", ids="<&'")
-            def test_func(arg1):
-                print(arg1)
-                assert 0
-        """
-        )
-        result, dom = run_and_parse(
-            "-o", "junit_logging=system-out", family=xunit_family
-        )
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(failures=3, tests=3)
-
-        for index, char in enumerate("<&'"):
-
-            tnode = node.find_nth_by_tag("testcase", index)
-            tnode.assert_attr(
-                classname="test_failure_escape", name="test_func[%s]" % char
-            )
-            sysout = tnode.find_first_by_tag("system-out")
-            text = sysout.text
-            assert "%s\n" % char in text
-
-    @parametrize_families
-    def test_junit_prefixing(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            def test_func():
-                assert 0
-            class TestHello(object):
-                def test_hello(self):
-                    pass
-        """
-        )
-        result, dom = run_and_parse("--junitprefix=xyz", family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(failures=1, tests=2)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="xyz.test_junit_prefixing", name="test_func")
-        tnode = node.find_nth_by_tag("testcase", 1)
-        tnode.assert_attr(
-            classname="xyz.test_junit_prefixing.TestHello", name="test_hello"
-        )
-
-    @parametrize_families
-    def test_xfailure_function(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            def test_xfail():
-                pytest.xfail("42")
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert not result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(skipped=1, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="test_xfailure_function", name="test_xfail")
-        fnode = tnode.find_first_by_tag("skipped")
-        fnode.assert_attr(type="pytest.xfail", message="42")
-
-    @parametrize_families
-    def test_xfailure_marker(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.mark.xfail(reason="42")
-            def test_xfail():
-                assert False
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        assert not result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(skipped=1, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="test_xfailure_marker", name="test_xfail")
-        fnode = tnode.find_first_by_tag("skipped")
-        fnode.assert_attr(type="pytest.xfail", message="42")
-
-    @pytest.mark.parametrize(
-        "junit_logging", ["no", "log", "system-out", "system-err", "out-err", "all"]
-    )
-    def test_xfail_captures_output_once(self, testdir, junit_logging, run_and_parse):
-        testdir.makepyfile(
-            """
-            import sys
-            import pytest
-
-            @pytest.mark.xfail()
-            def test_fail():
-                sys.stdout.write('XFAIL This is stdout')
-                sys.stderr.write('XFAIL This is stderr')
-                assert 0
-        """
-        )
-        result, dom = run_and_parse("-o", "junit_logging=%s" % junit_logging)
-        node = dom.find_first_by_tag("testsuite")
-        tnode = node.find_first_by_tag("testcase")
-        if junit_logging in ["system-err", "out-err", "all"]:
-            assert len(tnode.find_by_tag("system-err")) == 1
-        else:
-            assert len(tnode.find_by_tag("system-err")) == 0
-
-        if junit_logging in ["log", "system-out", "out-err", "all"]:
-            assert len(tnode.find_by_tag("system-out")) == 1
-        else:
-            assert len(tnode.find_by_tag("system-out")) == 0
-
-    @parametrize_families
-    def test_xfailure_xpass(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.mark.xfail
-            def test_xpass():
-                pass
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        # assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(skipped=0, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="test_xfailure_xpass", name="test_xpass")
-
-    @parametrize_families
-    def test_xfailure_xpass_strict(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile(
-            """
-            import pytest
-            @pytest.mark.xfail(strict=True, reason="This needs to fail!")
-            def test_xpass():
-                pass
-        """
-        )
-        result, dom = run_and_parse(family=xunit_family)
-        # assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(skipped=0, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(classname="test_xfailure_xpass_strict", name="test_xpass")
-        fnode = tnode.find_first_by_tag("failure")
-        fnode.assert_attr(message="[XPASS(strict)] This needs to fail!")
-
-    @parametrize_families
-    def test_collect_error(self, testdir, run_and_parse, xunit_family):
-        testdir.makepyfile("syntax error")
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(errors=1, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        fnode = tnode.find_first_by_tag("error")
-        fnode.assert_attr(message="collection failure")
-        assert "SyntaxError" in fnode.toxml()
-
-    def test_unicode(self, testdir, run_and_parse):
-        value = "hx\xc4\x85\xc4\x87\n"
-        testdir.makepyfile(
-            """\
-            # coding: latin1
-            def test_hello():
-                print(%r)
-                assert 0
-            """
-            % value
-        )
-        result, dom = run_and_parse()
-        assert result.ret == 1
-        tnode = dom.find_first_by_tag("testcase")
-        fnode = tnode.find_first_by_tag("failure")
-        assert "hx" in fnode.toxml()
-
-    def test_assertion_binchars(self, testdir, run_and_parse):
-        """this test did fail when the escaping wasnt strict"""
-        testdir.makepyfile(
-            """
-
-            M1 = '\x01\x02\x03\x04'
-            M2 = '\x01\x02\x03\x05'
-
-            def test_str_compare():
-                assert M1 == M2
-            """
-        )
-        result, dom = run_and_parse()
-        print(dom.toxml())
-
-    @pytest.mark.parametrize("junit_logging", ["no", "system-out"])
-    def test_pass_captures_stdout(self, testdir, run_and_parse, junit_logging):
-        testdir.makepyfile(
-            """
-            def test_pass():
-                print('hello-stdout')
-        """
-        )
-        result, dom = run_and_parse("-o", "junit_logging=%s" % junit_logging)
-        node = dom.find_first_by_tag("testsuite")
-        pnode = node.find_first_by_tag("testcase")
-        if junit_logging == "no":
-            assert not node.find_by_tag(
-                "system-out"
-            ), "system-out should not be generated"
-        if junit_logging == "system-out":
-            systemout = pnode.find_first_by_tag("system-out")
-            assert (
-                "hello-stdout" in systemout.toxml()
-            ), "'hello-stdout' should be in system-out"
-
-    @pytest.mark.parametrize("junit_logging", ["no", "system-err"])
-    def test_pass_captures_stderr(self, testdir, run_and_parse, junit_logging):
-        testdir.makepyfile(
-            """
-            import sys
-            def test_pass():
-                sys.stderr.write('hello-stderr')
-        """
-        )
-        result, dom = run_and_parse("-o", "junit_logging=%s" % junit_logging)
-        node = dom.find_first_by_tag("testsuite")
-        pnode = node.find_first_by_tag("testcase")
-        if junit_logging == "no":
-            assert not node.find_by_tag(
-                "system-err"
-            ), "system-err should not be generated"
-        if junit_logging == "system-err":
-            systemerr = pnode.find_first_by_tag("system-err")
-            assert (
-                "hello-stderr" in systemerr.toxml()
-            ), "'hello-stderr' should be in system-err"
-
-    @pytest.mark.parametrize("junit_logging", ["no", "system-out"])
-    def test_setup_error_captures_stdout(self, testdir, run_and_parse, junit_logging):
-        testdir.makepyfile(
-            """
-            import pytest
-
-            @pytest.fixture
-            def arg(request):
-                print('hello-stdout')
-                raise ValueError()
-            def test_function(arg):
-                pass
-        """
-        )
-        result, dom = run_and_parse("-o", "junit_logging=%s" % junit_logging)
-        node = dom.find_first_by_tag("testsuite")
-        pnode = node.find_first_by_tag("testcase")
-        if junit_logging == "no":
-            assert not node.find_by_tag(
-                "system-out"
-            ), "system-out should not be generated"
-        if junit_logging == "system-out":
-            systemout = pnode.find_first_by_tag("system-out")
-            assert (
-                "hello-stdout" in systemout.toxml()
-            ), "'hello-stdout' should be in system-out"
-
-    @pytest.mark.parametrize("junit_logging", ["no", "system-err"])
-    def test_setup_error_captures_stderr(self, testdir, run_and_parse, junit_logging):
-        testdir.makepyfile(
-            """
-            import sys
-            import pytest
-
-            @pytest.fixture
-            def arg(request):
-                sys.stderr.write('hello-stderr')
-                raise ValueError()
-            def test_function(arg):
-                pass
-        """
-        )
-        result, dom = run_and_parse("-o", "junit_logging=%s" % junit_logging)
-        node = dom.find_first_by_tag("testsuite")
-        pnode = node.find_first_by_tag("testcase")
-        if junit_logging == "no":
-            assert not node.find_by_tag(
-                "system-err"
-            ), "system-err should not be generated"
-        if junit_logging == "system-err":
-            systemerr = pnode.find_first_by_tag("system-err")
-            assert (
-                "hello-stderr" in systemerr.toxml()
-            ), "'hello-stderr' should be in system-err"
-
-    @pytest.mark.parametrize("junit_logging", ["no", "system-out"])
-    def test_avoid_double_stdout(self, testdir, run_and_parse, junit_logging):
-        testdir.makepyfile(
-            """
-            import sys
-            import pytest
-
-            @pytest.fixture
-            def arg(request):
-                yield
-                sys.stdout.write('hello-stdout teardown')
-                raise ValueError()
-            def test_function(arg):
-                sys.stdout.write('hello-stdout call')
-        """
-        )
-        result, dom = run_and_parse("-o", "junit_logging=%s" % junit_logging)
-        node = dom.find_first_by_tag("testsuite")
-        pnode = node.find_first_by_tag("testcase")
-        if junit_logging == "no":
-            assert not node.find_by_tag(
-                "system-out"
-            ), "system-out should not be generated"
-        if junit_logging == "system-out":
-            systemout = pnode.find_first_by_tag("system-out")
-            assert "hello-stdout call" in systemout.toxml()
-            assert "hello-stdout teardown" in systemout.toxml()
-
-
-def test_mangle_test_address():
-    from _pytest.junitxml import mangle_test_address
-
-    address = "::".join(["a/my.py.thing.py", "Class", "()", "method", "[a-1-::]"])
-    newnames = mangle_test_address(address)
-    assert newnames == ["a.my.py.thing", "Class", "method", "[a-1-::]"]
-
-
-def test_dont_configure_on_workers(tmpdir) -> None:
-    gotten = []  # type: List[object]
-
-    class FakeConfig:
-        if TYPE_CHECKING:
-            workerinput = None
-
-        def __init__(self):
-            self.pluginmanager = self
-            self.option = self
-            self._store = Store()
-
-        def getini(self, name):
-            return "pytest"
-
-        junitprefix = None
-        # XXX: shouldn't need tmpdir ?
-        xmlpath = str(tmpdir.join("junix.xml"))
-        register = gotten.append
-
-    fake_config = cast(Config, FakeConfig())
-    from _pytest import junitxml
-
-    junitxml.pytest_configure(fake_config)
-    assert len(gotten) == 1
-    FakeConfig.workerinput = None
-    junitxml.pytest_configure(fake_config)
-    assert len(gotten) == 1
-
-
-class TestNonPython:
-    @parametrize_families
-    def test_summing_simple(self, testdir, run_and_parse, xunit_family):
-        testdir.makeconftest(
-            """
-            import pytest
-            def pytest_collect_file(path, parent):
-                if path.ext == ".xyz":
-                    return MyItem(path, parent)
-            class MyItem(pytest.Item):
-                def __init__(self, path, parent):
-                    super(MyItem, self).__init__(path.basename, parent)
-                    self.fspath = path
-                def runtest(self):
-                    raise ValueError(42)
-                def repr_failure(self, excinfo):
-                    return "custom item runtest failed"
-        """
-        )
-        testdir.tmpdir.join("myfile.xyz").write("hello")
-        result, dom = run_and_parse(family=xunit_family)
-        assert result.ret
-        node = dom.find_first_by_tag("testsuite")
-        node.assert_attr(errors=0, failures=1, skipped=0, tests=1)
-        tnode = node.find_first_by_tag("testcase")
-        tnode.assert_attr(name="myfile.xyz")
-        fnode = tnode.find_first_by_tag("failure")
-        fnode.assert_attr(message="custom item runtest failed")
-        assert "custom item runtest failed" in fnode.toxml()
-
-
-@pytest.mark.parametrize("junit_logging", ["no", "system-out"])
-def test_nullbyte(testdir, junit_logging):
-    # A null byte can not occur in XML (see section 2.2 of the spec)
-    testdir.makepyfile(
-        """
-        import sys
-        def test_print_nullbyte():
-            sys.stdout.write('Here the null -->' + chr(0) + '<--')
-            sys.stdout.write('In repr form -->' + repr(chr(0)) + '<--')
-            assert False
-    """
-    )
-    xmlf = testdir.tmpdir.join("junit.xml")
-    testdir.runpytest("--junitxml=%s" % xmlf, "-o", "junit_logging=%s" % junit_logging)
-    text = xmlf.read()
-    assert "\x00" not in text
-    if junit_logging == "system-out":
-        assert "#x00" in text
-    if junit_logging == "no":
-        assert "#x00" not in text
-
-
-@pytest.mark.parametrize("junit_logging", ["no", "system-out"])
-def test_nullbyte_replace(testdir, junit_logging):
-    # Check if the null byte gets replaced
-    testdir.makepyfile(
-        """
-        import sys
-        def test_print_nullbyte():
-            sys.stdout.write('Here the null -->' + chr(0) + '<--')
-            sys.stdout.write('In repr form -->' + repr(chr(0)) + '<--')
-            assert False
-    """
-    )
-    xmlf = testdir.tmpdir.join("junit.xml")
-    testdir.runpytest("--junitxml=%s" % xmlf, "-o", "junit_logging=%s" % junit_logging)
-    text = xmlf.read()
-    if junit_logging == "system-out":
-        assert "#x0" in text
-    if junit_logging == "no":
-        assert "#x0" not in text
-
-
-def test_invalid_xml_escape():
-    # Test some more invalid xml chars, the full range should be
-    # tested really but let's just test the edges of the ranges
-    # instead.
-    # XXX This only tests low unicode character points for now as
-    #     there are some issues with the testing infrastructure for
-    #     the higher ones.
-    # XXX Testing 0xD (\r) is tricky as it overwrites the just written
-    #     line in the output, so we skip it too.
-    invalid = (
-        0x00,
-        0x1,
-        0xB,
-        0xC,
-        0xE,
-        0x19,
-        27,  # issue #126
-        0xD800,
-        0xDFFF,
-        0xFFFE,
-        0x0FFFF,
-    )  # , 0x110000)
-    valid = (0x9, 0xA, 0x20)
-    # 0xD, 0xD7FF, 0xE000, 0xFFFD, 0x10000, 0x10FFFF)
-
-    for i in invalid:
-        got = bin_xml_escape(chr(i)).uniobj
-        if i <= 0xFF:
-            expected = "#x%02X" % i
-        else:
-            expected = "#x%04X" % i
-        assert got == expected
-    for i in valid:
-        assert chr(i) == bin_xml_escape(chr(i)).uniobj
-
-
-def test_logxml_path_expansion(tmpdir, monkeypatch):
-    home_tilde = py.path.local(os.path.expanduser("~")).join("test.xml")
-    xml_tilde = LogXML("~%stest.xml" % tmpdir.sep, None)
-    assert xml_tilde.logfile == home_tilde
-
-    monkeypatch.setenv("HOME", str(tmpdir))
-    home_var = os.path.normpath(os.path.expandvars("$HOME/test.xml"))
-    xml_var = LogXML("$HOME%stest.xml" % tmpdir.sep, None)
-    assert xml_var.logfile == home_var
-
-
-def test_logxml_changingdir(testdir):
-    testdir.makepyfile(
-        """
-        def test_func():
-            import os
-            os.chdir("a")
-    """
-    )
-    testdir.tmpdir.mkdir("a")
-    result = testdir.runpytest("--junitxml=a/x.xml")
-    assert result.ret == 0
-    assert testdir.tmpdir.join("a/x.xml").check()
-
-
-def test_logxml_makedir(testdir):
-    """--junitxml should automatically create directories for the xml file"""
-    testdir.makepyfile(
-        """
-        def test_pass():
-            pass
-    """
-    )
-    result = testdir.runpytest("--junitxml=path/to/results.xml")
-    assert result.ret == 0
-    assert testdir.tmpdir.join("path/to/results.xml").check()
-
-
-def test_logxml_check_isdir(testdir):
-    """Give an error if --junit-xml is a directory (#2089)"""
-    result = testdir.runpytest("--junit-xml=.")
-    result.stderr.fnmatch_lines(["*--junitxml must be a filename*"])
-
-
-def test_escaped_parametrized_names_xml(testdir, run_and_parse):
-    testdir.makepyfile(
-        """\
-        import pytest
-        @pytest.mark.parametrize('char', ["\\x00"])
-        def test_func(char):
-            assert char
-        """
-    )
-    result, dom = run_and_parse()
-    assert result.ret == 0
-    node = dom.find_first_by_tag("testcase")
-    node.assert_attr(name="test_func[\\x00]")
-
-
-def test_double_colon_split_function_issue469(testdir, run_and_parse):
-    testdir.makepyfile(
-        """
-        import pytest
-        @pytest.mark.parametrize('param', ["double::colon"])
-        def test_func(param):
-            pass
-    """
-    )
-    result, dom = run_and_parse()
-    assert result.ret == 0
-    node = dom.find_first_by_tag("testcase")
-    node.assert_attr(classname="test_double_colon_split_function_issue469")
-    node.assert_attr(name="test_func[double::colon]")
-
-
-def test_double_colon_split_method_issue469(testdir, run_and_parse):
-    testdir.makepyfile(
-        """
-        import pytest
-        class TestClass(object):
-            @pytest.mark.parametrize('param', ["double::colon"])
-            def test_func(self, param):
-                pass
-    """
-    )
-    result, dom = run_and_parse()
-    assert result.ret == 0
-    node = dom.find_first_by_tag("testcase")
-    node.assert_attr(classname="test_double_colon_split_method_issue469.TestClass")
-    node.assert_attr(name="test_func[double::colon]")
-
-
-def test_unicode_issue368(testdir) -> None:
-    path = testdir.tmpdir.join("test.xml")
-    log = LogXML(str(path), None)
-    ustr = "ВНИ!"
-
-    class Report(BaseReport):
-        longrepr = ustr
-        sections = []  # type: List[Tuple[str, str]]
-        nodeid = "something"
-        location = "tests/filename.py", 42, "TestClass.method"
-
-    test_report = cast(TestReport, Report())
-
-    # hopefully this is not too brittle ...
-    log.pytest_sessionstart()
-    node_reporter = log._opentestcase(test_report)
-    node_reporter.append_failure(test_report)
-    node_reporter.append_collect_error(test_report)
-    node_reporter.append_collect_skipped(test_report)
-    node_reporter.append_error(test_report)
-    test_report.longrepr = "filename", 1, ustr
-    node_reporter.append_skipped(test_report)
-    test_report.longrepr = "filename", 1, "Skipped: 卡嘣嘣"
-    node_reporter.append_skipped(test_report)
-    test_report.wasxfail = ustr  # type: ignore[attr-defined]
-    node_reporter.append_skipped(test_report)
-    log.pytest_sessionfinish()
-
-
-def test_record_property(testdir, run_and_parse):
-    testdir.makepyfile(
-        """
-        import pytest
-
-        @pytest.fixture
-        def other(record_property):
-            record_property("bar", 1)
-        def test_record(record_property, other):
-            record_property("foo", "<1");
-    """
-    )
-    result, dom = run_and_parse()
-    node = dom.find_first_by_tag("testsuite")
-    tnode = node.find_first_by_tag("testcase")
-    psnode = tnode.find_first_by_tag("properties")
-    pnodes = psnode.find_by_tag("property")
-    pnodes[0].assert_attr(name="bar", value="1")
-    pnodes[1].assert_attr(name="foo", value="<1")
-    result.stdout.fnmatch_lines(["*= 1 passed in *"])
-
-
-def test_record_property_same_name(testdir, run_and_parse):
-    testdir.makepyfile(
-        """
-        def test_record_with_same_name(record_property):
-            record_property("foo", "bar")
-            record_property("foo", "baz")
-    """
-    )
-    result, dom = run_and_parse()
-    node = dom.find_first_by_tag("testsuite")
-    tnode = node.find_first_by_tag("testcase")
-    psnode = tnode.find_first_by_tag("properties")
-    pnodes = psnode.find_by_tag("property")
-    pnodes[0].assert_attr(name="foo", value="bar")
-    pnodes[1].assert_attr(name="foo", value="baz")
-
-
-@pytest.mark.parametrize("fixture_name", ["record_property", "record_xml_attribute"])
-def test_record_fixtures_without_junitxml(testdir, fixture_name):
-    testdir.makepyfile(
-        """
-        def test_record({fixture_name}):
-            {fixture_name}("foo", "bar")
-    """.format(
-            fixture_name=fixture_name
-        )
-    )
-    result = testdir.runpytest()
-    assert result.ret == 0
-
-
-@pytest.mark.filterwarnings("default")
-def test_record_attribute(testdir, run_and_parse):
-    testdir.makeini(
-        """
-        [pytest]
-        junit_family = xunit1
-    """
-    )
-    testdir.makepyfile(
-        """
-        import pytest
-
-        @pytest.fixture
-        def other(record_xml_attribute):
-            record_xml_attribute("bar", 1)
-        def test_record(record_xml_attribute, other):
-            record_xml_attribute("foo", "<1");
-    """
-    )
-    result, dom = run_and_parse()
-    node = dom.find_first_by_tag("testsuite")
-    tnode = node.find_first_by_tag("testcase")
-    tnode.assert_attr(bar="1")
-    tnode.assert_attr(foo="<1")
-    result.stdout.fnmatch_lines(
-        ["*test_record_attribute.py:6:*record_xml_attribute is an experimental feature"]
-    )
-
-
-@pytest.mark.filterwarnings("default")
-@pytest.mark.parametrize("fixture_name", ["record_xml_attribute", "record_property"])
-def test_record_fixtures_xunit2(testdir, fixture_name, run_and_parse):
-    """Ensure record_xml_attribute and record_property drop values when outside of legacy family
-    """
-    testdir.makeini(
-        """
-        [pytest]
-        junit_family = xunit2
-    """
-    )
-    testdir.makepyfile(
-        """
-        import pytest
-
-        @pytest.fixture
-        def other({fixture_name}):
-            {fixture_name}("bar", 1)
-        def test_record({fixture_name}, other):
-            {fixture_name}("foo", "<1");
-    """.format(
-            fixture_name=fixture_name
-        )
-    )
-
-    result, dom = run_and_parse(family=None)
-    expected_lines = []
-    if fixture_name == "record_xml_attribute":
-        expected_lines.append(
-            "*test_record_fixtures_xunit2.py:6:*record_xml_attribute is an experimental feature"
-        )
-    expected_lines = [
-        "*test_record_fixtures_xunit2.py:6:*{fixture_name} is incompatible "
-        "with junit_family 'xunit2' (use 'legacy' or 'xunit1')".format(
-            fixture_name=fixture_name
-        )
-    ]
-    result.stdout.fnmatch_lines(expected_lines)
-
-
-def test_random_report_log_xdist(testdir, monkeypatch, run_and_parse):
-    """xdist calls pytest_runtest_logreport as they are executed by the workers,
-    with nodes from several nodes overlapping, so junitxml must cope with that
-    to produce correct reports. #1064
-    """
-    pytest.importorskip("xdist")
-    monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
-    testdir.makepyfile(
-        """
-        import pytest, time
-        @pytest.mark.parametrize('i', list(range(30)))
-        def test_x(i):
-            assert i != 22
-    """
-    )
-    _, dom = run_and_parse("-n2")
-    suite_node = dom.find_first_by_tag("testsuite")
-    failed = []
-    for case_node in suite_node.find_by_tag("testcase"):
-        if case_node.find_first_by_tag("failure"):
-            failed.append(case_node["name"])
-
-    assert failed == ["test_x[22]"]
-
-
-@parametrize_families
-def test_root_testsuites_tag(testdir, run_and_parse, xunit_family):
-    testdir.makepyfile(
-        """
-        def test_x():
-            pass
-    """
-    )
-    _, dom = run_and_parse(family=xunit_family)
-    root = dom.get_unique_child
-    assert root.tag == "testsuites"
-    suite_node = root.get_unique_child
-    assert suite_node.tag == "testsuite"
-
-
-def test_runs_twice(testdir, run_and_parse):
-    f = testdir.makepyfile(
-        """
-        def test_pass():
-            pass
-    """
-    )
-
-    result, dom = run_and_parse(f, f)
-    result.stdout.no_fnmatch_line("*INTERNALERROR*")
-    first, second = [x["classname"] for x in dom.find_by_tag("testcase")]
-    assert first == second
-
-
-def test_runs_twice_xdist(testdir, run_and_parse):
-    pytest.importorskip("xdist")
-    testdir.monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD")
-    f = testdir.makepyfile(
-        """
-        def test_pass():
-            pass
-    """
-    )
-
-    result, dom = run_and_parse(f, "--dist", "each", "--tx", "2*popen")
-    result.stdout.no_fnmatch_line("*INTERNALERROR*")
-    first, second = [x["classname"] for x in dom.find_by_tag("testcase")]
-    assert first == second
-
-
-def test_fancy_items_regression(testdir, run_and_parse):
-    # issue 1259
-    testdir.makeconftest(
-        """
-        import pytest
-        class FunItem(pytest.Item):
-            def runtest(self):
-                pass
-        class NoFunItem(pytest.Item):
-            def runtest(self):
-                pass
-
-        class FunCollector(pytest.File):
-            def collect(self):
-                return [
-                    FunItem('a', self),
-                    NoFunItem('a', self),
-                    NoFunItem('b', self),
-                ]
-
-        def pytest_collect_file(path, parent):
-            if path.check(ext='.py'):
-                return FunCollector(path, parent)
-    """
-    )
-
-    testdir.makepyfile(
-        """
-        def test_pass():
-            pass
-    """
-    )
-
-    result, dom = run_and_parse()
-
-    result.stdout.no_fnmatch_line("*INTERNALERROR*")
-
-    items = sorted("%(classname)s %(name)s" % x for x in dom.find_by_tag("testcase"))
-    import pprint
-
-    pprint.pprint(items)
-    assert items == [
-        "conftest a",
-        "conftest a",
-        "conftest b",
-        "test_fancy_items_regression a",
-        "test_fancy_items_regression a",
-        "test_fancy_items_regression b",
-        "test_fancy_items_regression test_pass",
-    ]
-
-
-@parametrize_families
-def test_global_properties(testdir, xunit_family) -> None:
-    path = testdir.tmpdir.join("test_global_properties.xml")
-    log = LogXML(str(path), None, family=xunit_family)
-
-    class Report(BaseReport):
-        sections = []  # type: List[Tuple[str, str]]
-        nodeid = "test_node_id"
-
-    log.pytest_sessionstart()
-    log.add_global_property("foo", "1")
-    log.add_global_property("bar", "2")
-    log.pytest_sessionfinish()
-
-    dom = minidom.parse(str(path))
-
-    properties = dom.getElementsByTagName("properties")
-
-    assert properties.length == 1, "There must be one <properties> node"
-
-    property_list = dom.getElementsByTagName("property")
-
-    assert property_list.length == 2, "There most be only 2 property nodes"
-
-    expected = {"foo": "1", "bar": "2"}
-    actual = {}
-
-    for p in property_list:
-        k = str(p.getAttribute("name"))
-        v = str(p.getAttribute("value"))
-        actual[k] = v
-
-    assert actual == expected
-
-
-def test_url_property(testdir) -> None:
-    test_url = "http://www.github.com/pytest-dev"
-    path = testdir.tmpdir.join("test_url_property.xml")
-    log = LogXML(str(path), None)
-
-    class Report(BaseReport):
-        longrepr = "FooBarBaz"
-        sections = []  # type: List[Tuple[str, str]]
-        nodeid = "something"
-        location = "tests/filename.py", 42, "TestClass.method"
-        url = test_url
-
-    test_report = cast(TestReport, Report())
-
-    log.pytest_sessionstart()
-    node_reporter = log._opentestcase(test_report)
-    node_reporter.append_failure(test_report)
-    log.pytest_sessionfinish()
-
-    test_case = minidom.parse(str(path)).getElementsByTagName("testcase")[0]
-
-    assert (
-        test_case.getAttribute("url") == test_url
-    ), "The URL did not get written to the xml"
-
-
-@parametrize_families
-def test_record_testsuite_property(testdir, run_and_parse, xunit_family):
-    testdir.makepyfile(
-        """
-        def test_func1(record_testsuite_property):
-            record_testsuite_property("stats", "all good")
-
-        def test_func2(record_testsuite_property):
-            record_testsuite_property("stats", 10)
-    """
-    )
-    result, dom = run_and_parse(family=xunit_family)
-    assert result.ret == 0
-    node = dom.find_first_by_tag("testsuite")
-    properties_node = node.find_first_by_tag("properties")
-    p1_node = properties_node.find_nth_by_tag("property", 0)
-    p2_node = properties_node.find_nth_by_tag("property", 1)
-    p1_node.assert_attr(name="stats", value="all good")
-    p2_node.assert_attr(name="stats", value="10")
-
-
-def test_record_testsuite_property_junit_disabled(testdir):
-    testdir.makepyfile(
-        """
-        def test_func1(record_testsuite_property):
-            record_testsuite_property("stats", "all good")
-    """
-    )
-    result = testdir.runpytest()
-    assert result.ret == 0
-
-
-@pytest.mark.parametrize("junit", [True, False])
-def test_record_testsuite_property_type_checking(testdir, junit):
-    testdir.makepyfile(
-        """
-        def test_func1(record_testsuite_property):
-            record_testsuite_property(1, 2)
-    """
-    )
-    args = ("--junitxml=tests.xml",) if junit else ()
-    result = testdir.runpytest(*args)
-    assert result.ret == 1
-    result.stdout.fnmatch_lines(
-        ["*TypeError: name parameter needs to be a string, but int given"]
-    )
-
-
-@pytest.mark.parametrize("suite_name", ["my_suite", ""])
-@parametrize_families
-def test_set_suite_name(testdir, suite_name, run_and_parse, xunit_family):
-    if suite_name:
-        testdir.makeini(
-            """
-            [pytest]
-            junit_suite_name={suite_name}
-            junit_family={family}
-        """.format(
-                suite_name=suite_name, family=xunit_family
-            )
-        )
-        expected = suite_name
-    else:
-        expected = "pytest"
-    testdir.makepyfile(
-        """
-        import pytest
-
-        def test_func():
-            pass
-    """
-    )
-    result, dom = run_and_parse(family=xunit_family)
-    assert result.ret == 0
-    node = dom.find_first_by_tag("testsuite")
-    node.assert_attr(name=expected)
-
-
-def test_escaped_skipreason_issue3533(testdir, run_and_parse):
-    testdir.makepyfile(
-        """
-        import pytest
-        @pytest.mark.skip(reason='1 <> 2')
-        def test_skip():
-            pass
-    """
-    )
-    _, dom = run_and_parse()
-    node = dom.find_first_by_tag("testcase")
-    snode = node.find_first_by_tag("skipped")
-    assert "1 <> 2" in snode.text
-    snode.assert_attr(message="1 <> 2")
-
-
-@parametrize_families
-def test_logging_passing_tests_disabled_does_not_log_test_output(
-    testdir, run_and_parse, xunit_family
-):
-    testdir.makeini(
-        """
-        [pytest]
-        junit_log_passing_tests=False
-        junit_logging=system-out
-        junit_family={family}
-    """.format(
-            family=xunit_family
-        )
-    )
-    testdir.makepyfile(
-        """
-        import pytest
-        import logging
-        import sys
-
-        def test_func():
-            sys.stdout.write('This is stdout')
-            sys.stderr.write('This is stderr')
-            logging.warning('hello')
-    """
-    )
-    result, dom = run_and_parse(family=xunit_family)
-    assert result.ret == 0
-    node = dom.find_first_by_tag("testcase")
-    assert len(node.find_by_tag("system-err")) == 0
-    assert len(node.find_by_tag("system-out")) == 0
-
-
-@parametrize_families
-@pytest.mark.parametrize("junit_logging", ["no", "system-out", "system-err"])
-def test_logging_passing_tests_disabled_logs_output_for_failing_test_issue5430(
-    testdir, junit_logging, run_and_parse, xunit_family
-):
-    testdir.makeini(
-        """
-        [pytest]
-        junit_log_passing_tests=False
-        junit_family={family}
-    """.format(
-            family=xunit_family
-        )
-    )
-    testdir.makepyfile(
-        """
-        import pytest
-        import logging
-        import sys
-
-        def test_func():
-            logging.warning('hello')
-            assert 0
-    """
-    )
-    result, dom = run_and_parse(
-        "-o", "junit_logging=%s" % junit_logging, family=xunit_family
-    )
-    assert result.ret == 1
-    node = dom.find_first_by_tag("testcase")
-    if junit_logging == "system-out":
-        assert len(node.find_by_tag("system-err")) == 0
-        assert len(node.find_by_tag("system-out")) == 1
-    elif junit_logging == "system-err":
-        assert len(node.find_by_tag("system-err")) == 1
-        assert len(node.find_by_tag("system-out")) == 0
-    else:
-        assert junit_logging == "no"
-        assert len(node.find_by_tag("system-err")) == 0
-        assert len(node.find_by_tag("system-out")) == 0
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -e .
Obtaining file:///testbed
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: attrs>=17.4.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==6.0.0rc2.dev34+gcd4beeff4.d20251003) (23.1.0)
Requirement already satisfied: iniconfig in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==6.0.0rc2.dev34+gcd4beeff4.d20251003) (2.0.0)
Requirement already satisfied: more-itertools>=4.0.0 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==6.0.0rc2.dev34+gcd4beeff4.d20251003) (10.1.0)
Requirement already satisfied: packaging in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==6.0.0rc2.dev34+gcd4beeff4.d20251003) (23.1)
Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==6.0.0rc2.dev34+gcd4beeff4.d20251003) (0.13.1)
Requirement already satisfied: py>=1.8.2 in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==6.0.0rc2.dev34+gcd4beeff4.d20251003) (1.11.0)
Requirement already satisfied: toml in /opt/miniconda3/envs/testbed/lib/python3.9/site-packages (from pytest==6.0.0rc2.dev34+gcd4beeff4.d20251003) (0.10.2)
Building wheels for collected packages: pytest
  Building editable for pytest (pyproject.toml): started
  Building editable for pytest (pyproject.toml): finished with status 'done'
  Created wheel for pytest: filename=pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003-0.editable-py3-none-any.whl size=5339 sha256=84880bddeb9e4ef35f1557194730ceae0696c985f32119d26641227202b0e8e4
  Stored in directory: /tmp/pip-ephem-wheel-cache-i9jr3jno/wheels/7d/66/67/70d1ee2124ccf21d601c352e25cdca10f611f7c8b3f9ffb9e4
Successfully built pytest
Installing collected packages: pytest
  Attempting uninstall: pytest
    Found existing installation: pytest 6.0.0rc2.dev33+g7f7a36478
    Uninstalling pytest-6.0.0rc2.dev33+g7f7a36478:
      Successfully uninstalled pytest-6.0.0rc2.dev33+g7f7a36478
Successfully installed pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
+ git checkout 7f7a36478abe7dd1fa993b115d22606aa0e35e88 testing/test_skipping.py
Updated 0 paths from 6966cf7b5
+ git apply -v -
Checking patch testing/test_skipping.py...
Applied patch testing/test_skipping.py cleanly.
+ : '>>>>> Start Test Output'
+ pytest -rA testing/test_skipping.py
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /testbed, configfile: pyproject.toml
collected 81 items

testing/test_skipping.py ............................FF................. [ 58%]
..................................                                       [100%]

=================================== FAILURES ===================================
____________ TestXFail.test_dynamic_xfail_set_during_runtest_failed ____________

self = <test_skipping.TestXFail object at 0x7654dc44fe20>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_dynamic_xfail_set_during_runtest_failed0')>

    def test_dynamic_xfail_set_during_runtest_failed(self, testdir: Testdir) -> None:
        # Issue #7486.
        p = testdir.makepyfile(
            """
            import pytest
            def test_this(request):
                request.node.add_marker(pytest.mark.xfail(reason="xfail"))
                assert 0
        """
        )
        result = testdir.runpytest(p)
>       result.assert_outcomes(xfailed=1)
E       AssertionError: assert {'errors': 0,...pped': 0, ...} == {'errors': 0,...pped': 0, ...}
E         Omitting 4 identical items, use -vv to show
E         Differing items:
E         {'failed': 1} != {'failed': 0}
E         {'xfailed': 0} != {'xfailed': 1}
E         Use -v to get the full diff

/testbed/testing/test_skipping.py:440: AssertionError
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_dynamic_xfail_set_during_runtest_failed0
collected 1 item

test_dynamic_xfail_set_during_runtest_failed.py F                        [100%]

=================================== FAILURES ===================================
__________________________________ test_this ___________________________________

request = <FixtureRequest for <Function test_this>>

    def test_this(request):
        request.node.add_marker(pytest.mark.xfail(reason="xfail"))
>       assert 0
E       assert 0

test_dynamic_xfail_set_during_runtest_failed.py:4: AssertionError
=========================== short test summary info ============================
FAILED test_dynamic_xfail_set_during_runtest_failed.py::test_this - assert 0
============================== 1 failed in 0.01s ===============================
________ TestXFail.test_dynamic_xfail_set_during_runtest_passed_strict _________

self = <test_skipping.TestXFail object at 0x7654dc3aac40>
testdir = <Testdir local('/tmp/pytest-of-root/pytest-0/test_dynamic_xfail_set_during_runtest_passed_strict0')>

    def test_dynamic_xfail_set_during_runtest_passed_strict(
        self, testdir: Testdir
    ) -> None:
        # Issue #7486.
        p = testdir.makepyfile(
            """
            import pytest
            def test_this(request):
                request.node.add_marker(pytest.mark.xfail(reason="xfail", strict=True))
        """
        )
        result = testdir.runpytest(p)
>       result.assert_outcomes(failed=1)
E       AssertionError: assert {'errors': 0,...pped': 0, ...} == {'errors': 0,...pped': 0, ...}
E         Omitting 4 identical items, use -vv to show
E         Differing items:
E         {'passed': 1} != {'passed': 0}
E         {'failed': 0} != {'failed': 1}
E         Use -v to get the full diff

/testbed/testing/test_skipping.py:454: AssertionError
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_dynamic_xfail_set_during_runtest_passed_strict0
collected 1 item

test_dynamic_xfail_set_during_runtest_passed_strict.py .                 [100%]

============================== 1 passed in 0.00s ===============================
==================================== PASSES ====================================
________________________ TestEvaluation.test_no_marker _________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_no_marker0
collected 0 items

============================ no tests ran in 0.00s =============================
___________________ TestEvaluation.test_marked_xfail_no_args ___________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_xfail_no_args0
collected 0 items

============================ no tests ran in 0.00s =============================
__________________ TestEvaluation.test_marked_skipif_no_args ___________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_skipif_no_args0
collected 0 items

============================ no tests ran in 0.00s =============================
______________________ TestEvaluation.test_marked_one_arg ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_one_arg0
collected 0 items

============================ no tests ran in 0.00s =============================
________________ TestEvaluation.test_marked_one_arg_with_reason ________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_one_arg_with_reason0
collected 0 items

============================ no tests ran in 0.00s =============================
___________________ TestEvaluation.test_marked_one_arg_twice ___________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_one_arg_twice0
collected 0 items

============================ no tests ran in 0.00s =============================
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_one_arg_twice0
collected 0 items

============================ no tests ran in 0.00s =============================
__________________ TestEvaluation.test_marked_one_arg_twice2 ___________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_one_arg_twice20
collected 0 items

============================ no tests ran in 0.00s =============================
________ TestEvaluation.test_marked_skipif_with_boolean_without_reason _________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_skipif_with_boolean_without_reason0
collected 0 items

============================ no tests ran in 0.00s =============================
____________ TestEvaluation.test_marked_skipif_with_invalid_boolean ____________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_marked_skipif_with_invalid_boolean0
collected 0 items

============================ no tests ran in 0.01s =============================
_______________________ TestEvaluation.test_skipif_class _______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_class0
collected 0 items

============================ no tests ran in 0.00s =============================
______________________ TestXFail.test_xfail_simple[True] _______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_simple0
collected 0 items

============================ no tests ran in 0.00s =============================
______________________ TestXFail.test_xfail_simple[False] ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_simple1
collected 0 items

============================ no tests ran in 0.00s =============================
_________________________ TestXFail.test_xfail_xpassed _________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_xpassed0
collected 0 items

============================ no tests ran in 0.00s =============================
_____________________ TestXFail.test_xfail_using_platform ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_using_platform0
collected 0 items

============================ no tests ran in 0.00s =============================
_____________________ TestXFail.test_xfail_xpassed_strict ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_xpassed_strict0
collected 0 items

============================ no tests ran in 0.00s =============================
_______________________ TestXFail.test_xfail_run_anyway ________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_run_anyway0
collected 2 items

test_xfail_run_anyway.py F.                                              [100%]

=================================== FAILURES ===================================
__________________________________ test_func ___________________________________

    @pytest.mark.xfail
    def test_func():
>       assert 0
E       assert 0

test_xfail_run_anyway.py:4: AssertionError
=========================== short test summary info ============================
FAILED test_xfail_run_anyway.py::test_func - assert 0
========================= 1 failed, 1 passed in 0.01s ==========================
________ TestXFail.test_xfail_run_with_skip_mark[test_input0-expected0] ________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_run_with_skip_mark0
collected 1 item

test_sample.py s                                                         [100%]

=========================== short test summary info ============================
SKIPPED [1] test_sample.py:2: unconditional skip
============================== 1 skipped in 0.00s ==============================
________ TestXFail.test_xfail_run_with_skip_mark[test_input1-expected1] ________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_run_with_skip_mark1
collected 1 item

test_sample.py s                                                         [100%]

=========================== short test summary info ============================
SKIPPED [1] test_sample.py:2: unconditional skip
============================== 1 skipped in 0.01s ==============================
___________________ TestXFail.test_xfail_evalfalse_but_fails ___________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_evalfalse_but_fails0
collected 0 items

============================ no tests ran in 0.00s =============================
___________________ TestXFail.test_xfail_not_report_default ____________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1 -- /opt/miniconda3/envs/testbed/bin/python
cachedir: .pytest_cache
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_not_report_default0
collecting ... collected 1 item

test_one.py::test_this XFAIL                                             [100%]

============================== 1 xfailed in 0.01s ==============================
_________________ TestXFail.test_xfail_not_run_xfail_reporting _________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_not_run_xfail_reporting0
collected 3 items

test_one.py xx.                                                          [100%]

=========================== short test summary info ============================
XFAIL test_one.py::test_this
  reason: [NOTRUN] noway
XFAIL test_one.py::test_this_true
  reason: [NOTRUN] condition: True
========================= 1 passed, 2 xfailed in 0.06s =========================
__________________ TestXFail.test_xfail_not_run_no_setup_run ___________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_not_run_no_setup_run0
collected 1 item

test_one.py x                                                            [100%]

=========================== short test summary info ============================
XFAIL test_one.py::test_this
  reason: [NOTRUN] hello
============================== 1 xfailed in 0.03s ==============================
__________________________ TestXFail.test_xfail_xpass __________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_xpass0
collected 1 item

test_one.py X                                                            [100%]

=========================== short test summary info ============================
XPASS test_one.py::test_that 
============================== 1 xpassed in 0.00s ==============================
_______________________ TestXFail.test_xfail_imperative ________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_imperative0
collected 1 item

test_xfail_imperative.py x                                               [100%]

============================== 1 xfailed in 0.00s ==============================
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_imperative0
collected 1 item

test_xfail_imperative.py x                                               [100%]

=========================== short test summary info ============================
XFAIL test_xfail_imperative.py::test_this
  reason: hello
============================== 1 xfailed in 0.00s ==============================
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_imperative0
collected 1 item

test_xfail_imperative.py .                                               [100%]

============================== 1 passed in 0.00s ===============================
______________ TestXFail.test_xfail_imperative_in_setup_function _______________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_imperative_in_setup_function0
collected 1 item

test_xfail_imperative_in_setup_function.py x                             [100%]

============================== 1 xfailed in 0.01s ==============================
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_imperative_in_setup_function0
collected 1 item

test_xfail_imperative_in_setup_function.py x                             [100%]

=========================== short test summary info ============================
XFAIL test_xfail_imperative_in_setup_function.py::test_this
  reason: hello
============================== 1 xfailed in 0.03s ==============================
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_imperative_in_setup_function0
collected 1 item

test_xfail_imperative_in_setup_function.py F                             [100%]

=================================== FAILURES ===================================
__________________________________ test_this ___________________________________

    def test_this():
>       assert 0
E       assert 0

test_xfail_imperative_in_setup_function.py:6: AssertionError
=========================== short test summary info ============================
FAILED test_xfail_imperative_in_setup_function.py::test_this - assert 0
============================== 1 failed in 0.00s ===============================
_____________________ TestXFail.test_dynamic_xfail_no_run ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_dynamic_xfail_no_run0
collected 1 item

test_dynamic_xfail_no_run.py x                                           [100%]

=========================== short test summary info ============================
XFAIL test_dynamic_xfail_no_run.py::test_this
  reason: [NOTRUN] 
============================== 1 xfailed in 0.03s ==============================
____________ TestXFail.test_dynamic_xfail_set_during_funcarg_setup _____________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_dynamic_xfail_set_during_funcarg_setup0
collected 1 item

test_dynamic_xfail_set_during_funcarg_setup.py x                         [100%]

============================== 1 xfailed in 0.01s ==============================
_________ TestXFail.test_xfail_raises[TypeError-TypeError-*1 xfailed*] _________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_raises0
collected 1 item

test_xfail_raises.py x                                                   [100%]

============================== 1 xfailed in 0.01s ==============================
_ TestXFail.test_xfail_raises[(AttributeError, TypeError)-TypeError-*1 xfailed*] _
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_raises1
collected 1 item

test_xfail_raises.py x                                                   [100%]

============================== 1 xfailed in 0.01s ==============================
_________ TestXFail.test_xfail_raises[TypeError-IndexError-*1 failed*] _________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_raises2
collected 1 item

test_xfail_raises.py F                                                   [100%]

=================================== FAILURES ===================================
_________________________________ test_raises __________________________________

    @pytest.mark.xfail(raises=TypeError)
    def test_raises():
>       raise IndexError()
E       IndexError

test_xfail_raises.py:4: IndexError
=========================== short test summary info ============================
FAILED test_xfail_raises.py::test_raises - IndexError
============================== 1 failed in 0.01s ===============================
_ TestXFail.test_xfail_raises[(AttributeError, TypeError)-IndexError-*1 failed*] _
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_raises3
collected 1 item

test_xfail_raises.py F                                                   [100%]

=================================== FAILURES ===================================
_________________________________ test_raises __________________________________

    @pytest.mark.xfail(raises=(AttributeError, TypeError))
    def test_raises():
>       raise IndexError()
E       IndexError

test_xfail_raises.py:4: IndexError
=========================== short test summary info ============================
FAILED test_xfail_raises.py::test_raises - IndexError
============================== 1 failed in 0.01s ===============================
_________________________ TestXFail.test_strict_sanity _________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_strict_sanity0
collected 1 item

test_strict_sanity.py x                                                  [100%]

=========================== short test summary info ============================
XFAIL test_strict_sanity.py::test_foo
  unsupported feature
============================== 1 xfailed in 0.01s ==============================
______________________ TestXFail.test_strict_xfail[True] _______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_strict_xfail0
collected 1 item

test_strict_xfail.py F                                                   [100%]

=================================== FAILURES ===================================
___________________________________ test_foo ___________________________________
[XPASS(strict)] unsupported feature
============================== 1 failed in 0.00s ===============================
______________________ TestXFail.test_strict_xfail[False] ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_strict_xfail1
collected 1 item

test_strict_xfail.py X                                                   [100%]

=========================== short test summary info ============================
XPASS test_strict_xfail.py::test_foo unsupported feature
============================== 1 xpassed in 0.01s ==============================
_________________ TestXFail.test_strict_xfail_condition[True] __________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_strict_xfail_condition0
collected 1 item

test_strict_xfail_condition.py .                                         [100%]

============================== 1 passed in 0.01s ===============================
_________________ TestXFail.test_strict_xfail_condition[False] _________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_strict_xfail_condition1
collected 1 item

test_strict_xfail_condition.py .                                         [100%]

============================== 1 passed in 0.00s ===============================
_________________ TestXFail.test_xfail_condition_keyword[True] _________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_condition_keyword0
collected 1 item

test_xfail_condition_keyword.py .                                        [100%]

============================== 1 passed in 0.01s ===============================
________________ TestXFail.test_xfail_condition_keyword[False] _________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_condition_keyword1
collected 1 item

test_xfail_condition_keyword.py .                                        [100%]

============================== 1 passed in 0.00s ===============================
_____________ TestXFail.test_strict_xfail_default_from_file[true] ______________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_strict_xfail_default_from_file0, configfile: tox.ini
collected 1 item

test_strict_xfail_default_from_file.py F                                 [100%]

=================================== FAILURES ===================================
___________________________________ test_foo ___________________________________
[XPASS(strict)] unsupported feature
============================== 1 failed in 0.00s ===============================
_____________ TestXFail.test_strict_xfail_default_from_file[false] _____________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_strict_xfail_default_from_file1, configfile: tox.ini
collected 1 item

test_strict_xfail_default_from_file.py X                                 [100%]

=========================== short test summary info ============================
XPASS test_strict_xfail_default_from_file.py::test_foo unsupported feature
============================== 1 xpassed in 0.00s ==============================
_____________ TestXFailwithSetupTeardown.test_failing_setup_issue9 _____________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_failing_setup_issue90
collected 1 item

test_failing_setup_issue9.py x                                           [100%]

============================== 1 xfailed in 0.01s ==============================
___________ TestXFailwithSetupTeardown.test_failing_teardown_issue9 ____________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_failing_teardown_issue90
collected 1 item

test_failing_teardown_issue9.py Xx                                       [100%]

======================== 1 xfailed, 1 xpassed in 0.05s =========================
___________________________ TestSkip.test_skip_class ___________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skip_class0
collected 3 items

test_skip_class.py ss.                                                   [100%]

========================= 1 passed, 2 skipped in 0.01s =========================
_____________________ TestSkip.test_skips_on_false_string ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skips_on_false_string0
collected 1 item

test_skips_on_false_string.py s                                          [100%]

============================== 1 skipped in 0.00s ==============================
_________________________ TestSkip.test_arg_as_reason __________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_arg_as_reason0
collected 1 item

test_arg_as_reason.py s                                                  [100%]

=========================== short test summary info ============================
SKIPPED [1] test_arg_as_reason.py:2: testing stuff
============================== 1 skipped in 0.01s ==============================
_________________________ TestSkip.test_skip_no_reason _________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skip_no_reason0
collected 1 item

test_skip_no_reason.py s                                                 [100%]

=========================== short test summary info ============================
SKIPPED [1] test_skip_no_reason.py:2: unconditional skip
============================== 1 skipped in 0.01s ==============================
________________________ TestSkip.test_skip_with_reason ________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skip_with_reason0
collected 1 item

test_skip_with_reason.py s                                               [100%]

=========================== short test summary info ============================
SKIPPED [1] test_skip_with_reason.py:2: for lolz
============================== 1 skipped in 0.00s ==============================
_____________________ TestSkip.test_only_skips_marked_test _____________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_only_skips_marked_test0
collected 3 items

test_only_skips_marked_test.py ss.                                       [100%]

=========================== short test summary info ============================
SKIPPED [1] test_only_skips_marked_test.py:2: unconditional skip
SKIPPED [1] test_only_skips_marked_test.py:5: nothing in particular
========================= 1 passed, 2 skipped in 0.01s =========================
________________________ TestSkip.test_strict_and_skip _________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_strict_and_skip0
collected 1 item

test_strict_and_skip.py s                                                [100%]

=========================== short test summary info ============================
SKIPPED [1] test_strict_and_skip.py:2: unconditional skip
============================== 1 skipped in 0.01s ==============================
______________________ TestSkipif.test_skipif_conditional ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_conditional0
collected 0 items

============================ no tests ran in 0.00s =============================
_________ TestSkipif.test_skipif_reporting["hasattr(sys, 'platform')"] _________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_reporting0
collected 1 item

test_foo.py s

=========================== short test summary info ============================
SKIPPED [1] test_foo.py:2: condition: hasattr(sys, 'platform')
============================== 1 skipped in 0.01s ==============================
______ TestSkipif.test_skipif_reporting[True, reason="invalid platform"] _______
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_reporting1
collected 1 item

test_foo.py s

=========================== short test summary info ============================
SKIPPED [1] test_foo.py:2: invalid platform
============================== 1 skipped in 0.00s ==============================
____________________ TestSkipif.test_skipif_using_platform _____________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_using_platform0
collected 0 items

============================ no tests ran in 0.00s =============================
________ TestSkipif.test_skipif_reporting_multiple[skipif-SKIP-skipped] ________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_reporting_multiple0
collected 1 item

test_foo.py s

=========================== short test summary info ============================
SKIPPED [1] test_foo.py:2: second_condition
============================== 1 skipped in 0.00s ==============================
________ TestSkipif.test_skipif_reporting_multiple[xfail-XPASS-xpassed] ________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_reporting_multiple1
collected 1 item

test_foo.py X

=========================== short test summary info ============================
XPASS test_foo.py::test_foobar second_condition
============================== 1 xpassed in 0.00s ==============================
_________________________ test_skip_not_report_default _________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1 -- /opt/miniconda3/envs/testbed/bin/python
cachedir: .pytest_cache
rootdir: /tmp/pytest-of-root/pytest-0/test_skip_not_report_default0
collecting ... collected 1 item

test_one.py::test_this SKIPPED                                           [100%]

============================== 1 skipped in 0.02s ==============================
______________________________ test_skipif_class _______________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_class1
collected 2 items

test_skipif_class.py ss                                                  [100%]

============================== 2 skipped in 0.01s ==============================
_______________________ test_skipped_reasons_functional ________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipped_reasons_functional0
collected 3 items

test_one.py sss                                                          [100%]

=========================== short test summary info ============================
SKIPPED [2] conftest.py:4: test
SKIPPED [1] test_one.py:14: via_decorator
============================== 3 skipped in 0.01s ==============================
_____________________________ test_skipped_folding _____________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipped_folding0
collected 2 items

test_one.py ss                                                           [100%]

=========================== short test summary info ============================
SKIPPED [2] test_one.py: Folding
============================== 2 skipped in 0.01s ==============================
_______________________________ test_reportchars _______________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_reportchars0
collected 4 items

test_reportchars.py FxXs                                                 [100%]

=================================== FAILURES ===================================
____________________________________ test_1 ____________________________________

    def test_1():
>       assert 0
E       assert 0

test_reportchars.py:3: AssertionError
=========================== short test summary info ============================
FAILED test_reportchars.py::test_1 - assert 0
XFAIL test_reportchars.py::test_2
XPASS test_reportchars.py::test_3 
SKIPPED [1] test_reportchars.py:11: four
============== 1 failed, 1 skipped, 1 xfailed, 1 xpassed in 0.01s ==============
____________________________ test_reportchars_error ____________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_reportchars_error0
collected 1 item

test_simple.py .E                                                        [100%]

==================================== ERRORS ====================================
________________________ ERROR at teardown of test_foo _________________________

    def pytest_runtest_teardown():
>       assert 0
E       assert 0

conftest.py:2: AssertionError
=========================== short test summary info ============================
ERROR test_simple.py::test_foo - assert 0
========================== 1 passed, 1 error in 0.01s ==========================
_____________________________ test_reportchars_all _____________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_reportchars_all0
collected 5 items

test_reportchars_all.py FxXsE                                            [100%]

==================================== ERRORS ====================================
___________________________ ERROR at setup of test_5 ___________________________

    @pytest.fixture
    def fail():
>       assert 0
E       assert 0

test_reportchars_all.py:14: AssertionError
=================================== FAILURES ===================================
____________________________________ test_1 ____________________________________

    def test_1():
>       assert 0
E       assert 0

test_reportchars_all.py:3: AssertionError
=========================== short test summary info ============================
SKIPPED [1] test_reportchars_all.py:11: four
XFAIL test_reportchars_all.py::test_2
XPASS test_reportchars_all.py::test_3 
ERROR test_reportchars_all.py::test_5 - assert 0
FAILED test_reportchars_all.py::test_1 - assert 0
========= 1 failed, 1 skipped, 1 xfailed, 1 xpassed, 1 error in 0.01s ==========
__________________________ test_reportchars_all_error __________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_reportchars_all_error0
collected 1 item

test_simple.py .E                                                        [100%]

==================================== ERRORS ====================================
________________________ ERROR at teardown of test_foo _________________________

    def pytest_runtest_teardown():
>       assert 0
E       assert 0

conftest.py:2: AssertionError
=========================== short test summary info ============================
ERROR test_simple.py::test_foo - assert 0
========================== 1 passed, 1 error in 0.01s ==========================
____________________ test_errors_in_xfail_skip_expressions _____________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_errors_in_xfail_skip_expressions0
collected 3 items

test_errors_in_xfail_skip_expressions.py EE.                             [100%]

==================================== ERRORS ====================================
_______________________ ERROR at setup of test_nameerror _______________________
name 'asd' is not defined

During handling of the above exception, another exception occurred:
Error evaluating 'skipif' condition
    asd
NameError: name 'asd' is not defined
________________________ ERROR at setup of test_syntax _________________________
unexpected EOF while parsing (<xfail condition>, line 1)

During handling of the above exception, another exception occurred:
Error evaluating 'xfail' condition
    syntax error
            ^
SyntaxError: invalid syntax
=========================== short test summary info ============================
ERROR test_errors_in_xfail_skip_expressions.py::test_nameerror
ERROR test_errors_in_xfail_skip_expressions.py::test_syntax
========================= 1 passed, 2 errors in 0.01s ==========================
________________________ test_xfail_skipif_with_globals ________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_skipif_with_globals0
collected 2 items

test_xfail_skipif_with_globals.py sx                                     [100%]

=========================== short test summary info ============================
SKIPPED [1] test_xfail_skipif_with_globals.py:3: condition: x == 3
XFAIL test_xfail_skipif_with_globals.py::test_boolean
  condition: x == 3
======================== 1 skipped, 1 xfailed in 0.01s =========================
_____________________________ test_default_markers _____________________________
----------------------------- Captured stdout call -----------------------------
@pytest.mark.filterwarnings(warning): add a warning filter to the given test. see https://docs.pytest.org/en/stable/warnings.html#pytest-mark-filterwarnings 

@pytest.mark.skip(reason=None): skip the given test function with an optional reason. Example: skip(reason="no way of currently testing this") skips the test.

@pytest.mark.skipif(condition, ..., *, reason=...): skip the given test function if any of the conditions evaluate to True. Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif

@pytest.mark.xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): mark the test function as an expected failure if any of the conditions evaluate to True. Optionally specify a reason for better reporting and run=False if you don't even want to execute the test function. If only specific exception(s) are expected, you can list them in raises, and if the test fails in other ways, it will be reported as a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail

@pytest.mark.parametrize(argnames, argvalues): call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead to two calls of the decorated test function, one with arg1=1 and another with arg1=2.see https://docs.pytest.org/en/stable/parametrize.html for more info and examples.

@pytest.mark.usefixtures(fixturename1, fixturename2, ...): mark tests as needing all of the specified fixtures. see https://docs.pytest.org/en/stable/fixture.html#usefixtures 

@pytest.mark.tryfirst: mark a hook implementation function such that the plugin machinery will try to call it first/as early as possible.

@pytest.mark.trylast: mark a hook implementation function such that the plugin machinery will try to call it last/as late as possible.

_______________________ test_xfail_test_setup_exception ________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_test_setup_exception0
collected 1 item

test_xfail_test_setup_exception.py x                                     [100%]

============================== 1 xfailed in 0.00s ==============================
______________________ test_imperativeskip_on_xfail_test _______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_imperativeskip_on_xfail_test0
collected 2 items

test_imperativeskip_on_xfail_test.py ss                                  [100%]

=========================== short test summary info ============================
SKIPPED [1] conftest.py:3: abc
SKIPPED [1] test_imperativeskip_on_xfail_test.py:6: condition: True
============================== 2 skipped in 0.00s ==============================
_______________________ TestBooleanCondition.test_skipif _______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif0
collected 2 items

test_skipif.py s.                                                        [100%]

========================= 1 passed, 1 skipped in 0.00s =========================
__________________ TestBooleanCondition.test_skipif_noreason ___________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_skipif_noreason0
collected 1 item

test_skipif_noreason.py E                                                [100%]

==================================== ERRORS ====================================
_________________________ ERROR at setup of test_func __________________________
Error evaluating 'skipif': you need to specify reason=STRING when using booleans as conditions.
=============================== 1 error in 0.00s ===============================
_______________________ TestBooleanCondition.test_xfail ________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail0
collected 1 item

test_xfail.py x                                                          [100%]

=========================== short test summary info ============================
XFAIL test_xfail.py::test_func
  True123
============================== 1 xfailed in 0.01s ==============================
_______________________________ test_xfail_item ________________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_xfail_item0
collected 1 item

foo x                                                                    [100%]

=============================== warnings summary ===============================
conftest.py:9
  /tmp/pytest-of-root/pytest-0/test_xfail_item0/conftest.py:9: PytestDeprecationWarning: Direct construction of MyItem has been deprecated, please use MyItem.from_parent.
  See https://docs.pytest.org/en/stable/deprecations.html#node-construction-changed-to-node-from-parent for more details.
    return MyItem("foo", parent)

-- Docs: https://docs.pytest.org/en/stable/warnings.html
======================== 1 xfailed, 1 warning in 0.04s =========================
_________________________ test_module_level_skip_error _________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_module_level_skip_error0
collected 0 items / 1 error

==================================== ERRORS ====================================
_______________ ERROR collecting test_module_level_skip_error.py _______________
Using pytest.skip outside of a test is not allowed. To decorate a test function, use the @pytest.mark.skip or @pytest.mark.skipif decorators instead, and to skip a module use `pytestmark = pytest.mark.{skip,skipif}.
=========================== short test summary info ============================
ERROR test_module_level_skip_error.py
!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
=============================== 1 error in 0.05s ===============================
________________ test_module_level_skip_with_allow_module_level ________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_module_level_skip_with_allow_module_level0
collected 0 items / 1 skipped

=========================== short test summary info ============================
SKIPPED [1] test_module_level_skip_with_allow_module_level.py:2: skip_module_level
============================== 1 skipped in 0.00s ==============================
_____________________ test_invalid_skip_keyword_parameter ______________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_invalid_skip_keyword_parameter0
collected 0 items / 1 error

==================================== ERRORS ====================================
___________ ERROR collecting test_invalid_skip_keyword_parameter.py ____________
test_invalid_skip_keyword_parameter.py:2: in <module>
    pytest.skip("skip_module_level", unknown=1)
E   TypeError: skip() got an unexpected keyword argument 'unknown'
=========================== short test summary info ============================
ERROR test_invalid_skip_keyword_parameter.py - TypeError: skip() got an unexp...
!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
=============================== 1 error in 0.04s ===============================
_____________________________ test_mark_xfail_item _____________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_mark_xfail_item0
collected 1 item

foo x                                                                    [100%]

=============================== warnings summary ===============================
conftest.py:14
  /tmp/pytest-of-root/pytest-0/test_mark_xfail_item0/conftest.py:14: PytestDeprecationWarning: Direct construction of MyItem has been deprecated, please use MyItem.from_parent.
  See https://docs.pytest.org/en/stable/deprecations.html#node-construction-changed-to-node-from-parent for more details.
    return MyItem("foo", parent)

-- Docs: https://docs.pytest.org/en/stable/warnings.html
======================== 1 xfailed, 1 warning in 0.03s =========================
________________________ test_summary_list_after_errors ________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_summary_list_after_errors0
collected 1 item

test_summary_list_after_errors.py F                                      [100%]

=================================== FAILURES ===================================
__________________________________ test_fail ___________________________________

    def test_fail():
>       assert 0
E       assert 0

test_summary_list_after_errors.py:3: AssertionError
=========================== short test summary info ============================
FAILED test_summary_list_after_errors.py::test_fail - assert 0
============================== 1 failed in 0.06s ===============================
_____________________________ test_relpath_rootdir _____________________________
----------------------------- Captured stdout call -----------------------------
============================= test session starts ==============================
platform linux -- Python 3.9.20, pytest-6.0.0rc2.dev34+gcd4beeff4.d20251003, py-1.11.0, pluggy-0.13.1
rootdir: /tmp/pytest-of-root/pytest-0/test_relpath_rootdir0/tests
collected 1 item

tests/test_1.py s                                                        [100%]

=========================== short test summary info ============================
SKIPPED [1] tests/test_1.py:2: unconditional skip
============================== 1 skipped in 0.00s ==============================
=========================== short test summary info ============================
PASSED testing/test_skipping.py::test_importorskip
PASSED testing/test_skipping.py::TestEvaluation::test_no_marker
PASSED testing/test_skipping.py::TestEvaluation::test_marked_xfail_no_args
PASSED testing/test_skipping.py::TestEvaluation::test_marked_skipif_no_args
PASSED testing/test_skipping.py::TestEvaluation::test_marked_one_arg
PASSED testing/test_skipping.py::TestEvaluation::test_marked_one_arg_with_reason
PASSED testing/test_skipping.py::TestEvaluation::test_marked_one_arg_twice
PASSED testing/test_skipping.py::TestEvaluation::test_marked_one_arg_twice2
PASSED testing/test_skipping.py::TestEvaluation::test_marked_skipif_with_boolean_without_reason
PASSED testing/test_skipping.py::TestEvaluation::test_marked_skipif_with_invalid_boolean
PASSED testing/test_skipping.py::TestEvaluation::test_skipif_class
PASSED testing/test_skipping.py::TestXFail::test_xfail_simple[True]
PASSED testing/test_skipping.py::TestXFail::test_xfail_simple[False]
PASSED testing/test_skipping.py::TestXFail::test_xfail_xpassed
PASSED testing/test_skipping.py::TestXFail::test_xfail_using_platform
PASSED testing/test_skipping.py::TestXFail::test_xfail_xpassed_strict
PASSED testing/test_skipping.py::TestXFail::test_xfail_run_anyway
PASSED testing/test_skipping.py::TestXFail::test_xfail_run_with_skip_mark[test_input0-expected0]
PASSED testing/test_skipping.py::TestXFail::test_xfail_run_with_skip_mark[test_input1-expected1]
PASSED testing/test_skipping.py::TestXFail::test_xfail_evalfalse_but_fails
PASSED testing/test_skipping.py::TestXFail::test_xfail_not_report_default
PASSED testing/test_skipping.py::TestXFail::test_xfail_not_run_xfail_reporting
PASSED testing/test_skipping.py::TestXFail::test_xfail_not_run_no_setup_run
PASSED testing/test_skipping.py::TestXFail::test_xfail_xpass
PASSED testing/test_skipping.py::TestXFail::test_xfail_imperative
PASSED testing/test_skipping.py::TestXFail::test_xfail_imperative_in_setup_function
PASSED testing/test_skipping.py::TestXFail::test_dynamic_xfail_no_run
PASSED testing/test_skipping.py::TestXFail::test_dynamic_xfail_set_during_funcarg_setup
PASSED testing/test_skipping.py::TestXFail::test_xfail_raises[TypeError-TypeError-*1 xfailed*]
PASSED testing/test_skipping.py::TestXFail::test_xfail_raises[(AttributeError, TypeError)-TypeError-*1 xfailed*]
PASSED testing/test_skipping.py::TestXFail::test_xfail_raises[TypeError-IndexError-*1 failed*]
PASSED testing/test_skipping.py::TestXFail::test_xfail_raises[(AttributeError, TypeError)-IndexError-*1 failed*]
PASSED testing/test_skipping.py::TestXFail::test_strict_sanity
PASSED testing/test_skipping.py::TestXFail::test_strict_xfail[True]
PASSED testing/test_skipping.py::TestXFail::test_strict_xfail[False]
PASSED testing/test_skipping.py::TestXFail::test_strict_xfail_condition[True]
PASSED testing/test_skipping.py::TestXFail::test_strict_xfail_condition[False]
PASSED testing/test_skipping.py::TestXFail::test_xfail_condition_keyword[True]
PASSED testing/test_skipping.py::TestXFail::test_xfail_condition_keyword[False]
PASSED testing/test_skipping.py::TestXFail::test_strict_xfail_default_from_file[true]
PASSED testing/test_skipping.py::TestXFail::test_strict_xfail_default_from_file[false]
PASSED testing/test_skipping.py::TestXFailwithSetupTeardown::test_failing_setup_issue9
PASSED testing/test_skipping.py::TestXFailwithSetupTeardown::test_failing_teardown_issue9
PASSED testing/test_skipping.py::TestSkip::test_skip_class
PASSED testing/test_skipping.py::TestSkip::test_skips_on_false_string
PASSED testing/test_skipping.py::TestSkip::test_arg_as_reason
PASSED testing/test_skipping.py::TestSkip::test_skip_no_reason
PASSED testing/test_skipping.py::TestSkip::test_skip_with_reason
PASSED testing/test_skipping.py::TestSkip::test_only_skips_marked_test
PASSED testing/test_skipping.py::TestSkip::test_strict_and_skip
PASSED testing/test_skipping.py::TestSkipif::test_skipif_conditional
PASSED testing/test_skipping.py::TestSkipif::test_skipif_reporting["hasattr(sys, 'platform')"]
PASSED testing/test_skipping.py::TestSkipif::test_skipif_reporting[True, reason="invalid platform"]
PASSED testing/test_skipping.py::TestSkipif::test_skipif_using_platform
PASSED testing/test_skipping.py::TestSkipif::test_skipif_reporting_multiple[skipif-SKIP-skipped]
PASSED testing/test_skipping.py::TestSkipif::test_skipif_reporting_multiple[xfail-XPASS-xpassed]
PASSED testing/test_skipping.py::test_skip_not_report_default
PASSED testing/test_skipping.py::test_skipif_class
PASSED testing/test_skipping.py::test_skipped_reasons_functional
PASSED testing/test_skipping.py::test_skipped_folding
PASSED testing/test_skipping.py::test_reportchars
PASSED testing/test_skipping.py::test_reportchars_error
PASSED testing/test_skipping.py::test_reportchars_all
PASSED testing/test_skipping.py::test_reportchars_all_error
PASSED testing/test_skipping.py::test_errors_in_xfail_skip_expressions
PASSED testing/test_skipping.py::test_xfail_skipif_with_globals
PASSED testing/test_skipping.py::test_default_markers
PASSED testing/test_skipping.py::test_xfail_test_setup_exception
PASSED testing/test_skipping.py::test_imperativeskip_on_xfail_test
PASSED testing/test_skipping.py::TestBooleanCondition::test_skipif
PASSED testing/test_skipping.py::TestBooleanCondition::test_skipif_noreason
PASSED testing/test_skipping.py::TestBooleanCondition::test_xfail
PASSED testing/test_skipping.py::test_xfail_item
PASSED testing/test_skipping.py::test_module_level_skip_error
PASSED testing/test_skipping.py::test_module_level_skip_with_allow_module_level
PASSED testing/test_skipping.py::test_invalid_skip_keyword_parameter
PASSED testing/test_skipping.py::test_mark_xfail_item
PASSED testing/test_skipping.py::test_summary_list_after_errors
PASSED testing/test_skipping.py::test_relpath_rootdir
FAILED testing/test_skipping.py::TestXFail::test_dynamic_xfail_set_during_runtest_failed
FAILED testing/test_skipping.py::TestXFail::test_dynamic_xfail_set_during_runtest_passed_strict
========================= 2 failed, 79 passed in 2.50s =========================
+ : '>>>>> End Test Output'
+ git checkout 7f7a36478abe7dd1fa993b115d22606aa0e35e88 testing/test_skipping.py
Updated 1 path from 6966cf7b5
