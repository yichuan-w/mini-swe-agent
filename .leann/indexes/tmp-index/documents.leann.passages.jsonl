{"id": "0", "text": "[CS 294-264] Agent Assignment\nDeadline \nFri, Oct 3rd, 2025, 11:59 p.m. PST \nThis is the only homework of the semester! \nWhy an Agent Assignment?\nWe believe this assignment will help you tackle harder, more realistic problems more quickly, and position us at the forefront of using AI tools to accelerate systems research.\nAlphaEvolve, OpenEvolve, and SATLUTION help to discover new algorithms by editing code.  They edit code instead of pseudocode because real code can be executed, tested, and verified for correctness.  However, the coding approaches used by such systems are pretty straightforward.  They just make a single call to an LLM with a prompt to edit the code.  Code modified by a single call to an LLM is restrictive:\nThe code generated may contain some minor errors that an agent can correct in a couple of iterations.\nThey can only handle small pieces of code\nThey are limited to Python\nTherefore, if we need to evolve an algorithm that spans across multiple files, possibly written in languages other than Python, we need to have a better AI-based code editor.", "metadata": {}}
{"id": "1", "text": "However, the coding approaches used by such systems are pretty straightforward.  They just make a single call to an LLM with a prompt to edit the code.  Code modified by a single call to an LLM is restrictive:\nThe code generated may contain some minor errors that an agent can correct in a couple of iterations.\nThey can only handle small pieces of code\nThey are limited to Python\nTherefore, if we need to evolve an algorithm that spans across multiple files, possibly written in languages other than Python, we need to have a better AI-based code editor.  Note that system code is often written in C/C++ for efficiency.  AI is not as practical for generating correct and safe code in C/C++ as it is in Python.\nThe assignment will ask you to write an effective and powerful agent that can address the above limitations of generating and editing code.  This, in turn, will enable you to evolve big systems projects that are currently beyond the reach of OpenEvolve.  The assignment will allow you to write an agent that can incorporate various advanced search techniques, such as parallel search, reflective prompt optimization, etc.  Hopefully, hands-on experience with these search techniques will allow you to rapidly evolve OpenEvolve.", "metadata": {}}
{"id": "2", "text": "AI is not as practical for generating correct and safe code in C/C++ as it is in Python.\nThe assignment will ask you to write an effective and powerful agent that can address the above limitations of generating and editing code.  This, in turn, will enable you to evolve big systems projects that are currently beyond the reach of OpenEvolve.  The assignment will allow you to write an agent that can incorporate various advanced search techniques, such as parallel search, reflective prompt optimization, etc.  Hopefully, hands-on experience with these search techniques will allow you to rapidly evolve OpenEvolve. \nIn the project, you will write a software engineering agent (SWE agent) that enables you to edit code in a repository given some natural language description about the edits. We decided to ask you to write an SWE agent because we have an excellent benchmark suite for SWE tasks, called SWE-Bench.  The benchmark has not been saturated.\nWhat is a ReAct Agent?", "metadata": {}}
{"id": "3", "text": "The assignment will allow you to write an agent that can incorporate various advanced search techniques, such as parallel search, reflective prompt optimization, etc.  Hopefully, hands-on experience with these search techniques will allow you to rapidly evolve OpenEvolve. \nIn the project, you will write a software engineering agent (SWE agent) that enables you to edit code in a repository given some natural language description about the edits. We decided to ask you to write an SWE agent because we have an excellent benchmark suite for SWE tasks, called SWE-Bench.  The benchmark has not been saturated.\nWhat is a ReAct Agent?\nA ReAct agent (short for Reasoning + Acting) is a type of agent that alternates between two steps:\nReasoning: The agent reasons in natural language about the problem, keeping track of what it knows, what it needs to do, and why.", "metadata": {}}
{"id": "4", "text": "We decided to ask you to write an SWE agent because we have an excellent benchmark suite for SWE tasks, called SWE-Bench.  The benchmark has not been saturated.\nWhat is a ReAct Agent?\nA ReAct agent (short for Reasoning + Acting) is a type of agent that alternates between two steps:\nReasoning: The agent reasons in natural language about the problem, keeping track of what it knows, what it needs to do, and why.\n\n\nActing: The agent calls an external tool (like a calculator, database, or shell command) to gather more information or take an action in the real world. \nThis loop continues until the agent reaches a final answer. The advantage of ReAct is that the agent doesn‚Äôt need to know everything internally ‚Äî it can reason step by step and use tools to extend its abilities. \nReAct Agent Specification \nProblem statement\nWrite a MINIMAL ReAct agent that solves software engineering tasks from SWE-Bench. \nImplement baseline, and improve it with optimizations \nYou can use any coding agent to complete the homework; however, you must own the code and understand it.", "metadata": {}}
{"id": "5", "text": "We provide the skeleton code in üëâ [GITHUB], follow the code and find TODO(student) on where you should implement things. \nMessage History Tree \nYour agent must:\nKeep a message history tree (tree dicts with role, content, timestamp, id, parent / children) \nrole: \"system\", \"user\", \"assistant\", ‚Äútool‚Äù, etc.\ncontent: the text content of the message (e.g. can be Markdown) \ntimestamp: when the message was created\nunique_id: unique identifier for each message (must be a counter starting at 1)\nchildren: list of pointers (unique_id) to the child messages in the history\nparent: pointer (unique_id) to the parent message", "metadata": {}}
{"id": "6", "text": "The structure of this tree will be: \nSystem prompt node \nThe root of the history tree is unique and contains the system prompt.\nA system prompt has three parts\nMain content is fixed: ‚ÄúYou are a Smart ReAct agent.‚Äù\nList of available tools (with docstrings)\nA tool description will contain the signature of the callable tool and its docstring describing its functionality. This should be constructed automatically.\nAn output format description \nUser prompt node \nRoot node has only one child node that contains the user prompt or the task description. \nInstruction node \nUser prompt node‚Äôs only child. \nRepresents the mandatory instructions for the agent to follow, and a list of ideas and insights the agent has learned so far. \nThe agent can update the content of the node by providing a tool to backtrack (see later) \nNOTE \nThe path from the root node (having no parent) to a leaf node (having no children) concatenated together forms the context of the agent.\nThe format of a message sent to the LLM is the following:\n\t\n------------------\n|MESSAGE(role=\"user\", id=97, step=9)|\nHello LLM.", "metadata": {}}
{"id": "7", "text": "Any node in the history tree (including the instruction node) can have multiple children.\nA child is added when the agent backtracks or continues from a node.\nNodes cannot be deleted, but their content can be modified.\nExample: update mandatory instructions or set content to an empty string (ignored in context).\nThe agent must maintain two pointers\nRoot node (system prompt)\nCurrent node (most recent message from LLM or tool)\nAgent Implementation \nA ReAct agent must be a class that implements a special function called \nrun(): str \nwhich will run the agent and return the final result as a string.  If the agent encounters any unhandled exception, it will be re-raised by the function.  \nThe class should also provide a method\n \tadd_functions(functions: List[Callable]) \nwhich will add the functions (i.e., tools) that the agent could call to get information from the environment. \nThe constructor of the agent class \n__init__(self, name: str, parser: ResponseParser)\nshould take a name and a response parser object.  You MUST NOT use XML or JSON format for function calls.", "metadata": {}}
{"id": "8", "text": "If the agent encounters any unhandled exception, it will be re-raised by the function.  \nThe class should also provide a method\n \tadd_functions(functions: List[Callable]) \nwhich will add the functions (i.e., tools) that the agent could call to get information from the environment. \nThe constructor of the agent class \n__init__(self, name: str, parser: ResponseParser)\nshould take a name and a response parser object.  You MUST NOT use XML or JSON format for function calls.  This is because the LLM must escape special XML and JSON characters, respectively, and some LLMs (such as Gemini) can make mistakes.  For example, a simple textual format could be\nLet me think step-by-step:\n...\n----FUNTION_CALL----\nname\n----ARG----\nname\nvalue\n.\n.\n.\n----ARG----\n.\n.\n.\n----FUNCTION_CALL_END----", "metadata": {}}
{"id": "9", "text": "The constructor of the agent class \n__init__(self, name: str, parser: ResponseParser)\nshould take a name and a response parser object.  You MUST NOT use XML or JSON format for function calls.  This is because the LLM must escape special XML and JSON characters, respectively, and some LLMs (such as Gemini) can make mistakes.  For example, a simple textual format could be\nLet me think step-by-step:\n...\n----FUNTION_CALL----\nname\n----ARG----\nname\nvalue\n.\n.\n.\n----ARG----\n.\n.\n.\n----FUNCTION_CALL_END----\n\nYou should use string rfind to extract the function call.  Note that an LLM response must end a function call.  rfind helps you skip the reasoning text from parsing. The LLM must produce a single function call at the end of its response.  You can use -‚Äî--FUNCTION_CALL_END‚Äì‚Äî- as the stopping token.\n\nTool Implementation \nThe following tools are implemented. \n\nimport subprocess\n\ndef run_bash_cmd(command: str, description: str):\n\"\"\"Run the command in a bash shell and return the output or throw a ValueError exception if the process returns non-zero exit code", "metadata": {}}
{"id": "10", "text": "You should use string rfind to extract the function call.  Note that an LLM response must end a function call.  rfind helps you skip the reasoning text from parsing. The LLM must produce a single function call at the end of its response.  You can use -‚Äî--FUNCTION_CALL_END‚Äì‚Äî- as the stopping token.\n\nTool Implementation \nThe following tools are implemented. \n\nimport subprocess\n\ndef run_bash_cmd(command: str, description: str):\n\"\"\"Run the command in a bash shell and return the output or throw a ValueError exception if the process returns non-zero exit code\n\nArgs;\n  command (str): the shell command to run\n  description (str): A single-line short natural language description of what the command achivees\n\nReturns:\n  The output of running the shell command\n\"\"\"\n\ndef finish(result: str):\n\"\"\"The agent must call this function with the final result when it has solved the given task.\nEv\nArgs: \n  result (str); the result generated by the agent\n\nReturns:\n  The result passed as an argument.  The result is then returned by the agent's run method.\n\"\"\"\n\nYou need to implement an additional tool:", "metadata": {}}
{"id": "11", "text": "Args;\n  command (str): the shell command to run\n  description (str): A single-line short natural language description of what the command achivees\n\nReturns:\n  The output of running the shell command\n\"\"\"\n\ndef finish(result: str):\n\"\"\"The agent must call this function with the final result when it has solved the given task.\nEv\nArgs: \n  result (str); the result generated by the agent\n\nReturns:\n  The result passed as an argument.  The result is then returned by the agent's run method.\n\"\"\"\n\nYou need to implement an additional tool: \n\ndef add_instructions_and_backtrack(instructions: str, at_message_id: str):\n\"\"\"The agent should call this function if it is making too many mistakes or is stuck in finding a solution,\n\nThe function changes the content of the instruction node with 'instructions' and backtracks at the node with id 'at_message_id'.  \n\"\"\"\nTODO: implement this \n\n\nOptional Tool Implementations \nYou will notice that LLM could make repeated mistakes in invoking specific commands.  In such situations, you should add custom functions that could run a sequence of commands in addition to run_bash_cmd.", "metadata": {}}
{"id": "12", "text": "def add_instructions_and_backtrack(instructions: str, at_message_id: str):\n\"\"\"The agent should call this function if it is making too many mistakes or is stuck in finding a solution,\n\nThe function changes the content of the instruction node with 'instructions' and backtracks at the node with id 'at_message_id'.  \n\"\"\"\nTODO: implement this \n\n\nOptional Tool Implementations \nYou will notice that LLM could make repeated mistakes in invoking specific commands.  In such situations, you should add custom functions that could run a sequence of commands in addition to run_bash_cmd.\n\nFor example, the LLM might try to use the sed command to edit a file, but you will find that it often fails and is not efficient.\n\nYou may want to write a custom function such as\n\ndef replace_in_file(file_path: str, from_line: str, to_line: str, content: str)\n\nwhich replaces the lines from_line to to_line (both inclusive) with the content.\n\nSimilarly, you can define \n\ndef show_file(file_path: str)", "metadata": {}}
{"id": "13", "text": "For example, the LLM might try to use the sed command to edit a file, but you will find that it often fails and is not efficient.\n\nYou may want to write a custom function such as\n\ndef replace_in_file(file_path: str, from_line: str, to_line: str, content: str)\n\nwhich replaces the lines from_line to to_line (both inclusive) with the content.\n\nSimilarly, you can define \n\ndef show_file(file_path: str) \n\nwhich will call run_bash_cmd with f‚Äùcat -n {file_path}‚Äù to show the file contents with line numbers.  \nEvaluation \nYou need to evaluate the agent on the SWE-bench verified subset and report the performance. We set up a GitHub repository for this homework. Please follow the README to run the evaluation. \n\nGoal: improve the accuracy of the agent by creating custom functions and the most optimized user prompt containing instructions on how to solve a generic SWE issue.\nYou can refer to https://agents.md/ for examples of thousands of SWE prompts for various SWE tasks and workflows.", "metadata": {}}
{"id": "14", "text": "which will call run_bash_cmd with f‚Äùcat -n {file_path}‚Äù to show the file contents with line numbers.  \nEvaluation \nYou need to evaluate the agent on the SWE-bench verified subset and report the performance. We set up a GitHub repository for this homework. Please follow the README to run the evaluation. \n\nGoal: improve the accuracy of the agent by creating custom functions and the most optimized user prompt containing instructions on how to solve a generic SWE issue.\nYou can refer to https://agents.md/ for examples of thousands of SWE prompts for various SWE tasks and workflows. \n\nFor evaluation: \nBackend LLM should be GPT 5-mini (medium reasoning) \nYou must restrict the number of steps to 100, i.e., MAX_STEPS=100.", "metadata": {}}
{"id": "15", "text": "Goal: improve the accuracy of the agent by creating custom functions and the most optimized user prompt containing instructions on how to solve a generic SWE issue.\nYou can refer to https://agents.md/ for examples of thousands of SWE prompts for various SWE tasks and workflows. \n\nFor evaluation: \nBackend LLM should be GPT 5-mini (medium reasoning) \nYou must restrict the number of steps to 100, i.e., MAX_STEPS=100.\nBaseline: report accuracy without using add_instructions_and_backtrack\nReport improved accuracy with add_instructions_and_backtrack and any customized tools you want to add \nSubmission Instructions\nYou will submit three things to the submission server here: \nCode artifact\nA zip uploaded to the portal \nMust contain everything needed to build and run an end-to-end evaluation \nDo not commit secrets/keys \nReport (pdf) \nYou should report your accuracy number, the custom tools you created and the reason behind making them, and the lessons you learned.", "metadata": {}}
{"id": "16", "text": "Baseline: report accuracy without using add_instructions_and_backtrack\nReport improved accuracy with add_instructions_and_backtrack and any customized tools you want to add \nSubmission Instructions\nYou will submit three things to the submission server here: \nCode artifact\nA zip uploaded to the portal \nMust contain everything needed to build and run an end-to-end evaluation \nDo not commit secrets/keys \nReport (pdf) \nYou should report your accuracy number, the custom tools you created and the reason behind making them, and the lessons you learned. \nJSON results for leaderboard \nYou should submit the final SWEBench harness evaluation results to our leaderboard, which should have the following structure:\n\t\t\n{\n    \"total_instances\": 20,\n    \"submitted_instances\": 20,\n    \"completed_instances\": 19,\n    \"resolved_instances\": 9,\n    \"unresolved_instances\": 10,\n    \"empty_patch_instances\": 1,\n    \"error_instances\": 0,\n    \"completed_ids\": [\"astropy__astropy-7166\", ...],\n    \"resolved_ids\": [\"astropy__astropy-7166\", ...],\n    \"unresolved_ids\": [\"django__django-10973\", ...],\n    \"schema_version\": 2\n}", "metadata": {}}
{"id": "17", "text": "You might find the checklist here helpful. \n\nMore Information Coming‚Ä¶", "metadata": {}}
