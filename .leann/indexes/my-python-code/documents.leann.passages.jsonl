{"id": "0", "text": "\"\"\"\nStarter scaffold for the CS 294-264 HW1 ReAct agent.\n\nStudents must implement a minimal ReAct agent that:\n- Maintains a message history tree (role, content, timestamp, unique_id, parent, children)\n- Uses a textual function-call format (see ResponseParser) with rfind-based parsing\n- Alternates Reasoning and Acting until calling the tool `finish`\n- Supports tools: `run_bash_cmd`, `finish`, and `add_instructions_and_backtrack`\n\nThis file intentionally omits core implementations and replaces them with\nclear specifications and TODOs.\n\"\"\"\n\nfrom typing import List, Callable, Dict, Any\n\nfrom response_parser import ResponseParser\nfrom llm import LLM, OpenAIModel\nimport inspect\n\nclass ReactAgent:\n    \"\"\"\n    Minimal ReAct agent that:\n    - Maintains a message history tree with unique ids\n    - Builds the LLM context from the root to current node\n    - Registers callable tools with auto-generated docstrings in the system prompt\n    - Runs a Reason-Act loop until `finish` is called or MAX_STEPS is reached\n    \"\"\"", "metadata": {}}
{"id": "1", "text": "from typing import List, Callable, Dict, Any\n\nfrom response_parser import ResponseParser\nfrom llm import LLM, OpenAIModel\nimport inspect\n\nclass ReactAgent:\n    \"\"\"\n    Minimal ReAct agent that:\n    - Maintains a message history tree with unique ids\n    - Builds the LLM context from the root to current node\n    - Registers callable tools with auto-generated docstrings in the system prompt\n    - Runs a Reason-Act loop until `finish` is called or MAX_STEPS is reached\n    \"\"\"\n\n    def __init__(self, name: str, parser: ResponseParser, llm: LLM):\n        self.name: str = name\n        self.parser = parser\n        self.llm = llm\n\n        # Message tree storage\n        self.id_to_message: List[Dict[str, Any]] = []\n        self.root_message_id: int = -1\n        self.current_message_id: int = -1\n\n        # Registered tools\n        self.function_map: Dict[str, Callable] = {}\n\n        # Set up the initial structure of the history\n        # Create required root nodes and a user node (task) and an instruction node.", "metadata": {}}
{"id": "2", "text": "# Message tree storage\n        self.id_to_message: List[Dict[str, Any]] = []\n        self.root_message_id: int = -1\n        self.current_message_id: int = -1\n\n        # Registered tools\n        self.function_map: Dict[str, Callable] = {}\n\n        # Set up the initial structure of the history\n        # Create required root nodes and a user node (task) and an instruction node.\n        self.system_message_id = self.add_message(\n            \"system\",\n            (\n                \"You are a Smart ReAct agent for SWE-Bench. Obey the protocol strictly, think step-by-step, and make real code changes.\\n\"\n                \"- Always reason briefly, then ACT using exactly one tool call.\\n\"\n                \"- Prefer small, verifiable steps.", "metadata": {}}
{"id": "3", "text": "# Registered tools\n        self.function_map: Dict[str, Callable] = {}\n\n        # Set up the initial structure of the history\n        # Create required root nodes and a user node (task) and an instruction node.\n        self.system_message_id = self.add_message(\n            \"system\",\n            (\n                \"You are a Smart ReAct agent for SWE-Bench. Obey the protocol strictly, think step-by-step, and make real code changes.\\n\"\n                \"- Always reason briefly, then ACT using exactly one tool call.\\n\"\n                \"- Prefer small, verifiable steps. Read tool output and adapt.\\n\"\n                \"- Do not finish until changes are staged (git diff --cached --name-only non-empty).\\n\"\n                \"- Output must follow the function-call format with the end token.\\n\"\n            ),\n        )\n        self.user_message_id = self.add_message(\"user\", \"\")\n        self.instructions_message_id = self.add_message(\n            \"instructor\",\n            (\n                \"SWE-Bench Playbook — YOU MUST PRODUCE CODE CHANGES. Single tool: run_bash_cmd.", "metadata": {}}
{"id": "4", "text": "Read tool output and adapt.\\n\"\n                \"- Do not finish until changes are staged (git diff --cached --name-only non-empty).\\n\"\n                \"- Output must follow the function-call format with the end token.\\n\"\n            ),\n        )\n        self.user_message_id = self.add_message(\"user\", \"\")\n        self.instructions_message_id = self.add_message(\n            \"instructor\",\n            (\n                \"SWE-Bench Playbook — YOU MUST PRODUCE CODE CHANGES. Single tool: run_bash_cmd. Think hard, observe feedback, execute precise commands, verify, then finish.\\n\\n\"\n                \"NON-NEGOTIABLE RULES:\\n\"\n                \"- Do NOT call finish until there are staged files (git diff --cached --name-only is non-empty).\\n\"\n                \"- Every message MUST end with EXACTLY ONE function call in the specified format and the end token.\\n\"\n                \"- Use ONLY run_bash_cmd (plus finish/add_instructions_and_backtrack).\\n\"\n                \"- No interactive editors.", "metadata": {}}
{"id": "5", "text": "Single tool: run_bash_cmd. Think hard, observe feedback, execute precise commands, verify, then finish.\\n\\n\"\n                \"NON-NEGOTIABLE RULES:\\n\"\n                \"- Do NOT call finish until there are staged files (git diff --cached --name-only is non-empty).\\n\"\n                \"- Every message MUST end with EXACTLY ONE function call in the specified format and the end token.\\n\"\n                \"- Use ONLY run_bash_cmd (plus finish/add_instructions_and_backtrack).\\n\"\n                \"- No interactive editors. Use ed or a small python here-doc for edits.\\n\"\n                \"- Keep outputs minimal (use -q, head -n, tail -n, or sed -n).\\n\\n\"\n                \"WORKFLOW (REPEAT UNTIL PASS):\\n\"\n                \"1) Test: run_bash_cmd('pytest -q').\\n\"\n                \"2) Triage: grep for symbols or failing tests: run_bash_cmd('grep -R \\\"<pattern>\\\" -n .", "metadata": {}}
{"id": "6", "text": "Use ed or a small python here-doc for edits.\\n\"\n                \"- Keep outputs minimal (use -q, head -n, tail -n, or sed -n).\\n\\n\"\n                \"WORKFLOW (REPEAT UNTIL PASS):\\n\"\n                \"1) Test: run_bash_cmd('pytest -q').\\n\"\n                \"2) Triage: grep for symbols or failing tests: run_bash_cmd('grep -R \\\"<pattern>\\\" -n .'); open files: run_bash_cmd('nl -ba path/to/file.py | sed -n \\\"<start>,<end>p\\\"').\\n\"\n                \"3) Edit (precise, minimal):\\n\"\n                \"   - ed example (replace range):\\n\"\n                \"     ed -s path/to/file.py <<'ED'\\n<from>,<to>d\\n<from-1>a\\n<new code here>\\n.\\nw\\nq\\nED\\n\"\n                \"   - python in-place example:\\n\"\n                \"     python - <<'PY'\\nfrom pathlib import Path\\np = Path('path/to/file.py')\\ns = p.read_text()\\ns = s.replace('OLD', 'NEW', 1)\\np.write_text(s)\\nPY\\n\"\n                \"4) Re-test: run_bash_cmd('pytest -q'); move to next failure.\\n\"\n                \"5) Stage+verify: run_bash_cmd('git add -A && git diff --cached --name-only').", "metadata": {}}
{"id": "7", "text": "Must list files.\\n\"\n                \"6) Finish ONLY then.\\n\\n\"\n                \"STRATEGY & FEEDBACK:\\n\"\n                \"- After each command, read output and plan the next exact command (file, lines, change).\\n\"\n                \"- If a command fails, fix the command and retry; do not give up.\\n\"\n                \"- If stuck after two loops, use add_instructions_and_backtrack to refine the plan.\\n\\n\"\n                \"EXAMPLES:\\n\"\n                \"Run tests:\\n\"\n                \"----BEGIN_FUNCTION_CALL----\\nrun_bash_cmd\\n----ARG----\\ncommand\\npytest -q\\n----END_FUNCTION_CALL----\\n\\n\"\n                \"View lines 110-160:\\n\"\n                \"----BEGIN_FUNCTION_CALL----\\nrun_bash_cmd\\n----ARG----\\ncommand\\nnl -ba path/to/file.py | sed -n '110,160p'\\n----END_FUNCTION_CALL----\\n\\n\"\n                \"Apply edit via ed:\\n\"\n                \"----BEGIN_FUNCTION_CALL----\\nrun_bash_cmd\\n----ARG----\\ncommand\\ned -s path/to/file.py <<'ED'\\n120,", "metadata": {}}
{"id": "8", "text": "py | sed -n '110,160p'\\n----END_FUNCTION_CALL----\\n\\n\"\n                \"Apply edit via ed:\\n\"\n                \"----BEGIN_FUNCTION_CALL----\\nrun_bash_cmd\\n----ARG----\\ncommand\\ned -s path/to/file.py <<'ED'\\n120,135d\\n119a\\n<new code here>\\n.\\nw\\nq\\nED\\n----END_FUNCTION_CALL----\\n\\n\"\n                \"Stage and verify:\\n\"\n                \"----BEGIN_FUNCTION_CALL----\\nrun_bash_cmd\\n----ARG----\\ncommand\\ngit add -A && git diff --cached --name-only\\n----END_FUNCTION_CALL----\\n\\n\"\n                \"Finish (only after staged files exist):\\n\"\n                \"----BEGIN_FUNCTION_CALL----\\nfinish\\n----ARG----\\nresult\\nApplied minimal fix; tests pass locally.\\n----END_FUNCTION_CALL----\\n\"\n            ),\n        )\n        \n        # NOTE: mandatory finish function that terminates the agent\n        self.add_functions([self.finish])", "metadata": {}}
{"id": "9", "text": "tests pass locally.\\n----END_FUNCTION_CALL----\\n\"\n            ),\n        )\n        \n        # NOTE: mandatory finish function that terminates the agent\n        self.add_functions([self.finish])\n\n    # -------------------- MESSAGE TREE --------------------\n    def add_message(self, role: str, content: str) -> int:\n        \"\"\"\n        Create a new message and add it to the tree.\n\n        The message must include fields: role, content, timestamp, unique_id, parent, children.\n        Maintain a pointer to the current node and the root node.\n        \"\"\"\n        unique_id = len(self.id_to_message) + 1\n        parent_id = self.current_message_id if self.current_message_id != -1 else None\n        message = {\n            \"role\": role,\n            \"content\": content,\n            \"timestamp\": unique_id,  # simple monotonic timestamp surrogate\n            \"unique_id\": unique_id,\n            \"parent\": parent_id,\n            \"children\": [],\n        }\n        self.id_to_message.append(message)\n\n        # Link into parent's children if exists\n        if parent_id is not None:\n            self.id_to_message[parent_id - 1][\"children\"].append(unique_id)", "metadata": {}}
{"id": "10", "text": "# Link into parent's children if exists\n        if parent_id is not None:\n            self.id_to_message[parent_id - 1][\"children\"].append(unique_id)\n\n        # Initialize root and current pointers\n        if self.root_message_id == -1:\n            self.root_message_id = unique_id\n        self.current_message_id = unique_id\n        return unique_id\n\n    def set_message_content(self, message_id: int, content: str) -> None:\n        \"\"\"Update message content by id.\"\"\"\n        if message_id <= 0 or message_id > len(self.id_to_message):\n            raise ValueError(\"Invalid message_id\")\n        self.id_to_message[message_id - 1][\"content\"] = content", "metadata": {}}
{"id": "11", "text": "# Initialize root and current pointers\n        if self.root_message_id == -1:\n            self.root_message_id = unique_id\n        self.current_message_id = unique_id\n        return unique_id\n\n    def set_message_content(self, message_id: int, content: str) -> None:\n        \"\"\"Update message content by id.\"\"\"\n        if message_id <= 0 or message_id > len(self.id_to_message):\n            raise ValueError(\"Invalid message_id\")\n        self.id_to_message[message_id - 1][\"content\"] = content\n\n    def get_context(self) -> str:\n        \"\"\"\n        Build the full LLM context by walking from the root to the current message.\n        \"\"\"\n        # Build path from root to current by following parents\n        if self.current_message_id == -1:\n            return \"\"\n        path: list[int] = []\n        cursor = self.current_message_id\n        while cursor is not None:\n            path.append(cursor)\n            parent = self.id_to_message[cursor - 1][\"parent\"]\n            cursor = parent\n        path.reverse()", "metadata": {}}
{"id": "12", "text": "def get_context(self) -> str:\n        \"\"\"\n        Build the full LLM context by walking from the root to the current message.\n        \"\"\"\n        # Build path from root to current by following parents\n        if self.current_message_id == -1:\n            return \"\"\n        path: list[int] = []\n        cursor = self.current_message_id\n        while cursor is not None:\n            path.append(cursor)\n            parent = self.id_to_message[cursor - 1][\"parent\"]\n            cursor = parent\n        path.reverse()\n\n        # Concatenate each message context block\n        parts: list[str] = []\n        for mid in path:\n            parts.append(self.message_id_to_context(mid))\n        return \"\".join(parts)\n\n    # -------------------- REQUIRED TOOLS --------------------\n    def add_functions(self, tools: List[Callable]):\n        \"\"\"\n        Add callable tools to the agent's function map.", "metadata": {}}
{"id": "13", "text": "# Concatenate each message context block\n        parts: list[str] = []\n        for mid in path:\n            parts.append(self.message_id_to_context(mid))\n        return \"\".join(parts)\n\n    # -------------------- REQUIRED TOOLS --------------------\n    def add_functions(self, tools: List[Callable]):\n        \"\"\"\n        Add callable tools to the agent's function map.\n\n        The system prompt must include tool descriptions that cover:\n        - The signature of each tool\n        - The docstring of each tool\n        \"\"\"\n        for tool in tools:\n            self.function_map[tool.__name__] = tool\n        # Ensure core tools are present\n        self.function_map[\"finish\"] = self.finish\n        self.function_map[\"add_instructions_and_backtrack\"] = self.add_instructions_and_backtrack\n        # Keep system message minimal; the full tool list is rendered dynamically in message_id_to_context\n        base = (\n            \"You are a Smart ReAct agent for SWE-Bench. Obey the protocol strictly, think step-by-step, and make real code changes.\\n\"\n            \"- Always reason briefly, then ACT using exactly one tool call.\\n\"\n            \"- Prefer small, verifiable steps.", "metadata": {}}
{"id": "14", "text": "Obey the protocol strictly, think step-by-step, and make real code changes.\\n\"\n            \"- Always reason briefly, then ACT using exactly one tool call.\\n\"\n            \"- Prefer small, verifiable steps. Read tool output and adapt.\\n\"\n            \"- Do not finish until changes are staged (git diff --cached --name-only non-empty).\\n\"\n            \"- Output must follow the function-call format with the end token.\\n\"\n        )\n        self.set_message_content(self.system_message_id, base)\n    \n    def finish(self, result: str):\n        \"\"\"The agent must call this function with the final result when it has solved the given task. The function calls \"git add -A and git diff --cached\" to generate a patch and returns the patch as submission.\n\n        Args: \n            result (str); the result generated by the agent\n\n        Returns:\n            The result passed as an argument.  The result is then returned by the agent's run method.\n        \"\"\"\n        return result \n\n    def add_instructions_and_backtrack(self, instructions: str, at_message_id: int):\n        \"\"\"\n        The agent should call this function if it is making too many mistakes or is stuck.", "metadata": {}}
{"id": "15", "text": "The function calls \"git add -A and git diff --cached\" to generate a patch and returns the patch as submission.\n\n        Args: \n            result (str); the result generated by the agent\n\n        Returns:\n            The result passed as an argument.  The result is then returned by the agent's run method.\n        \"\"\"\n        return result \n\n    def add_instructions_and_backtrack(self, instructions: str, at_message_id: int):\n        \"\"\"\n        The agent should call this function if it is making too many mistakes or is stuck.\n\n        The function changes the content of the instruction node with 'instructions' and\n        backtracks at the node with id 'at_message_id'. Backtracking means the current node\n        pointer moves to the specified node and subsequent context is rebuilt from there.\n\n        Returns a short success string.\n        \"\"\"\n        # Update instruction node content\n        self.set_message_content(self.instructions_message_id, instructions)\n        # Validate target id and backtrack current pointer\n        if at_message_id <= 0 or at_message_id > len(self.id_to_message):\n            raise ValueError(\"Invalid backtrack message id\")\n        self.current_message_id = at_message_id\n        return \"Updated instructions and backtracked\"", "metadata": {}}
{"id": "16", "text": "Returns a short success string.\n        \"\"\"\n        # Update instruction node content\n        self.set_message_content(self.instructions_message_id, instructions)\n        # Validate target id and backtrack current pointer\n        if at_message_id <= 0 or at_message_id > len(self.id_to_message):\n            raise ValueError(\"Invalid backtrack message id\")\n        self.current_message_id = at_message_id\n        return \"Updated instructions and backtracked\"\n\n    # -------------------- MAIN LOOP --------------------\n    def run(self, task: str, max_steps: int) -> str:\n        \"\"\"\n        Run the agent's main ReAct loop:\n        - Set the user prompt\n        - Loop up to max_steps (<= 100):\n            - Build context from the message tree\n            - Query the LLM\n            - Parse a single function call at the end (see ResponseParser)\n            - Execute the tool\n            - Append tool result to the tree\n            - If `finish` is called, return the final result\n        \"\"\"\n        if max_steps > 100:\n            max_steps = 100\n        # Set user task content into the user node, and position current at instructor\n        self.set_message_content(self.user_message_id, task)\n        self.current_message_id = self.instructions_message_id", "metadata": {}}
{"id": "17", "text": "for step in range(max_steps):\n            print(f\"Step {step}\")\n            # Build context and query LLM\n            context = self.get_context()\n            try:\n                llm_output = self.llm.generate(context)\n            except Exception as e:\n                # Record error as assistant message and continue\n                self.add_message(\"assistant\", f\"LLM error: {e}\")\n                continue\n\n            # Add assistant raw output\n            assistant_id = self.add_message(\"assistant\", llm_output)\n\n            # Parse function call\n            try:\n                call = self.parser.parse(llm_output)\n            except Exception as e:\n                # Add tool error and continue\n                self.add_message(\"tool\", f\"Parser error: {e}\")\n                continue\n\n            name = call.get(\"name\")\n            args = call.get(\"arguments\", {})\n            if name not in self.function_map:\n                self.add_message(\"tool\", f\"Unknown function: {name}\")\n                continue", "metadata": {}}
{"id": "18", "text": "# Add assistant raw output\n            assistant_id = self.add_message(\"assistant\", llm_output)\n\n            # Parse function call\n            try:\n                call = self.parser.parse(llm_output)\n            except Exception as e:\n                # Add tool error and continue\n                self.add_message(\"tool\", f\"Parser error: {e}\")\n                continue\n\n            name = call.get(\"name\")\n            args = call.get(\"arguments\", {})\n            if name not in self.function_map:\n                self.add_message(\"tool\", f\"Unknown function: {name}\")\n                continue\n\n            tool = self.function_map[name]\n            # Execute tool\n            try:\n                result = tool(**args)\n            except TypeError:\n                # Fallback: try passing a single positional dict if signature mismatches\n                try:\n                    result = tool(args)\n                except Exception as e:\n                    result = f\"Tool execution error: {e}\"\n            except Exception as e:\n                result = f\"Tool execution error: {e}\"\n\n            # Append tool result\n            self.add_message(\"tool\", str(result))\n            print(f\"Tool result: {result}\")", "metadata": {}}
{"id": "19", "text": "tool = self.function_map[name]\n            # Execute tool\n            try:\n                result = tool(**args)\n            except TypeError:\n                # Fallback: try passing a single positional dict if signature mismatches\n                try:\n                    result = tool(args)\n                except Exception as e:\n                    result = f\"Tool execution error: {e}\"\n            except Exception as e:\n                result = f\"Tool execution error: {e}\"\n\n            # Append tool result\n            self.add_message(\"tool\", str(result))\n            print(f\"Tool result: {result}\")\n\n            # If finish was called, return result as final output\n            if name == \"finish\":\n                print(f\"Finish called with result: {result}\")\n                return str(result)\n\n        # If we exit the loop without finish, return a default message\n        return \"MAX_STEPS reached without calling finish\"", "metadata": {}}
{"id": "20", "text": "# Append tool result\n            self.add_message(\"tool\", str(result))\n            print(f\"Tool result: {result}\")\n\n            # If finish was called, return result as final output\n            if name == \"finish\":\n                print(f\"Finish called with result: {result}\")\n                return str(result)\n\n        # If we exit the loop without finish, return a default message\n        return \"MAX_STEPS reached without calling finish\"\n\n    def message_id_to_context(self, message_id: int) -> str:\n        \"\"\"\n        Helper function to convert a message id to a context string.\n        \"\"\"\n        # message_id is 1-based; list index is 0-based\n        message = self.id_to_message[message_id - 1]\n        header = f'----------------------------\\n|MESSAGE(role=\"{message[\"role\"]}\", id={message[\"unique_id\"]})|\\n'\n        content = message[\"content\"]\n        if message[\"role\"] == \"system\":\n            tool_descriptions = []\n            for tool in self.function_map.values():\n                signature = inspect.signature(tool)\n                docstring = inspect.getdoc(tool)\n                tool_description = f\"Function: {tool.__name__}{signature}\\n{docstring}\\n\"\n                tool_descriptions.append(tool_description)", "metadata": {}}
{"id": "21", "text": "tool_descriptions = \"\\n\".join(tool_descriptions)\n            return (\n                f\"{header}{content}\\n\"\n                f\"--- AVAILABLE TOOLS ---\\n{tool_descriptions}\\n\\n\"\n                f\"--- RESPONSE FORMAT ---\\n{self.parser.response_format}\\n\"\n            )\n        elif message[\"role\"] == \"instructor\":\n            return f\"{header}YOU MUST FOLLOW THE FOLLOWING INSTRUCTIONS AT ANY COST. OTHERWISE, YOU WILL BE DECOMISSIONED.\\n{content}\\n\"\n        else:\n            return f\"{header}{content}\\n\"\n\ndef main():\n    from envs import DumbEnvironment\n    llm = OpenAIModel(\"----END_FUNCTION_CALL----\", \"gpt-4o-mini\")\n    parser = ResponseParser()\n\n    env = DumbEnvironment()\n    dumb_agent = ReactAgent(\"dumb-agent\", parser, llm)\n    dumb_agent.add_functions([env.run_bash_cmd])\n    result = dumb_agent.run(\"Show the contents of all files in the current directory.\", max_steps=10)\n    print(result)", "metadata": {}}
{"id": "22", "text": "def main():\n    from envs import DumbEnvironment\n    llm = OpenAIModel(\"----END_FUNCTION_CALL----\", \"gpt-4o-mini\")\n    parser = ResponseParser()\n\n    env = DumbEnvironment()\n    dumb_agent = ReactAgent(\"dumb-agent\", parser, llm)\n    dumb_agent.add_functions([env.run_bash_cmd])\n    result = dumb_agent.run(\"Show the contents of all files in the current directory.\", max_steps=10)\n    print(result)\n\nif __name__ == \"__main__\":\n    # Optional: students can add their own quick manual test here.\n    main()", "metadata": {}}
{"id": "23", "text": "from utils import get_sb_environment\nimport subprocess\n\nclass LimitsExceeded(Exception):\n    \"\"\"Raised when the agent has reached its step limit.\"\"\"\n\n\nclass SWEEnvironment:\n    \"\"\"\n    Minimal interface to the SWEBench execution environment.\n\n    Students may use their own wrapper. The environment must expose:\n    - execute(command: str) -> str: Run a shell command and return stdout, or raise ValueError on failure\n    \"\"\"\n\n    def __init__(self, instance: dict):\n        self.env = get_sb_environment(instance)\n    \n    def _to_text(self, result) -> str:\n        \"\"\"Normalize environment execute() result to text.\"\"\"", "metadata": {}}
{"id": "24", "text": "class LimitsExceeded(Exception):\n    \"\"\"Raised when the agent has reached its step limit.\"\"\"\n\n\nclass SWEEnvironment:\n    \"\"\"\n    Minimal interface to the SWEBench execution environment.\n\n    Students may use their own wrapper. The environment must expose:\n    - execute(command: str) -> str: Run a shell command and return stdout, or raise ValueError on failure\n    \"\"\"\n\n    def __init__(self, instance: dict):\n        self.env = get_sb_environment(instance)\n    \n    def _to_text(self, result) -> str:\n        \"\"\"Normalize environment execute() result to text.\"\"\"\n        if isinstance(result, str):\n            return result\n        if isinstance(result, bytes):\n            return result.decode(\"utf-8\", errors=\"replace\")\n        if isinstance(result, dict):\n            # Prefer a combined 'output' if present\n            combined = result.get(\"output\", None)\n            if combined is not None:\n                if isinstance(combined, bytes):\n                    combined = combined.decode(\"utf-8\", errors=\"replace\")\n                return str(combined)\n            stdout = result.get(\"stdout\", \"\")\n            stderr = result.get(\"stderr\", \"\")\n            if isinstance(stdout, bytes):\n                stdout = stdout.decode(\"utf-8\", errors=\"replace\")\n            if isinstance(stderr, bytes):\n                stderr = stderr.decode(\"utf-8\", errors=\"replace\")\n            return f\"--STDOUT--\\n{stdout}\\n--STDERR--\\n{stderr}\"\n        return str(result)\n     \n    # -------------------- REQUIRED TOOLS --------------------\n    def run_bash_cmd(self, command: str) -> str:\n        \"\"\"\n        Run the command in a bash shell and return the output or throw a ValueError\n        if the process returns non-zero exit code.", "metadata": {}}
{"id": "25", "text": "Args;\n            command (str): the shell command to run\n\n        Returns:\n            The output of running the shell command\n        \"\"\"\n        try:\n            output = self.env.execute(command)\n            output = self._to_text(output)\n        except subprocess.TimeoutExpired as e:\n            output = e.output.decode(\"utf-8\", errors=\"replace\") if e.output else \"\"\n            raise ValueError(output)\n        except TimeoutError:\n            raise ValueError(\"TimeoutError\")\n        return output\n    \n    def generate_patch(self, result: str) -> str:\n        \"\"\"\n        Generate a patch from the result (for SWE-Bench)\n        \"\"\"\n        try:\n            # Debug: show working tree and staged files\n            try:\n                pre_status = self.env.execute(\"git status --porcelain\")\n                print(\"[DEBUG] git status --porcelain:\\n\", self._to_text(pre_status))\n            except Exception as _:\n                pass", "metadata": {}}
{"id": "26", "text": "# Stage all changes\n            try:\n                add_out = self.env.execute(\"git add -A\")\n                add_out_txt = self._to_text(add_out)\n                if add_out_txt.strip():\n                    print(\"[DEBUG] git add -A output:\\n\", add_out_txt)\n            except Exception as _:\n                pass\n\n            try:\n                name_only = self.env.execute(\"git diff --cached --name-only\")\n                print(\"[DEBUG] git diff --cached --name-only:\\n\", self._to_text(name_only))\n            except Exception as _:\n                pass\n\n            patch_output = self.env.execute(\"git diff --cached\")\n            # Extract only unified diff from env result. Do NOT include any narrative.\n            if isinstance(patch_output, dict):\n                out_val = patch_output.get(\"output\", None)\n                if out_val is None:\n                    out_val = patch_output.get(\"stdout\", b\"\")\n                if isinstance(out_val, bytes):\n                    out_val = out_val.decode(\"utf-8\", errors=\"replace\")\n                patch_text = out_val if isinstance(out_val, str) else str(out_val)\n            else:\n                patch_text = self._to_text(patch_output)", "metadata": {}}
{"id": "27", "text": "patch_output = self.env.execute(\"git diff --cached\")\n            # Extract only unified diff from env result. Do NOT include any narrative.\n            if isinstance(patch_output, dict):\n                out_val = patch_output.get(\"output\", None)\n                if out_val is None:\n                    out_val = patch_output.get(\"stdout\", b\"\")\n                if isinstance(out_val, bytes):\n                    out_val = out_val.decode(\"utf-8\", errors=\"replace\")\n                patch_text = out_val if isinstance(out_val, str) else str(out_val)\n            else:\n                patch_text = self._to_text(patch_output)\n\n            # If empty or whitespace, submit an empty patch\n            if not patch_text or not patch_text.strip():\n                print(\"[DEBUG] No staged diff detected (empty patch).\")\n                return \"\"", "metadata": {}}
{"id": "28", "text": "# If empty or whitespace, submit an empty patch\n            if not patch_text or not patch_text.strip():\n                print(\"[DEBUG] No staged diff detected (empty patch).\")\n                return \"\"\n\n            # Ensure trailing newline to avoid 'ends in middle of line'\n            if not patch_text.endswith(\"\\n\"):\n                patch_text += \"\\n\"\n            # Debug preview (first 500 chars)\n            print(\"[DEBUG] Unified diff preview (first 500 chars):\\n\", patch_text[:500])\n            return patch_text\n        except Exception as e:\n            # On error, return empty patch; the harness will mark it accordingly\n            print(\"[DEBUG] Error generating patch:\", e)\n            return \"\"\n    \n    # -------------------- TODO(student): add more functions here if you want --------------------\n    def has_staged_changes(self) -> bool:\n        \"\"\"Return True if there are staged changes in the repo (cached diff non-empty).\"\"\"\n        try:\n            out = self.env.execute(\"git diff --cached --name-only\")\n            txt = self._to_text(out)\n            return bool(txt.strip())\n        except Exception:\n            return False\n    def replace_in_file(self, file_path: str,", "metadata": {}}
{"id": "29", "text": "try:\n            out = self.env.execute(\"git diff --cached --name-only\")\n            txt = self._to_text(out)\n            return bool(txt.strip())\n        except Exception:\n            return False\n    def replace_in_file(self, file_path: str, from_line: int, to_line: int, content: str) -> str:\n        \"\"\"\n        [Optional] Replace the content of the file from the given line to the given line with the given content\n        \"\"\"\n        if from_line <= 0 or to_line < from_line:\n            raise ValueError(\"Invalid line range\")\n        # Use ed or sed to modify lines safely; here we construct a temporary file with awk\n        awk_cmd = (\n            \"awk 'NR<\" + str(from_line) + \"{print;next} NR>\" + str(to_line) + \"{p=1} p{print} ' \"\n            + file_path + \" > \"+ file_path + \".bak\"\n        )\n        try:\n            # Create backup with lines outside the range\n            self._to_text(self.env.", "metadata": {}}
{"id": "30", "text": "here we construct a temporary file with awk\n        awk_cmd = (\n            \"awk 'NR<\" + str(from_line) + \"{print;next} NR>\" + str(to_line) + \"{p=1} p{print} ' \"\n            + file_path + \" > \"+ file_path + \".bak\"\n        )\n        try:\n            # Create backup with lines outside the range\n            self._to_text(self.env.execute(awk_cmd))\n            # Insert new content at from_line position\n            # Build a here-doc to insert content\n            heredoc = (\n                \"ed -s \" + file_path + \n                \" <<'EDCMDS'\\n\" +\n                str(from_line) + \",\" + str(to_line) + \"d\\n\" +\n                str(from_line - 1) + \"a\\n\" + content + \"\\n.\\n\" +\n                \"w\\nq\\nEDCMDS\"\n            )\n            self._to_text(self.env.", "metadata": {}}
{"id": "31", "text": "_to_text(self.env.execute(awk_cmd))\n            # Insert new content at from_line position\n            # Build a here-doc to insert content\n            heredoc = (\n                \"ed -s \" + file_path + \n                \" <<'EDCMDS'\\n\" +\n                str(from_line) + \",\" + str(to_line) + \"d\\n\" +\n                str(from_line - 1) + \"a\\n\" + content + \"\\n.\\n\" +\n                \"w\\nq\\nEDCMDS\"\n            )\n            self._to_text(self.env.execute(heredoc))\n            return \"Replaced lines \" + str(from_line) + \"-\" + str(to_line) + \" in \" + file_path\n        except Exception as e:\n            raise ValueError(str(e))\n    \n    def show_file(self, file_path: str) -> str:\n        \"\"\"\n        [Optional]Show the content of the file\n        \"\"\"\n        try:\n            return self._to_text(self.env.execute(f\"nl -ba -- {file_path}\"))\n        except Exception as e:\n            raise ValueError(str(e))", "metadata": {}}
{"id": "32", "text": "_to_text(self.env.execute(heredoc))\n            return \"Replaced lines \" + str(from_line) + \"-\" + str(to_line) + \" in \" + file_path\n        except Exception as e:\n            raise ValueError(str(e))\n    \n    def show_file(self, file_path: str) -> str:\n        \"\"\"\n        [Optional]Show the content of the file\n        \"\"\"\n        try:\n            return self._to_text(self.env.execute(f\"nl -ba -- {file_path}\"))\n        except Exception as e:\n            raise ValueError(str(e))\n\nclass DumbEnvironment:\n    \"\"\"\n    Dumb environment that just executes the command\n    \"\"\"\n\n    def execute(self, command: str) -> str:\n        \"\"\"\n        Run the command in bash and return the output\n\n        Args;\n            command (str): the shell command to run", "metadata": {}}
{"id": "33", "text": "file_path: str) -> str:\n        \"\"\"\n        [Optional]Show the content of the file\n        \"\"\"\n        try:\n            return self._to_text(self.env.execute(f\"nl -ba -- {file_path}\"))\n        except Exception as e:\n            raise ValueError(str(e))\n\nclass DumbEnvironment:\n    \"\"\"\n    Dumb environment that just executes the command\n    \"\"\"\n\n    def execute(self, command: str) -> str:\n        \"\"\"\n        Run the command in bash and return the output\n\n        Args;\n            command (str): the shell command to run\n\n        Returns:\n            The output of running the shell command\n        \"\"\"\n        result = subprocess.run(command, capture_output=True, shell=True, check=False)\n        output = f\"--STDOUT--\\n{result.stdout.decode()}\\n--STDERR--\\n{result.stderr.decode()}\"\n        if result.returncode:\n            raise ValueError(output)\n        return output", "metadata": {}}
{"id": "34", "text": "from abc import ABC, abstractmethod\n\n\nclass LLM(ABC):\n    \"\"\"Abstract base class for Large Language Models.\"\"\"\n\n    @abstractmethod\n    def generate(self, prompt: str) -> str:\n        \"\"\"\n        Generate a response from the LLM given a prompt.\n        Must include any required stop-token logic at the caller level.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass OpenAIModel(LLM):\n    \"\"\"\n    Example LLM implementation using OpenAI's Responses API.\n\n    TODO(student): Implement this class to call your chosen backend (e.g., OpenAI GPT-5 mini)\n    and return the model's text output. You should ensure the model produces the response\n    format required by ResponseParser and include the stop token in the output string.\n    \"\"\"\n\n    def __init__(self, stop_token: str, model_name: str = \"gpt-5-mini\"):\n        # Initialize OpenAI client; API key is read from environment\n        from openai import OpenAI\n        self.stop_token = stop_token\n        self.model_name = model_name\n        self._client = OpenAI()\n\n    def generate(self, prompt: str) -> str:\n        # Call the model, obtain text,", "metadata": {}}
{"id": "35", "text": "def __init__(self, stop_token: str, model_name: str = \"gpt-5-mini\"):\n        # Initialize OpenAI client; API key is read from environment\n        from openai import OpenAI\n        self.stop_token = stop_token\n        self.model_name = model_name\n        self._client = OpenAI()\n\n    def generate(self, prompt: str) -> str:\n        # Call the model, obtain text, and ensure the stop token is present at the end\n        text: str = \"\"\n        try:\n            completion = self._client.chat.completions.create(\n                model=self.model_name,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.2,\n            )\n            # Be defensive: choices may be missing/empty\n            if getattr(completion, \"choices\", None) and len(completion.choices) > 0:\n                choice0 = completion.choices[0]\n                msg = getattr(choice0, \"message\", None)\n                if msg and getattr(msg, \"content\", None):\n                    text = msg.content\n                elif getattr(choice0, \"text\",", "metadata": {}}
{"id": "36", "text": "_client.chat.completions.create(\n                model=self.model_name,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.2,\n            )\n            # Be defensive: choices may be missing/empty\n            if getattr(completion, \"choices\", None) and len(completion.choices) > 0:\n                choice0 = completion.choices[0]\n                msg = getattr(choice0, \"message\", None)\n                if msg and getattr(msg, \"content\", None):\n                    text = msg.content\n                elif getattr(choice0, \"text\", None):\n                    text = choice0.text  # some providers use 'text'\n                else:\n                    text = \"\"\n            else:\n                text = \"\"\n        except Exception:\n            # Fallback to responses API if chat.completions is unavailable\n            try:\n                response = self._client.responses.create(\n                    model=self.model_name,\n                    tools=[{ \"type\": \"web_search_preview\" }],\n                    input=prompt,\n                )\n                # Try multiple extraction strategies depending on SDK version\n                extracted = getattr(response, \"output_text\",", "metadata": {}}
{"id": "37", "text": "content\n                elif getattr(choice0, \"text\", None):\n                    text = choice0.text  # some providers use 'text'\n                else:\n                    text = \"\"\n            else:\n                text = \"\"\n        except Exception:\n            # Fallback to responses API if chat.completions is unavailable\n            try:\n                response = self._client.responses.create(\n                    model=self.model_name,\n                    tools=[{ \"type\": \"web_search_preview\" }],\n                    input=prompt,\n                )\n                # Try multiple extraction strategies depending on SDK version\n                extracted = getattr(response, \"output_text\", None)\n                if not extracted:\n                    # Attempt to read from response.output list\n                    output = getattr(response, \"output\", None)\n                    if output and len(output) > 0:\n                        first = output[0]\n                        content = getattr(first, \"content\", None)\n                        if content and len(content) > 0:\n                            maybe_text = getattr(content[0], \"text\", None)\n                            extracted = getattr(maybe_text, \"value\", None) or getattr(maybe_text, \"content\", None)\n                text = extracted or \"\"\n            except Exception as e:\n                raise e", "metadata": {}}
{"id": "38", "text": "\"output_text\", None)\n                if not extracted:\n                    # Attempt to read from response.output list\n                    output = getattr(response, \"output\", None)\n                    if output and len(output) > 0:\n                        first = output[0]\n                        content = getattr(first, \"content\", None)\n                        if content and len(content) > 0:\n                            maybe_text = getattr(content[0], \"text\", None)\n                            extracted = getattr(maybe_text, \"value\", None) or getattr(maybe_text, \"content\", None)\n                text = extracted or \"\"\n            except Exception as e:\n                raise e\n\n        if text is None:\n            text = \"\"\n        # Ensure required stop token is present so the parser can find END_CALL\n        stripped = text.rstrip()\n        if not stripped.endswith(self.stop_token):\n            if stripped:\n                text = stripped + \"\\n\" + self.stop_token\n            else:\n                text = self.stop_token\n        return text", "metadata": {}}
{"id": "39", "text": "class ResponseParser:\n    \"\"\"\n    Parses LLM responses to extract a single function call using a rigid textual format.\n\n    The LLM must output exactly one function call at the end of its response.\n    Do NOT use JSON or XML. Use rfind to locate the final markers.\n    \"\"\"\n\n    BEGIN_CALL = \"----BEGIN_FUNCTION_CALL----\"\n    END_CALL = \"----END_FUNCTION_CALL----\"\n    ARG_SEP = \"----ARG----\"\n\n    # Students should include this exact template in the system prompt so the LLM follows it.\n    response_format = f\"\"\"\nyour_thoughts_here\n...\n{BEGIN_CALL}\nfunction_name\n{ARG_SEP}\narg1_name\narg1_value (can be multiline)\n{ARG_SEP}\narg2_name\narg2_value (can be multiline)\n...\n{END_CALL}\n\"\"\"\n\n    def parse(self, text: str) -> dict:\n        \"\"\"\n        Parse the function call from `text` using string.rfind to avoid confusion with\n        earlier delimiter-like content in the reasoning.", "metadata": {}}
{"id": "40", "text": "# Students should include this exact template in the system prompt so the LLM follows it.\n    response_format = f\"\"\"\nyour_thoughts_here\n...\n{BEGIN_CALL}\nfunction_name\n{ARG_SEP}\narg1_name\narg1_value (can be multiline)\n{ARG_SEP}\narg2_name\narg2_value (can be multiline)\n...\n{END_CALL}\n\"\"\"\n\n    def parse(self, text: str) -> dict:\n        \"\"\"\n        Parse the function call from `text` using string.rfind to avoid confusion with\n        earlier delimiter-like content in the reasoning.\n\n        Returns a dictionary: {\"thought\": str, \"name\": str, \"arguments\": dict}\n        \"\"\"\n        end_idx = text.rfind(self.END_CALL)\n        if end_idx == -1:\n            raise ValueError(\"Missing END_CALL marker in response\")\n\n        begin_idx = text.rfind(self.BEGIN_CALL, 0, end_idx)\n        if begin_idx == -1:\n            raise ValueError(\"Missing BEGIN_CALL marker before END_CALL in response\")\n\n        thought = text[:begin_idx].rstrip()\n        call_block = text[begin_idx + len(self.BEGIN_CALL):end_idx]", "metadata": {}}
{"id": "41", "text": "Returns a dictionary: {\"thought\": str, \"name\": str, \"arguments\": dict}\n        \"\"\"\n        end_idx = text.rfind(self.END_CALL)\n        if end_idx == -1:\n            raise ValueError(\"Missing END_CALL marker in response\")\n\n        begin_idx = text.rfind(self.BEGIN_CALL, 0, end_idx)\n        if begin_idx == -1:\n            raise ValueError(\"Missing BEGIN_CALL marker before END_CALL in response\")\n\n        thought = text[:begin_idx].rstrip()\n        call_block = text[begin_idx + len(self.BEGIN_CALL):end_idx]\n\n        # Normalize newlines and strip outer whitespace\n        call_block = call_block.strip(\"\\n\")\n        if not call_block:\n            raise ValueError(\"Empty function call block\")\n\n        # Split by ARG_SEP. First segment is function name, following come in pairs (name, value)\n        segments = call_block.split(self.ARG_SEP)\n        segments = [seg.strip(\"\\n\") for seg in segments]\n        if len(segments) < 1:\n            raise ValueError(\"Malformed function call: no segments found\")", "metadata": {}}
{"id": "42", "text": "# Normalize newlines and strip outer whitespace\n        call_block = call_block.strip(\"\\n\")\n        if not call_block:\n            raise ValueError(\"Empty function call block\")\n\n        # Split by ARG_SEP. First segment is function name, following come in pairs (name, value)\n        segments = call_block.split(self.ARG_SEP)\n        segments = [seg.strip(\"\\n\") for seg in segments]\n        if len(segments) < 1:\n            raise ValueError(\"Malformed function call: no segments found\")\n\n        # Function name is the first non-empty line of the first segment\n        func_name_block = segments[0].strip()\n        func_name_lines = [ln for ln in func_name_block.splitlines() if ln.strip()]\n        if not func_name_lines:\n            raise ValueError(\"Malformed function call: missing function name\")\n        func_name = func_name_lines[0].strip()", "metadata": {}}
{"id": "43", "text": "# Function name is the first non-empty line of the first segment\n        func_name_block = segments[0].strip()\n        func_name_lines = [ln for ln in func_name_block.splitlines() if ln.strip()]\n        if not func_name_lines:\n            raise ValueError(\"Malformed function call: missing function name\")\n        func_name = func_name_lines[0].strip()\n\n        # Parse arguments: subsequent segments should be name then value\n        arguments: dict = {}\n        # After splitting, segments[1:] contains alternating arg_name, arg_value blocks\n        # But format shows each ARG_SEP introduces a name and then value until next sep.\n        # We'll iterate by reading name from the first line of the segment and value from the remaining\n        pending_name = None\n        pending_value_lines: list[str] = []", "metadata": {}}
{"id": "44", "text": "# Parse arguments: subsequent segments should be name then value\n        arguments: dict = {}\n        # After splitting, segments[1:] contains alternating arg_name, arg_value blocks\n        # But format shows each ARG_SEP introduces a name and then value until next sep.\n        # We'll iterate by reading name from the first line of the segment and value from the remaining\n        pending_name = None\n        pending_value_lines: list[str] = []\n\n        # We treat each segment after the first as a pair block (name on first line, rest value)\n        for seg in segments[1:]:\n            seg_lines = seg.splitlines()\n            if not seg_lines:\n                continue\n            arg_name = seg_lines[0].strip()\n            arg_value = \"\\n\".join(seg_lines[1:]) if len(seg_lines) > 1 else \"\"\n            if not arg_name:\n                raise ValueError(\"Malformed function call: empty argument name\")\n            # If argument name repeats, last one wins\n            arguments[arg_name] = arg_value\n\n        return {\"thought\": thought, \"name\": func_name, \"arguments\": arguments}", "metadata": {}}
{"id": "45", "text": "#!/usr/bin/env python3\nimport concurrent.futures\nimport subprocess\nfrom pathlib import Path\n\nimport typer\nfrom datasets import load_dataset\n\nfrom utils import save_traj, update_preds_file, remove_from_preds_file, get_sb_environment\n\napp = typer.Typer(rich_markup_mode=\"rich\", add_completion=False)\n\nDATASET_MAPPING = {\n    \"cs294\": \"lynnliu030/swebench-eval-subset\",\n}\n\nfrom agent import ReactAgent\nfrom llm import OpenAIModel\nfrom response_parser import ResponseParser\nfrom envs import SWEEnvironment, DumbEnvironment\n\ndef process_instance(\n    instance: dict,\n    output_dir: Path,\n    model_name: str,\n    max_steps: int,\n) -> None:\n    \"\"\"Process a single SWEBench instance.\"\"\"\n    instance_id = instance[\"instance_id\"]\n    instance_dir = output_dir / instance_id\n    \n    # Avoid inconsistent state if something here fails and there's leftover previous files\n    remove_from_preds_file(output_dir / \"preds.json\", instance_id)\n    (instance_dir / f\"{instance_id}.traj.json\").", "metadata": {}}
{"id": "46", "text": "def process_instance(\n    instance: dict,\n    output_dir: Path,\n    model_name: str,\n    max_steps: int,\n) -> None:\n    \"\"\"Process a single SWEBench instance.\"\"\"\n    instance_id = instance[\"instance_id\"]\n    instance_dir = output_dir / instance_id\n    \n    # Avoid inconsistent state if something here fails and there's leftover previous files\n    remove_from_preds_file(output_dir / \"preds.json\", instance_id)\n    (instance_dir / f\"{instance_id}.traj.json\").unlink(missing_ok=True)\n    \n    # Initialize the model and parser\n    llm = OpenAIModel(ResponseParser.END_CALL, model_name)\n    parser = ResponseParser()\n    task = instance[\"problem_statement\"]\n    \n    print(f\"Processing instance {instance_id}\")\n    agent = None    \n    result = \"\"\n    \n    try:\n        # Initialize the environment\n        env = SWEEnvironment(instance)\n        # Initialize the agent\n        agent = ReactAgent(\"swe-agent\", parser, llm)\n        # Register environment tools\n        agent.add_functions([env.run_bash_cmd])\n        # agent.add_functions([env.run_bash_cmd, env.", "metadata": {}}
{"id": "47", "text": "END_CALL, model_name)\n    parser = ResponseParser()\n    task = instance[\"problem_statement\"]\n    \n    print(f\"Processing instance {instance_id}\")\n    agent = None    \n    result = \"\"\n    \n    try:\n        # Initialize the environment\n        env = SWEEnvironment(instance)\n        # Initialize the agent\n        agent = ReactAgent(\"swe-agent\", parser, llm)\n        # Register environment tools\n        agent.add_functions([env.run_bash_cmd])\n        # agent.add_functions([env.run_bash_cmd, env.replace_in_file, env.show_file])\n        # Provide env reference so the agent can query staged changes before finishing\n        agent.env = env  # type: ignore[attr-defined]\n        # Run the agent\n        output = agent.run(task, max_steps) \n        \n        # Generate patch for SWE-Bench\n        result = env.generate_patch(output)\n        \n    except Exception as e:\n        print(f\"Error processing instance {instance_id}: {e}\")\n        \n    finally:\n        # Save the trajectory and update the predictions file\n        save_traj(\n            agent,\n            instance_dir / f\"{instance_id}.traj.json\",", "metadata": {}}
{"id": "48", "text": "show_file])\n        # Provide env reference so the agent can query staged changes before finishing\n        agent.env = env  # type: ignore[attr-defined]\n        # Run the agent\n        output = agent.run(task, max_steps) \n        \n        # Generate patch for SWE-Bench\n        result = env.generate_patch(output)\n        \n    except Exception as e:\n        print(f\"Error processing instance {instance_id}: {e}\")\n        \n    finally:\n        # Save the trajectory and update the predictions file\n        save_traj(\n            agent,\n            instance_dir / f\"{instance_id}.traj.json\",\n            result=result,\n            instance_id=instance_id,\n        )\n        update_preds_file(output_dir / \"preds.json\", instance_id, model_name, result)\n        print(f\"Completed instance {instance_id}, result: {result}\")", "metadata": {}}
{"id": "49", "text": "generate_patch(output)\n        \n    except Exception as e:\n        print(f\"Error processing instance {instance_id}: {e}\")\n        \n    finally:\n        # Save the trajectory and update the predictions file\n        save_traj(\n            agent,\n            instance_dir / f\"{instance_id}.traj.json\",\n            result=result,\n            instance_id=instance_id,\n        )\n        update_preds_file(output_dir / \"preds.json\", instance_id, model_name, result)\n        print(f\"Completed instance {instance_id}, result: {result}\")\n\n@app.command(help=\"Run CS294 HW on subset of SWEBench instances.\")\ndef main(\n    subset: str = typer.Option(\"cs294\", \"--subset\", help=\"SWEBench subset used or path to a dataset\", rich_help_panel=\"Data selection\"),\n    split: str = typer.Option(\"test\", \"--split\", help=\"Dataset split\", rich_help_panel=\"Data selection\"),\n    output: str = typer.Option(\"results\", \"-o\", \"--output\", help=\"Output directory\", rich_help_panel=\"Basic\"),\n    model_name: str = typer.Option(\"gpt-5-mini\", \"--model\", help=\"Model used\", rich_help_panel=\"Basic\"),\n    max_steps: int = typer.Option(100, \"--max-steps\", help=\"Maximum number of steps\", rich_help_panel=\"Basic\"),\n    # NOTE: provide any extra arguments if needed\n) -> None:\n    output_path = Path(output)\n    output_path.mkdir(parents=True, exist_ok=True)\n    print(f\"Results will be saved to {output_path}\")", "metadata": {}}
{"id": "50", "text": "dataset_path = DATASET_MAPPING.get(subset, subset)\n    print(f\"Loading dataset {dataset_path}, split {split}...\")\n    instances = list(load_dataset(dataset_path, split=split))\n    print(f\"Loaded dataset {dataset_path}, split {split}...\")\n    print(f\"Running on {len(instances)} instances...\")\n    # instances = [instances[7]]\n    print(f\"Running on {len(instances)} instances...\")\n    # print(f\"Instances: {instances}\")\n\n    def process_futures(futures: dict[concurrent.futures.Future, str]):\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                future.result()\n            except concurrent.futures.CancelledError:\n                pass\n            except Exception as e:\n                instance_id = futures[future]\n                print(f\"Error in future for instance {instance_id}: {e}\")", "metadata": {}}
{"id": "51", "text": "def process_futures(futures: dict[concurrent.futures.Future, str]):\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                future.result()\n            except concurrent.futures.CancelledError:\n                pass\n            except Exception as e:\n                instance_id = futures[future]\n                print(f\"Error in future for instance {instance_id}: {e}\")\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        futures = {\n            executor.submit(process_instance, instance, output_path, model_name, max_steps): instance[\n                \"instance_id\"\n            ]\n            for instance in instances\n        }\n        try:\n            process_futures(futures)\n        except KeyboardInterrupt:\n            print(\"Cancelling all pending jobs. Press ^C again to exit immediately.\")\n            for future in futures:\n                if not future.running() and not future.done():\n                    future.cancel()\n            process_futures(futures)\n\n\nif __name__ == \"__main__\":\n    app()", "metadata": {}}
{"id": "52", "text": "import dataclasses\nimport json\nfrom collections.abc import Callable\nfrom pathlib import Path\nfrom typing import Any\nfrom minisweagent import Environment\nfrom minisweagent.environments import get_environment\nimport json\nimport threading\nimport subprocess\n\n_OUTPUT_FILE_LOCK = threading.Lock()\n    \ndef get_swebench_docker_image_name(instance: dict) -> str:\n    \"\"\"Get the image name for a SWEBench instance.\"\"\"\n    image_name = instance.get(\"image_name\", None)\n    if image_name is None:\n        # Docker doesn't allow double underscore, so we replace them with a magic token\n        iid = instance[\"instance_id\"]\n        id_docker_compatible = iid.replace(\"__\", \"_1776_\")\n        image_name = f\"docker.io/swebench/sweb.eval.x86_64.{id_docker_compatible}:latest\".lower()\n    return image_name", "metadata": {}}
{"id": "53", "text": "def get_sb_environment(instance: dict) -> Environment:\n    env_config = {\n        \"image\": get_swebench_docker_image_name(instance),\n        \"cwd\": \"/testbed\",\n        \"timeout\": 60,\n        \"env\": {\n            \"PAGER\": \"cat\",\n            \"MANPAGER\": \"cat\",\n            \"LESS\": \"-R\",\n            \"PIP_PROGRESS_BAR\": \"off\",\n            \"TQDM_DISABLE\": \"1\",\n        },\n        \"environment_class\": \"docker\",\n    }\n    env = get_environment(env_config)\n    return env\n\ndef update_preds_file(output_path: Path, instance_id: str, model_name: str, result: str):\n    \"\"\"Update the output JSON file with results from a single instance.\"\"\"\n    with _OUTPUT_FILE_LOCK:\n        output_data = {}\n        if output_path.exists():\n            output_data = json.loads(output_path.read_text())\n        output_data[instance_id] = {\n            \"model_name_or_path\": model_name,\n            \"instance_id\": instance_id,\n            \"model_patch\": result,\n        }\n        output_path.write_text(json.dumps(output_data, indent=2))", "metadata": {}}
{"id": "54", "text": "def update_preds_file(output_path: Path, instance_id: str, model_name: str, result: str):\n    \"\"\"Update the output JSON file with results from a single instance.\"\"\"\n    with _OUTPUT_FILE_LOCK:\n        output_data = {}\n        if output_path.exists():\n            output_data = json.loads(output_path.read_text())\n        output_data[instance_id] = {\n            \"model_name_or_path\": model_name,\n            \"instance_id\": instance_id,\n            \"model_patch\": result,\n        }\n        output_path.write_text(json.dumps(output_data, indent=2))\n\ndef remove_from_preds_file(output_path: Path, instance_id: str):\n    \"\"\"Remove an instance from the predictions file.\"\"\"\n    if not output_path.exists():\n        return\n    with _OUTPUT_FILE_LOCK:\n        output_data = json.loads(output_path.read_text())\n        if instance_id in output_data:\n            del output_data[instance_id]\n            output_path.write_text(json.dumps(output_data, indent=2))", "metadata": {}}
{"id": "55", "text": "def remove_from_preds_file(output_path: Path, instance_id: str):\n    \"\"\"Remove an instance from the predictions file.\"\"\"\n    if not output_path.exists():\n        return\n    with _OUTPUT_FILE_LOCK:\n        output_data = json.loads(output_path.read_text())\n        if instance_id in output_data:\n            del output_data[instance_id]\n            output_path.write_text(json.dumps(output_data, indent=2))\n\ndef save_traj(\n    agent: Any | None,\n    path: Path,\n    *,\n    print_path: bool = True,\n    result: str | None = None,\n    **kwargs,\n):\n    \"\"\"Save the trajectory of the agent to a file.\n\n    Args:\n        agent: The agent to save the trajectory of.\n        path: The path to save the trajectory to.\n        print_path: Whether to print confirmation of path to the terminal.\n        result: The result/submission of the agent.\n        **kwargs: Additional information to save (will be merged into top level)", "metadata": {}}
{"id": "56", "text": "def save_traj(\n    agent: Any | None,\n    path: Path,\n    *,\n    print_path: bool = True,\n    result: str | None = None,\n    **kwargs,\n):\n    \"\"\"Save the trajectory of the agent to a file.\n\n    Args:\n        agent: The agent to save the trajectory of.\n        path: The path to save the trajectory to.\n        print_path: Whether to print confirmation of path to the terminal.\n        result: The result/submission of the agent.\n        **kwargs: Additional information to save (will be merged into top level)\n\n    \"\"\"\n    data = {\n        \"info\": {\n            \"submission\": result,\n        },\n        \"messages\": [],\n        \"trajectory_format\": \"mini-swe-agent-1\",\n    } | kwargs\n    if agent is not None:\n        # NOTE: save messages if you want \n        data[\"info\"][\"config\"] = {\n            \"agent\": agent.name,\n            \"model\": agent.llm.model_name,\n        }\n        \n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json.dumps(data, indent=2))\n    if print_path:\n        print(f\"Saved trajectory to '{path}'\")", "metadata": {}}
